{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c11dcf5-8a18-49c6-8047-f8a78fc70d74",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Fine-tuning large language models (LLMs) almost always requires multiple GPUs to be practical (or possible at all). But if you're relatively new to deep learning, or you've only trained models on single GPUs before, making the jump to distributed training on multiple GPUs and multiple nodes can be extremely challenging and more than a little frustrating.\n",
    "\n",
    "As noted in the [readme](../README.md), the goal of this project is to start small and gradually add complexity. So we're not going to start with a \"large language model\" at all. We're starting with a very small model called [t5-small](https://huggingface.co/t5-small). Why start with a small model if we want to train considerably larger models?\n",
    "- Learning about model fine-tuning is a lot less frustrating if you start from a place of less complexity and are able to get results quickly!\n",
    "- When we get to the point of training larger models on distributed systems, we're going to spend a lot of time and energy on *how* to distribute the model, data, etc., across that system. Starting smaller lets us spend some time at the beginning focusing on the training metrics that directly relate to model performance rather than the complexity involved with distributed training. Eventually we will need both, but there's no reason to try to digest all of it all at once!\n",
    "- Starting small and then scaling up will give us a solid intuition of how, when, and why to use the various tools and techniques for training larger models or for using more compute resources to train models faster.\n",
    "\n",
    "## Fine-Tuning t5-small\n",
    "Our goals in this notebook are simple. We want to fine-tune the `t5-small` model and verify that its behavior has changed as a result of our fine-tuning.\n",
    "\n",
    "The [t5 (text-to-text transfer transformer) family of models](https://blog.research.google/2020/02/exploring-transfer-learning-with-t5.html) was developed by Google Research. It was presented as an advancement over BERT-style models which could output only a class label or a span of the input. t5 allows the same model, loss, and hyperparameters to be used for *any* nlp task. t5 differs from GPT models because it is an encoder-decoder model, while GPT models are decoder-only models.\n",
    "\n",
    "t5-small is a 60 million parameter model. This is *small*: the smallest version of GPT2 has more than twice as many parameters (124M); llama2-7b, one of the most commonly-used models at the time of writing, has more than 116 times as many parameters (7B, hence the name). What does this mean for us? Parameter count strongly impacts the amount of memory required to train a model. Eleuther's [Transformer Math blog post](https://blog.eleuther.ai/transformer-math/#training) has a great overview of the memory costs associated with training models of different sizes. We'll get into this in more detail in a later notebook.\n",
    "\n",
    "## A few things to keep in mind\n",
    "Check out the [Readme](README.md) if you haven't already, as it provides important context for this whole project. If you're looking for a set of absolute best practices for how to train particular models, this isn't the place to find them (though I will link them when I come across them, and will try to make improvements where I can, as long as they don't come at the cost of extra complexity!). The goal is to develop a high-level understanding and intuition on model training and fine-tuning, so you can fairly quickly get to something that *works* and then iterate to make it work *better*.\n",
    "\n",
    "## Compute used in this example\n",
    "I am using a `g4dn.4xlarge` AWS ec2 instance, which has a single T4 GPU with 16GB VRAM.\n",
    "\n",
    "# 1. Get the model and generate some predictions\n",
    "Before training the model, it helps to have some sense of its base behavior. Let's take a look. See appendix C of the [t5 paper](https://jmlr.org/papers/volume21/20-074/20-074.pdf) for examples of how to format inputs for various tasks.\n",
    "\n",
    "We will be using the [transformers](https://huggingface.co/docs/transformers/index) library to load and run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54649ba-e0cb-4f64-a10b-dd509fc16976",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade transformers torch accelerate\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37ae82d9-587e-4f75-9b30-41f253e0fbbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Check if GPU is available and move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Sample text\n",
    "input_text = (\n",
    "    \"question: What is the deepspeed license?  context: DeepSpeed \"\n",
    "    \"is an open source deep learning optimization library for PyTorch. \"\n",
    "    \"The library is designed to reduce computing power and memory use \"\n",
    "    \"and to train large distributed models with better parallelism on \"\n",
    "    \"existing computer hardware. DeepSpeed is optimized for low latency, \"\n",
    "    \"high throughput training. It includes the Zero Redundancy Optimizer \"\n",
    "    \"(ZeRO) for training models with 1 trillion or more parameters. \"\n",
    "    \"Features include mixed precision training, single-GPU, multi-GPU, \"\n",
    "    \"and multi-node training as well as custom model parallelism. The \"\n",
    "    \"DeepSpeed source code is licensed under MIT License and available on GitHub.\"\n",
    ")\n",
    "\n",
    "# Another task you could try:\n",
    "# input_text = \"Translate English to German: The house is wonderful.\"\n",
    "\n",
    "# Encode and generate response\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(input_ids, max_new_tokens=20)[0]\n",
    "\n",
    "# Decode and print the output text\n",
    "output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you understand what the above code did, great! Feel free to move on. If not, also great! Here's some suggested reading.\n",
    "- Learn about loading models with the Transformers library [here](https://huggingface.co/learn/nlp-course/chapter2/3). The whole course is well worth the time and will give a lot of useful background for this fine-tuning guide.\n",
    "- Learn about tokenizers from Hugging Face [here](https://huggingface.co/docs/transformers/main/en/preprocessing).\n",
    "- We'll talk a lot more about GPUs later, but the `device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")` line checks whether the system has a CUDA-enabled GPU and the necessary software to use it. If yes, it will use the GPU; otherwise, it will fall back to CPU. This is a common pattern when training models with PyTorch, which is the underlying framework used by the transformers library in the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c46abb2d-7ea5-49b8-9277-ece228ecb165",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2. Pick a Fine-Tuning Task\n",
    "\n",
    "We want to train the model to do something it's not already capable of. Let's see if we can get it to distinguish between a few different programming languages. Is it able to do this now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fbdce04-5fe1-4d4c-bf30-f2522a2038f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_text = \"question: what programming languauage is this?  code: `df <- read.csv('data.csv'); summary(df)`\"\n",
    "\n",
    "# Encode and generate response\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(input_ids, max_new_tokens=20)[0]\n",
    "\n",
    "# Decode and print the output text\n",
    "output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "387cd971-5d73-4a12-a9cf-ab129f212a3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Doesn't look like it! Based on the T5 paper, I couldn't identify a prompt that would elicit this behavior. Furthermore, When T5 was trained, code examples were apparently explicitly excluded. So let's see if we can add this capability.\n",
    "\n",
    "> Some pages inadvertently contained code. Since the curly bracket “{” appears in many programming languages (such as Javascript, widely used on the web) but not in natural text, we removed any pages that contained a curly bracket.\n",
    "\n",
    "We will use the [tiny-codes](https://huggingface.co/datasets/nampdn-ai/tiny-codes) dataset, which includes code snippets along with their corresponding language, as training data. Let's pull down the dataset and do a quick exploratory analysis. Then we will have to do some cleaning and prep to make the code usable for our purposes.\n",
    "\n",
    "# 3. Get and Explore the Data\n",
    "\n",
    "You will need to create a hugging face account and log in using the `notebook_login()` interface below in order to download this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7efdd00-6e31-4f70-a818-bb3250ac37a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from huggingface_hub import notebook_login\n",
    "# Login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d869ea4-90b4-4962-b780-520ba16fb45a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "tinycodes = load_dataset('nampdn-ai/tiny-codes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what languages are included and how many examples of each are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the counts of the programming_language column\n",
    "language_counts = tinycodes['train'].to_pandas()['programming_language'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(language_counts)\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(language_counts.index, language_counts.values, alpha=0.8)\n",
    "plt.title('Programming Language Distribution')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Programming Language', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')  # Angle the x-axis labels to 45 degrees and align them to the right\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df31d5ce-f264-4757-995b-1239a8f04bf3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are no major imbalances and each of these languages has plenty of examples, so we're not going to do any rebalancing. Now let's take a look at what a given example looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21523987-f15d-48a6-996d-b8aca82317a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tinycodes['train'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9164f228-6bf5-47eb-983c-f40e714df4c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Given our goal of creating a language classifier, we only need the `programming_language` field and the `response`. We will also need to transform the response, extracting only the code between the backticks (omitting the language name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e826ae7-9317-4a48-b6dc-9b2d541c5fbb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "def preprocess_tinycodes(tinycodes: datasets.DatasetDict):\n",
    "    tinycodes_subset = tinycodes[\"train\"].to_pandas()[\n",
    "        [\"idx\", \"programming_language\", \"response\"]\n",
    "    ]\n",
    "    # Extract code from response\n",
    "    tinycodes_subset[\"code\"] = tinycodes_subset[\"response\"].str.extract(\n",
    "        r\"```[a-zA-Z+#]*\\s+([\\s\\S]*?)```\", expand=False\n",
    "    )\n",
    "    return tinycodes_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e12ba3e-7f8c-4779-9afe-628401b759e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_test = preprocess_tinycodes(tinycodes)\n",
    "data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed30a3d4-7baf-4fda-b5fe-12ad73a4fc71",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You'll notice that this is far from perfect: many examples were not captured by this regex match. Let's investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f95f94-5032-4e68-904c-289ec8cee25a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate the percentage of NA for each programming language in data_test\n",
    "na_percent = data_test.groupby(\"programming_language\")[\"code\"].apply(lambda x: x.isna().mean() * 100)\n",
    "\n",
    "# Display the percentage in a dataframe\n",
    "na_percent_df = pd.DataFrame(na_percent)\n",
    "na_percent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30656538-d092-47a8-bd73-5b93664fdf99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We would generally spend a lot more time iterating on the data and trying to capture code from a much greater proportion of examples. However, in this case, we're just going to subset to those languages where less than 25% of examples are NA and proceed with the fine tuning. Let's update accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75de9586-3034-4d60-9b21-f934659cc9e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def extract_code_from_tinycodes(\n",
    "    tinycodes: datasets.DatasetDict, train_frac=0.7, valid_frac=0.2\n",
    "):\n",
    "    # Subset languages to Bash, C#, JavaScript, Julia, Python, Rust, TypeScript\n",
    "    languages = [\"Bash\", \"C#\", \"JavaScript\", \"Julia\", \"Python\", \"Rust\", \"TypeScript\"]\n",
    "    tinycodes_subset = tinycodes[\"train\"].to_pandas()[\n",
    "        [\"idx\", \"programming_language\", \"response\"]\n",
    "    ]\n",
    "    tinycodes_subset = tinycodes_subset[\n",
    "        tinycodes_subset[\"programming_language\"].isin(languages)\n",
    "    ]\n",
    "    # Extract code from response\n",
    "    tinycodes_subset[\"code\"] = tinycodes_subset[\"response\"].str.extract(\n",
    "        r\"```[a-zA-Z+#]*\\s+([\\s\\S]*?)```\", expand=False\n",
    "    )\n",
    "    # Drop rows with missing code\n",
    "    tinycodes_subset = tinycodes_subset.dropna(subset=[\"code\"])\n",
    "\n",
    "    # Shuffle the data\n",
    "    tinycodes_subset = tinycodes_subset.sample(frac=1, random_state=42)\n",
    "    # Split the data into train, valid, and test sets\n",
    "    train, valid, test = np.split(\n",
    "        tinycodes_subset,\n",
    "        [\n",
    "            int(train_frac * len(tinycodes_subset)),\n",
    "            int((train_frac + valid_frac) * len(tinycodes_subset)),\n",
    "        ],\n",
    "    )\n",
    "    # Convert the pandas dataframes back to DatasetDict\n",
    "    tinycodes_subset = datasets.DatasetDict(\n",
    "        {\n",
    "            \"train\": datasets.Dataset.from_pandas(train),\n",
    "            \"valid\": datasets.Dataset.from_pandas(valid),\n",
    "            \"test\": datasets.Dataset.from_pandas(test),\n",
    "        }\n",
    "    )\n",
    "    return tinycodes_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02b4005-8612-4345-bc60-dea4329a2615",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tinycodes_subset = extract_code_from_tinycodes(tinycodes)\n",
    "tinycodes_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d2f867-f421-4ccf-8b37-b88176a6bd93",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# count number of each programming language\n",
    "tinycodes_subset['train'].to_pandas()['programming_language'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a769f2d0-b126-442a-a1d4-7bd7944067a3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We still have a reasonably balanced training set, with between 69,000 and 92,000 examples of each language. Let's proceed! Next we need to preprocess the data for training by adding a prefix specifying the task and then tokenizing the data.\n",
    "\n",
    "## Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d0bacdc-390b-445d-8790-07e452519e57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    prefix = \"question: what programming language is this?  code:\\n\\n\"\n",
    "    inputs = [prefix + ex for ex in examples[\"code\"]]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        text_target=examples[\"programming_language\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Assuming 'dataset' is your DatasetDict\n",
    "tokenized_datasets = tinycodes_subset.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    batched=True,\n",
    "    batch_size=4096,\n",
    "    load_from_cache_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fb2a40a-cbd2-448c-8976-ab434efc114d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# set up dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=16)\n",
    "val_dataloader = DataLoader(tokenized_datasets[\"valid\"], batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dbff09c-046b-4061-8608-2d6584c33259",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aacd7588-d5e0-45b4-9d7e-57f89005a1c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"t5-small\"  # Or another T5 variant\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Volumes/daniel_liden/fine_tune/assets/results/\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1e4,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"/Volumes/daniel_liden/fine_tune/assets/logs/\",\n",
    "    learning_rate=2e-5,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Start training\n",
    "with mlflow.start_run():\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f30d855d-387a-4814-a990-ac2753dfd663",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del model\n",
    "del tokenizer\n",
    "del trainer\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a012e06-5012-4b6a-bb54-3f719c85d8dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Inference with our fine-tuned model\n",
    "\n",
    "Before we get too far, there are a few important things to point out here:\n",
    "- The choice of training hyperparameters was more-or-less arbitrary. In real life, we would do a lot more experimentation with e.g. batch size, regularization, etc.\n",
    "- Similarily, the data mix was chosen for expediency, not for suitability for the current task. If I had to venture a guess before seeing any results, I would guess that the model learned shortcuts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b3e013f-b578-4bc4-911c-71c6b8db3e7e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"/Volumes/daniel_liden/fine_tune/assets/results/checkpoint-50823\")\n",
    "\n",
    "# Check if GPU is available and move the model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "input_texts = [\n",
    "    \"question: what programming language is this?  code: def add_a_b(a, b): return a + b\",  # Python\n",
    "    \"question: what programming language is this?  code: public class HelloWorld { public static void Main() { Console.WriteLine(\\\"Hello, World!\\\"); } }\",  # C#\n",
    "    \"question: what programming language is this?  code: #include <iostream> int main() { std::cout << \\\"Hello, World!\\\" << std::endl; return 0; }\",  # C++\n",
    "    \"question: what programming language is this?  code: println(\\\"Hello, World!\\\")\",  # Julia\n",
    "    \"question: what programming language is this?  code: echo \\\"Hello, World!\\\"\",  # Bash\n",
    "    \"question: what programming language is this?  code: fn main() { println!(\\\"Hello, World!\\\"); }\"  # Rust\n",
    "]\n",
    "\n",
    "# Tokenize and encode the batch of input texts\n",
    "input_ids = tokenizer.batch_encode_plus(input_texts, padding=True, return_tensors=\"pt\", truncation=True).to(device)\n",
    "\n",
    "# Generate responses for the batch\n",
    "output_ids = model.generate(input_ids[\"input_ids\"], max_new_tokens=20)\n",
    "\n",
    "# Decode and print the output texts\n",
    "for i, output_id in enumerate(output_ids):\n",
    "    output_text = tokenizer.decode(output_id, skip_special_tokens=True)\n",
    "    print(f\"Input: {input_texts[i]}\")\n",
    "    print(f\"Output: {output_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a98cf455-7f22-4686-95c0-ad0d7af451f7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It misclassified C++ code as Rust code, but otherwise got the examples correct. We've added a new capability to this model. Now the question is...does it still remember how to do what it was trained on before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "564f395c-8de8-4549-adc6-d17b61aad038",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "input_text = (\n",
    "    \"question: What is the deepspeed license?  context: DeepSpeed \"\n",
    "    \"is an open source deep learning optimization library for PyTorch. \"\n",
    "    \"The library is designed to reduce computing power and memory use \"\n",
    "    \"and to train large distributed models with better parallelism on \"\n",
    "    \"existing computer hardware. DeepSpeed is optimized for low latency, \"\n",
    "    \"high throughput training. It includes the Zero Redundancy Optimizer \"\n",
    "    \"(ZeRO) for training models with 1 trillion or more parameters. \"\n",
    "    \"Features include mixed precision training, single-GPU, multi-GPU, \"\n",
    "    \"and multi-node training as well as custom model parallelism. The \"\n",
    "    \"DeepSpeed source code is licensed under MIT License and available on GitHub.\"\n",
    ")\n",
    "\n",
    "# Encode and generate response\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "output_ids = model.generate(input_ids, max_new_tokens=20)[0]\n",
    "\n",
    "# Decode and print the output text\n",
    "output_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7c72e96-98f1-4421-950e-ba300003b5d4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There's an interesting result here: if we try the input above after training for all three epochs, we get an incorrect result. However, if we use the checkpoint saved after only two epochs, it still gets the correct answer. This highlights something really important: fine-tuning on new knowledge or new tasks can make the model \"forget\" its prior training. To get around this, we need to be thoughtful about how much new data we use, how long we train for, how we evaluate the fine-tuned model, and what mixture of data we use for fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1180b26-2d86-4f25-9ee3-b4efc3200eba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this example, we fine-tuned `t5-small` on a new task. The basic process was:\n",
    "- Pick a model and task\n",
    "- Pick a dataset\n",
    "- Process the dataset\n",
    "- Train the model on the processed dataset\n",
    "\n",
    "This omits plenty of important steps we'll get further into in future notebooks. For example:\n",
    "- tuning hyperparameters\n",
    "- thoroughly evaluating the results\n",
    "- tuning training speed/efficiency\n",
    "- monitoring the training loop (we did track the experiment with MLflow but did not configure it in any way for our specific usage)\n",
    "\n",
    "We will get into each of these in future examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "babab007-fc2b-4dd9-8e48-bfee1b6ff7c3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. T5-Small on Single GPU",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python (mlops)",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
