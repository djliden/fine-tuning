{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Over the course of experimenting with fine-tuning tinyllama, I realized that a lot of the complexity comes from the data pre-processing step. This notebook will break down the pre-processing process and call attention to different possible considerations and approaches.\n",
    "\n",
    "In pre-processing the data, it is important to understand (at least) the following:\n",
    "- How is the raw data stuctured?\n",
    "- What does a single example from the raw data look like?\n",
    "- How do we need to transform the data for training?\n",
    "- What does a single example in the transformed data look like?\n",
    "- How do we need to tokenize the data for training?\n",
    "- How do we batch the tokenized data for training?\n",
    "\n",
    "## Setup\n",
    "\n",
    "We're going to load the tokenizer and the dataset, but not the model. We won't need it in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from -r ./tinyllama_requirements.txt (line 1)) (4.37.1)\n",
      "Requirement already satisfied: accelerate in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from -r ./tinyllama_requirements.txt (line 2)) (0.26.1)\n",
      "Requirement already satisfied: psutil in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from -r ./tinyllama_requirements.txt (line 3)) (5.9.8)\n",
      "Requirement already satisfied: pynvml in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from -r ./tinyllama_requirements.txt (line 4)) (11.5.0)\n",
      "Requirement already satisfied: bitsandbytes in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from -r ./tinyllama_requirements.txt (line 5)) (0.42.0)\n",
      "Requirement already satisfied: filelock in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from accelerate->-r ./tinyllama_requirements.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: scipy in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from bitsandbytes->-r ./tinyllama_requirements.txt (line 5)) (1.11.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r ./tinyllama_requirements.txt (line 1)) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r ./tinyllama_requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r ./tinyllama_requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r ./tinyllama_requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r ./tinyllama_requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from requests->transformers->-r ./tinyllama_requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from requests->transformers->-r ./tinyllama_requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from requests->transformers->-r ./tinyllama_requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from requests->transformers->-r ./tinyllama_requirements.txt (line 1)) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->-r ./tinyllama_requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate->-r ./tinyllama_requirements.txt (line 2)) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade -r ./tinyllama_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR =  \"../cache/TinyLlama/\" # the path to the cache directory; where cache files will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer corresponding to the model checkpoint\n",
    "model_ckpt = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_ckpt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does not specify a pad token.  If we want to use padding (more on this later), we need to set the pad token. We can do this with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "slimorca = load_dataset(\"Open-Orca/SlimOrca\", cache_dir=str(Path(CACHE_DIR) / \"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is the raw data stuctured?\n",
    "\n",
    "What have we actually loaded here? We used the Hugging Face `datasets` library to load the dataset, and the resulting object is a `datasetdict`. A `datasetdict` is a dictionary-like object for managing datasets, where the keys are the names of the datasets, and the values are the actual datasets. Usually the datasets are splits such as \"train\", \"valid\", and \"test\". In this case, we only have a \"train\" split, with only one feature, \"conversations\".\n",
    "\n",
    "`DatasetDict`s enable us to map various pre-processing operations to the datasets they contain concisely and efficiently.\n",
    "\n",
    "Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 517982\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slimorca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train` split within the `DatasetDict` is a `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations'],\n",
       "    num_rows: 517982\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slimorca[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we need to transform the data for training?\n",
    "\n",
    "Hugging Face `Dataset`s provide [various methods for querying, subsetting, and processing data](https://huggingface.co/docs/datasets/process), generally quite efficiently because they use the [Apache Arrow format](https://huggingface.co/docs/datasets/about_arrow). For example, if we want a validation set, we can split our training set into separate train/valid sets. The data are shuffled by default, so we don't need to shuffle as a separate step. Note that we apply this operation to the `Dataset`, not the `DatasetDict`.\n",
    "\n",
    "The resulting object is a new `DatasetDict` with two keys: \"train\" and \"test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 466183\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 51799\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slimorca_split = slimorca[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "slimorca_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does a single example from the raw data look like?\n",
    "\n",
    "Now let's look at some of the actual data examples. What do the `conversations` look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'system',\n",
       "   'value': 'You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.',\n",
       "   'weight': None},\n",
       "  {'from': 'human',\n",
       "   'value': 'Q:Read the article and select the best answer. Article: Tina was not like many of her classmates. She didn\\'t listen to popular music or watch many movies, and she wasn\\'t interested in nice clothes. When she got together with her friends, they wanted to listen to rock and pop music. When Tina asked  if  they would  like  to  try classical    music, they all looked at her strangely.\"Classical music  is  for old people, \" one of  her friends said. Tina was worried that something was wrong with her. She decided to talk to her father. As she entered his study  , her father could feel something was wrong. \"Dad, am I strange?\" she asked her father.\"Of course not, \" he answered. \"Why do you ask that?\" \"Because I don\\'t like the same things as my classmates do. They want to listen to Mariah Carey\\'s music. I like Yo Yo Ma\\'s.\" \"I can understand, Tina,  it\\'s all  right _ You don\\'t have to copy   what other people do. Everybody has different tastes. Some of them are popular, and others aren\\'t. \"After talking with her father, Tina felt better. She realized    that being different made her special.  It was an important lesson for her to learn. Question: Tina\\'s father made Tina feel  _  . Options: A: angry B: worried C: excited D: better\\nA:',\n",
       "   'weight': 0.0},\n",
       "  {'from': 'gpt', 'value': 'D: better', 'weight': 1.0}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slimorca_split[\"train\"][42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objects are dictionaries with information about the roles (\"system\", \"human\", or \"gpt\") and values making up the exchange. Our task is to map this to a format on which we can train the model.\n",
    "\n",
    "The Hugging Face Transformers library provides convenient [chat model templates](https://huggingface.co/docs/transformers/main/en/chat_templating#templates-for-chat-models). The Hugging Face docs recommend applying the chat templates as a preprocessing step.\n",
    "\n",
    "We won't go into too much detail about the concept of chat templates—you can read more [here](https://huggingface.co/blog/chat-templates). For now, just know that they provide a means of clearly indicating which part of a string came from the user and which part is the LLM's expected response. Models are trained on specific chat formats, and using different formats will generally result in bad responses at inference time.\n",
    "\n",
    "Here's an example of a chat dictionary translated to a string with the chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a helpful assistant and an expert at making coffee.\n",
      "<</SYS>>\n",
      "\n",
      "How do I make coffee with a Chemex coffee maker? [/INST] To make coffee with a Chemex:\n",
      "1. Boil water to about 200°F (93°C).\n",
      "2. Place the Chemex filter in the top and rinse it with hot water to remove paper taste and warm the vessel. Discard the rinse water.\n",
      "3. Add coffee grounds to the filter. Use a medium-coarse grind, about 1 gram of coffee per 16 grams of water.\n",
      "4. Pour just enough hot water to saturate the grounds. Wait 30 seconds for the coffee to 'bloom'.\n",
      "5. Slowly pour the remaining water over the grounds in a circular motion. Aim for a total brew time of 3.5 to 4.5 minutes.\n",
      "6. Once brewing is complete, remove the filter and enjoy. </s>\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant and an expert at making coffee.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How do I make coffee with a Chemex coffee maker?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"To make coffee with a Chemex:\\n1. Boil water to about 200°F (93°C).\\n2. Place the Chemex filter in the top and rinse it with hot water to remove paper taste and warm the vessel. Discard the rinse water.\\n3. Add coffee grounds to the filter. Use a medium-coarse grind, about 1 gram of coffee per 16 grams of water.\\n4. Pour just enough hot water to saturate the grounds. Wait 30 seconds for the coffee to 'bloom'.\\n5. Slowly pour the remaining water over the grounds in a circular motion. Aim for a total brew time of 3.5 to 4.5 minutes.\\n6. Once brewing is complete, remove the filter and enjoy.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(chat, tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that using the chat template adds some special tokens indicating the beginning/end of the chat (`<s>` and `</s>`) and the beginning/end of the instruction (`[INST]` and `[/INST]`). We will need to add these to our tokenizer as special tokens later.\n",
    "\n",
    "Let's map the `conversations` to the expected input for the chat template. The Hugging Face chat model templates expect a dictionary similar to `conversations` but with some notable differences. We need to replace `from` with `role`, `value` with `content`, `gpt` with `assistant`, and `human` with `user`. We will save the actual conversion until later, as there are other considerations we still need to address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'chat'],\n",
       "        num_rows: 466183\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conversations', 'chat'],\n",
       "        num_rows: 51799\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_chat(ex):\n",
    "    role_mapping = {\"gpt\": \"assistant\", \"system\": \"system\", \"human\": \"user\"}\n",
    "    chat = [\n",
    "        {\"role\": role_mapping[message[\"from\"]], \"content\": message[\"value\"]}\n",
    "        for message in ex[\"conversations\"]\n",
    "    ]\n",
    "\n",
    "    return {\"chat\": chat}\n",
    "\n",
    "\n",
    "slimorca_split_formatted_chat = slimorca_split.map(format_chat, num_proc=32)\n",
    "slimorca_split_formatted_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does a single example of the transformed data look like?\n",
    "\n",
    "Now we have added a 'chat' key to each example in `slimorca_split_formatted_chat`. Note how we applied this transformation. We wrote a function that processed a single example and then used the `map` method to apply it to all examples in the `DatasetDict`. The `num_proc` parameter specifies how many processes to use for parallel processing, dramatically increasing the speed of the process.\n",
    "\n",
    "Let's compare the original and formatted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'from': 'system', 'value': 'You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.', 'weight': None}, {'from': 'human', 'value': 'Q:Read the article and select the best answer. Article: Tina was not like many of her classmates. She didn\\'t listen to popular music or watch many movies, and she wasn\\'t interested in nice clothes. When she got together with her friends, they wanted to listen to rock and pop music. When Tina asked  if  they would  like  to  try classical    music, they all looked at her strangely.\"Classical music  is  for old people, \" one of  her friends said. Tina was worried that something was wrong with her. She decided to talk to her father. As she entered his study  , her father could feel something was wrong. \"Dad, am I strange?\" she asked her father.\"Of course not, \" he answered. \"Why do you ask that?\" \"Because I don\\'t like the same things as my classmates do. They want to listen to Mariah Carey\\'s music. I like Yo Yo Ma\\'s.\" \"I can understand, Tina,  it\\'s all  right _ You don\\'t have to copy   what other people do. Everybody has different tastes. Some of them are popular, and others aren\\'t. \"After talking with her father, Tina felt better. She realized    that being different made her special.  It was an important lesson for her to learn. Question: Tina\\'s father made Tina feel  _  . Options: A: angry B: worried C: excited D: better\\nA:', 'weight': 0.0}, {'from': 'gpt', 'value': 'D: better', 'weight': 1.0}]\n",
      "[{'content': 'You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.', 'role': 'system'}, {'content': 'Q:Read the article and select the best answer. Article: Tina was not like many of her classmates. She didn\\'t listen to popular music or watch many movies, and she wasn\\'t interested in nice clothes. When she got together with her friends, they wanted to listen to rock and pop music. When Tina asked  if  they would  like  to  try classical    music, they all looked at her strangely.\"Classical music  is  for old people, \" one of  her friends said. Tina was worried that something was wrong with her. She decided to talk to her father. As she entered his study  , her father could feel something was wrong. \"Dad, am I strange?\" she asked her father.\"Of course not, \" he answered. \"Why do you ask that?\" \"Because I don\\'t like the same things as my classmates do. They want to listen to Mariah Carey\\'s music. I like Yo Yo Ma\\'s.\" \"I can understand, Tina,  it\\'s all  right _ You don\\'t have to copy   what other people do. Everybody has different tastes. Some of them are popular, and others aren\\'t. \"After talking with her father, Tina felt better. She realized    that being different made her special.  It was an important lesson for her to learn. Question: Tina\\'s father made Tina feel  _  . Options: A: angry B: worried C: excited D: better\\nA:', 'role': 'user'}, {'content': 'D: better', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(slimorca_split_formatted_chat[\"train\"][42][\"conversations\"])\n",
    "print(slimorca_split_formatted_chat[\"train\"][42][\"chat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Data\n",
    "\n",
    "As mentioned above, our instruction formatting includes some special tokens we would like to add to the tokenizer's vocabulary. We can do that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the instruction tokens to the tokenizer\n",
    "special_tokens = [\"[INST]\", \"[/INST]\", \"<<SYS>>\", \"<</SYS>>\"]\n",
    "# Adding special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few questions might occur to you at this point:\n",
    "> Why don't we add the `<s>` and `</s>` tokens to the tokenizer? Those were used in the chat formatting too!\n",
    "\n",
    "We don't need to add those tokens because they are already the tokenizer's `bos` (beginning of sequence) and `eos` (end of sequence) tokens, as shown below.\n",
    "\n",
    "> What exactly does it mean to \"add tokens to the tokenizer\"?\n",
    "Adding tokens to the tokenizer expands the tokenizer's *vocabulary*, the list of possible tokens that the model can use. Tokens in the tokenizer's vocabulary won't be split into smaller units. Before adding `[INST]` to the vocabulary, for example, it could only be formed through the following combination of tokens: `['[', '/', 'INST', ']']`. This allows the model to recognize the `[INST]` token as a single, distinct semantic unit: it only needs to learn one token to recognize the start of an instruction, not a sequence of four tokens that might also be used in other contexts. Furthermore, since all of the sequences we want to train on will include the special tokens above, there are some effiency gains: each of these tokens will take up only one unit of context length rather than multiple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<s>', '</s>')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the <s> and </s> tokens are already part of the vocabulary\n",
    "tokenizer.bos_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure the Tokenized Data\n",
    "\n",
    "Before we actually tokenize the chat data in our `DatasetDict`, we need to make some decisions about how to structure the tokenized data. So far, we've been thinking in terms of individual *examples* from the training data. This isn't necessarily the most relevant perspective to the model. We should think in terms of *sequences* of tokes and *batches* of sequences.\n",
    "- A *sequence* is a single list of tokens from the training data, usually limited to some uniform length. Sequences of the desired length can be formed by starting with a single example and *padding* it with the tokenizer's padding token to make it the desired length if it is too short, or by *truncating* it if it is too long. Another option is sequence *packing*, wherein multiple shorter sequences are packed into a single longer sequence.\n",
    "- A *batch* is a list of sequences. During training, the model is fed a batch of sequences at a time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisions and Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
