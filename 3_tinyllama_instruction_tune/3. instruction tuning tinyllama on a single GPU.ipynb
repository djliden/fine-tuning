{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: WORK IN PROGRESS\n",
    "\n",
    "This is a work in progress and is under active development! The latest version worked quite poorly. I think it's because I split the chat-formatted inputs on the `[INST]` tag *before* tokenizing. The conseqeuence was that the tokenizer added the beginning/end of sequence tokens at the beginnings/endings of both the instruction and the output. I suspect this led to undesirable results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The [TinyLlama](https://github.com/jzhang38/TinyLlama) project \"aims to pretrain a 1.1B Llama model on 3 trillion tokens.\" 1.1B tokens represents a considerable step up from the small GPT model we [previously fine-tuned](../2_gpt2_single_gpu/2.%20GPT2%20on%20a%20single%20GPU.ipynb). That model had 124M parameters; TinyLlama, while still small by the standards of most widely-used LLMs, is almost ten times the size. We will need around 20GB VRAM at a bare minimum to fine-tune this model.\n",
    "\n",
    "## Instruction Tuning\n",
    "We are going to focus on instruction tuning in this example. Instruction Tuning is a supervised learning technique in which we train the model on instruction/output pairs with the goal of training the model to follow human instructions. Before instruction tuning, the base model is trained on next-token completion. We saw this in the GPT2 example: we provided the start of a story and the model completed it. An instruction-tuned model, on the other hand, is trained to answer a question or instruction.\n",
    "\n",
    "[This repository](https://github.com/xiaoya-li/Instruction-Tuning-Survey) contains a wealth of information on the current state of the field of instruction tuning.\n",
    "\n",
    "The [TinyLlama repository](https://github.com/jzhang38/TinyLlama/tree/main/sft) includes scripts for fine-tuning. While these will be useful references, we will try to proceed with an approach similar to that used in the gpt2 and t5-small notebooks--purely for the sake of making this notebook a reasonable learning step following those.\n",
    "\n",
    "# The Data\n",
    "We will use the [SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca) dataset. This is a curated subset of the much larger [OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca) dataset. Why this dataset? It's one of the most popular sources of instruction data on Hugging Face, and its size is more manageable than the full OpenOrca dataset. That's all!\n",
    "\n",
    "# 1. Load the model and try some examples\n",
    "\n",
    "We'll begin, as always, by loading the model and trying out some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade -r ./tinyllama_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Environment Setup\n",
    "OUTPUT_DIR = \"../results/TinyLlama/\" # the path to the output directory; where model checkpoints will be saved\n",
    "LOG_DIR = \"../logs/TinyLlama/\" # the path to the log directory; where logs will be saved\n",
    "CACHE_DIR = \"../cache/TinyLlama/\" # the path to the cache directory; where cache files will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_ckpt = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_ckpt,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_ckpt,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Inference\n",
    "def generate(prompt, max_new_tokens=100):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    gen_tokens = model.generate(input_ids, max_new_tokens=max_new_tokens,\n",
    "                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                repetition_penalty=1.1)\n",
    "    return tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "print(generate(\"Here are step-by-step instructions to make a great cup of coffee with a Chemex coffee maker:\\n1.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we structured our prompt with completion in mind: we generated the first part of the full text and asked the model to complete it. What happens if, instead, we ask a question or give an instruction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question\n",
    "print(generate(\"How do I make coffee with a Chemex coffee maker?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction\n",
    "print(generate(\"Tell me how to make coffee with a Chemex coffee maker.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These did not work because the model has not been instruction tuned. Our task is to change that!\n",
    "\n",
    "# 2. Getting and Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "slimorca = load_dataset('Open-Orca/SlimOrca',\n",
    "                           cache_dir=str(Path(CACHE_DIR) / \"data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(slimorca[\"train\"][0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that there are three components to the sample entry:\n",
    "1. A *system message*: this should be familiar if you've used e.g. ChatGPT via the API. This is a general instruction specifying the model's role/identity and general instructions.\n",
    "2. A *human message*: this is the specific instruction passed to the model by a human.\n",
    "3. a *gpt*: this is the AI model's response.\n",
    "\n",
    "So we want to use this dataset to fine-tune the model such that it will respond more like the *gpt* message when given the *system* and *human* messages.\n",
    "\n",
    "# 3. Formatting the Data\n",
    "\n",
    "First, we need to get these entries into a format we can actually use for fine-tuning. [Appendix A](#Appendix-A:-Looking-at-the-tinyllama-fine-tuning-code) digs into the fine tuning code from the TinyLlama repo to see how the authors handled data formatting. We're going to take a slightly different approach and use the [chat model templates](https://huggingface.co/docs/transformers/main/en/chat_templating#templates-for-chat-models) from the Transformers library. The Hugging Face docs recommend applying the chat templates as a preprocessing step. Let's take a look at how they work.\n",
    "\n",
    "## Transformers Chat Templates\n",
    "Chat Templates are attributes of tokenizers. If a chat template isn't set explicitly, the default template for that model class is used. Let's see if there is a chat template set here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.chat_template), print(tokenizer.default_chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no chat template defined for this tokenizer, so we'll use the default LlamaTokenizerFast class default template. To populate this template, we apply the template to this standard chat format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant and an expert at making coffee.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I make coffee with a Chemex coffee maker?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"To make coffee with a Chemex:\\n1. Boil water to about 200°F (93°C).\\n2. Place the Chemex filter in the top and rinse it with hot water to remove paper taste and warm the vessel. Discard the rinse water.\\n3. Add coffee grounds to the filter. Use a medium-coarse grind, about 1 gram of coffee per 16 grams of water.\\n4. Pour just enough hot water to saturate the grounds. Wait 30 seconds for the coffee to 'bloom'.\\n5. Slowly pour the remaining water over the grounds in a circular motion. Aim for a total brew time of 3.5 to 4.5 minutes.\\n6. Once brewing is complete, remove the filter and enjoy.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because we're trying to train on input/output pairs (following the TinyLlama fine tuning code examples), we'll split this at the `[/INST]` and have the input as everything up to and including the `[/INST]` and the output as everything after (including the space!). Let's write a method to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# configure the model and tokenizer with chat tokens\n",
    "# Add the instruction tokens to the tokenizer\n",
    "special_tokens = [\"[INST]\", \"[/INST]\", \"<<SYS>>\", \"<</SYS>>\"]\n",
    "# Adding special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "# Update the model's embeddings accordingly\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def format_slimorca(ex, tokenizer, input_max_length=512, output_max_length=512):\n",
    "    role_mapping = {\"gpt\": \"assistant\", \"system\": \"system\", \"human\": \"user\"}\n",
    "    chat = [\n",
    "        {\"role\": role_mapping[message[\"from\"]], \"content\": message[\"value\"]}\n",
    "        for message in ex[\"conversations\"]\n",
    "    ]\n",
    "    fmt_chat = tokenizer.apply_chat_template(\n",
    "        chat, tokenize=True, add_generation_prompt=False,\n",
    "    )\n",
    "    inst_token_id = tokenizer.encode(\"[/INST]\")[1]\n",
    "    split_index = fmt_chat.index(inst_token_id) + 1\n",
    "    input_ids = fmt_chat[:split_index]\n",
    "    output_ids = fmt_chat[split_index:]\n",
    "\n",
    "    # Apply separate padding/truncation for input and output\n",
    "    input_ids = torch.tensor(input_ids[:input_max_length] + [tokenizer.pad_token_id] * max(0, input_max_length - len(input_ids)))\n",
    "    output_ids = torch.tensor(output_ids[:output_max_length] + [tokenizer.pad_token_id] * max(0, output_max_length - len(output_ids)))\n",
    "\n",
    "    return input_ids, output_ids\n",
    "\n",
    "# Map to the dataset\n",
    "slimorca_tokenized = slimorca.map(\n",
    "    lambda ex: {\n",
    "        \"input_ids\": format_slimorca(ex, tokenizer)[0],\n",
    "        \"labels\": format_slimorca(ex, tokenizer)[1],\n",
    "    }, num_proc=32\n",
    ").remove_columns(\"conversations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slimorca_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(\"Length of input_ids:\", len(slimorca_tokenized[\"train\"][i]['input_ids']))\n",
    "    print(\"Length of labels:\", len(slimorca_tokenized[\"train\"][i]['labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect one example\n",
    "tokenizer.decode(slimorca_tokenized[\"train\"][25]['input_ids'])\n",
    "tokenizer.decode(slimorca_tokenized[\"train\"][25]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split the tokenized dataset into training and validation sets\n",
    "slimorca_tokenized_split = slimorca_tokenized['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "slimorca_tokenized_split[\"train\"] = slimorca_tokenized_split[\"train\"]\n",
    "slimorca_tokenized_split[\"test\"] = slimorca_tokenized_split[\"test\"]\n",
    "\n",
    "# Format the split datasets into a DatasetDict for compatibility with Hugging Face's Trainer\n",
    "slimorca_tokenized_split = DatasetDict(\n",
    "    {\n",
    "        \"train\": slimorca_tokenized_split[\"train\"],\n",
    "        \"valid\": slimorca_tokenized_split[\"test\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "slimorca_tokenized_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the [gpt2 example](../2_gpt2_single_gpu/2.%20GPT2%20on%20a%20single%20GPU.ipynb), we will configure a *collator*. We can take some inspiration from the [collator defined in the TinyLlama fine-tuning script](https://github.com/jzhang38/TinyLlama/blob/11a02ce085c1670bd009e6d4385701ff06a7f6cf/sft/finetune.py#L252C19-L252C19)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import mlflow\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32, \n",
    "    per_device_eval_batch_size=4,\n",
    "    auto_find_batch_size=True, \n",
    "    warmup_steps=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=LOG_DIR,\n",
    "    logging_steps=25,  # Log every 25 steps\n",
    "    evaluation_strategy=\"steps\",  # Evaluate every 'eval_steps'\n",
    "    eval_steps=5000,\n",
    "    bf16=True,\n",
    "    #fp16=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=False,\n",
    "    #optim=\"adamw_bnb_8bit\",\n",
    "    save_steps=10000\n",
    ")\n",
    "\n",
    "training_args.set_logging(report_to=[\"mlflow\"],\n",
    "                          steps=50,\n",
    "                          level=\"info\")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=slimorca_tokenized_split[\"train\"],\n",
    "    eval_dataset=slimorca_tokenized_split[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training and track with MLflow\n",
    "with mlflow.start_run(log_system_metrics=True):\n",
    "    trainer.train()\n",
    "    mlflow.log_params(training_args.to_dict())\n",
    "\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load the Fine-Tuned Model Checkpoint and Run some Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def load_latest_checkpoint(output_dir = OUTPUT_DIR,\n",
    "                           default_tokenizer=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"):\n",
    "    checkpoint_dir = max(\n",
    "        [d for d in next(os.walk(output_dir))[1] if re.match(r\"checkpoint-\\d+\", d)],\n",
    "        key=lambda d: int(d.split(\"-\")[-1]),\n",
    "    )\n",
    "    path = os.path.join(output_dir, checkpoint_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(path,\n",
    "                                                  device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        path\n",
    "        if os.path.exists(os.path.join(path, \"tokenizer_config.json\"))\n",
    "        else default_tokenizer\n",
    "    )\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft, tokenizer_ft = load_latest_checkpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant and an expert at making coffee.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I make coffee with a Chemex coffee maker?\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(prompt,\n",
    "                                       tokenize=False, add_generation_prompt=False)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the fine-tuned model\n",
    "\n",
    "The first version of this performed very badly. For example:\n",
    "\n",
    "```\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant and an expert at making coffee.\n",
    "<</SYS>>\n",
    "\n",
    "How do I make coffee with a Chemex coffee maker? [/INST]\n",
    "\n",
    "How can I make coffee with a Chemex coffee maker? [/INST]\n",
    "\n",
    "Please tell me if those questions are the same.\n",
    "Choose from:\n",
    "(a). no;\n",
    "(b). yes; [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining Changes\n",
    "1. add the instruction-related tokens as special tokens to the tokenizer\n",
    "2. increase the context size to 512 in / 512 out (from 512/256)\n",
    "3. increate gradient_accumulation_steps to 4\n",
    "4. add auto_find_batch_size=True\n",
    "5. change attention to flashattention2\n",
    "6. use regular adamw.\n",
    "7. use bf16 instead of fp16\n",
    "\n",
    "## Questions\n",
    "1. What enabled me to use regular adamw instead of adamw_bnb_8bit? Was it loading the model in bf16? Did flashattention2 make that big of a difference? Something else?\n",
    "2. How can I determine the batch size auto_find_batch_size landed on?\n",
    "3. I actually still have some vram to work with. How can I make the most use of it?\n",
    "\n",
    "Perhaps most importantly...we're going to try out the checkpoints along the way instead of training for more than a full day before testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model after the second fine-tuning attempt\n",
    "\n",
    "I tried out one of the model checkpoints during the fine-tuning process and it was performing better than the end result of the first fine-tuning attempt. It still didn't know *how* to make coffee with a Chemex coffee maker, but, well, it's a small model. What's important is that it took a question for a prompt and it responded with a more-or-less coherent answer.\n",
    "\n",
    "However—and here's another lesson learned about the perils of running everything from notebooks—I initialized the training run by running a whole notebook. A notebook that happened to have a cleanup script intended to delete intermediary checkpoints. So, yeah, I finished a 14.5 hour training run and deleted the results.\n",
    "\n",
    "On to round 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Next Steps\n",
    "\n",
    "This fine-tuning process pushed the limits of what we could accomplish on a single GPU. And it makes sense: our back-of-the-envelope calculations said that we would require *at least* 20GB of VRAM, before we even think about storing activations or scaling sequence lengths or batch sizes.\n",
    "\n",
    "We got around this in part by using a smaller sequence length than that shown in the tinyllama fine-tuning script. The biggest change we made was to use the `adamw_bnb_8bit` optimizer from the bitsandbytes library. The point is that we are running up against the limits of what we can reasonably accomplish with a single GPU, at least without more sophisticated approaches. So what's next? There are several directions we can pursue (and we can and should pursue them all):   \n",
    "1. Try to further optimize training this model on a single GPU. What can we do to make the training process run faster and more effectively? Can we find an approach that will still let us the normal `adamw` optimizer? Can we benefit from using e.g. [Deepspeed ZeRO](https://huggingface.co/docs/transformers/perf_train_gpu_one#deepspeed-zero)?\n",
    "2. Try to fine-tune this model on a multi-GPU setup. What are the benefits in terms of speed and ability to train larger batches and larger sequence lengths? And, perhaps more importantly in this setting, how do we make the leap from a single GPU to a multi-GPU setup?\n",
    "3. Train a bigger model! So far we have fine-tuned t5-small, gpt2, and tinyllama, with each subsequent model larger than the last. We ultimately want to work our way up to even larger models, so after this, it might be time to train a 3B parameter model, and then a 7B parameter model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A: Looking at the tinyllama fine-tuning code\n",
    "\n",
    "You can fine the tinyllama fine-tuning code [here](https://github.com/jzhang38/TinyLlama/tree/main/sft). It's worth the time, at this phase of learning about fine-tuning, to read through it and learn about some of the approaches they use.\n",
    "\n",
    "Let's first take a look at the [train()](https://github.com/jzhang38/TinyLlama/blob/11a02ce085c1670bd009e6d4385701ff06a7f6cf/sft/finetune.py#L492) method. It begins by using the [`HFArgumentParser`](https://huggingface.co/docs/transformers/v4.36.1/en/internal/trainer_utils#transformers.HfArgumentParser) to configure the training arguments. Earlier, the training code defined a number of `dataclass`es for e.g. training arguments, data arguments, etc. `HFArgumentParser` provides an approach for parsing command line arguments directly into instances of these dataclass types. So instead of simply defining arguments in notebook cells, as we've been doing, this approach provides a structured way to parse command line arguments. And, indeed, the repo provides a [shell script](https://github.com/jzhang38/TinyLlama/blob/main/sft/script.sh) for running the fine-tuning script with a defined set of arguments.\n",
    "\n",
    "The [next major section](https://github.com/jzhang38/TinyLlama/blob/11a02ce085c1670bd009e6d4385701ff06a7f6cf/sft/finetune.py#L514C3-L514C3) of the training script is focused on preparing the data, using the [`make_data_module`](https://github.com/jzhang38/TinyLlama/blob/11a02ce085c1670bd009e6d4385701ff06a7f6cf/sft/finetune.py#L354) method defined earlier in the script. That method is set up to handle a few different potential fine-tuning data sources (slimorca is not included among them). It maps each of them to the expected format: an input string and an output string.\n",
    "\n",
    "I found the handling of alpaca-formatted datasets instructive. The [alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca?row=0) dataset includes instructions and optional inputs that follow a specified format. For examples with inputs, the format is:\n",
    "\n",
    "```\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "```\n",
    "\n",
    "The following code snippet in the TinyLlama repo handles this formatting (in the alpaca dataset, the inputs/outputs are not pre-formatted)\n",
    "\n",
    "```python\n",
    "ALPACA_PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response: \"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response: \"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def extract_alpaca_dataset(example):\n",
    "    if example.get(\"input\", \"\") != \"\":\n",
    "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_input\"]\n",
    "    else:\n",
    "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_no_input\"]\n",
    "    return {'input': prompt_format.format(**example)}\n",
    "```\n",
    "\n",
    "We're seeing some repeating patterns across training scripts (the examples so far in this repo and the TinyLlama code). Each fine-tuning run so far requires the following:\n",
    "- process the data\n",
    "- set up training arguments\n",
    "- set up logging\n",
    "\n",
    "An additional step, as we get to multi-gpu and multi-node setups, will be configuring devices and processses—-see the [script.sh](https://github.com/jzhang38/TinyLlama/blob/main/sft/script.sh) shell script from TinyLlama for an example, which uses [accelerate launch](https://huggingface.co/docs/accelerate/basic_tutorials/launch), a helper command that makes it easier to launch training scripts on different hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B: Resuming from a Checkpoint\n",
    "\n",
    "I made a few mistakes in terms of handling checkpoints. In one of those cases, I saved checkpoints and assumed a final model would also be saved. This was not the case. So I had a checkpoint at step 20,000 out of 29,000. In this case, instead of starting over, it made more sense to load the checkpoint and finish training. To do so with the Hugging Face trainer, we can:\n",
    "\n",
    "1. Load the desired model checkpoint with e.g. \n",
    "```\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/path/to/checkpoint-20000\",\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "```\n",
    "\n",
    "Also make sure the tokenizer is loaded.\n",
    "2. After configuring the trainer/training arguments as before, call `trainer.train` with the `resume_from_checkpoint` argument set to the desired checkpoint.\n",
    "```\n",
    "trainer.train(resume_from_checkpoint=\"/path/to/checkpoint-20000\")\n",
    "```\n",
    "\n",
    "The training will then pick up at step 20,000. And then you can make sure to save the final model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
