{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The [TinyLlama](https://github.com/jzhang38/TinyLlama) project \"aims to pretrain a 1.1B Llama model on 3 trillion tokens.\" 1.1B tokens represents a considerable step up from the small GPT model we [previously fine-tuned](../2_gpt2_single_gpu/2.%20GPT2%20on%20a%20single%20GPU.ipynb). That model had 124M parameters; TinyLlama, while still small by the standards of most widely-used LLMs, is almost ten times the size. We will need around 20GB VRAM at a bare minimum to fine-tune this model.\n",
    "\n",
    "## Instruction Tuning\n",
    "We are going to focus on instruction tuning in this example. Instruction Tuning is a supervised learning technique in which we train the model on instruction/output pairs with the goal of training the model to follow human instructions. Before instruction tuning, the base model is trained on next-token completion. We saw this in the GPT2 example: we provided the start of a story and the model completed it. An instruction-tuned model, on the other hand, is trained to answer a question or instruction.\n",
    "\n",
    "[This repository](https://github.com/xiaoya-li/Instruction-Tuning-Survey) contains a wealth of information on the current state of the field of instruction tuning.\n",
    "\n",
    "The [TinyLlama repository](https://github.com/jzhang38/TinyLlama/tree/main/sft) includes scripts for fine-tuning. While these will be useful references, we will try to proceed with an approach similar to that used in the gpt2 and t5-small notebooks--purely for the sake of making this notebook a reasonable learning step following those.\n",
    "\n",
    "# The Data\n",
    "We will use the [SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca) dataset. This is a curated subset of the much larger [OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca) dataset. Why this dataset? It's one of the most popular sources of instruction data on Hugging Face, and its size is more manageable than the full OpenOrca dataset. That's all!\n",
    "\n",
    "# 1. Load the model and try some examples\n",
    "\n",
    "We'll begin, as always, by loading the model and trying out some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade -r ./tinyllama_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Environment Setup\n",
    "OUTPUT_DIR = \"../results/TinyLlama/\" # the path to the output directory; where model checkpoints will be saved\n",
    "LOG_DIR = \"../logs/TinyLlama/\" # the path to the log directory; where logs will be saved\n",
    "CACHE_DIR = \"../cache/TinyLlama/\" # the path to the cache directory; where cache files will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are step-by-step instructions to make a great cup of coffee with a Chemex coffee maker:\n",
      "1. Prepare the grounds\n",
      "2. Add water\n",
      "3. Grind the beans\n",
      "4. Pour the water into the pot\n",
      "5. Let it brew\n",
      "6. Enjoy your coffee!\n",
      "What is a Chemex?\n",
      "A Chemex is a type of coffee maker that uses a ceramic filter to extract the flavor from the coffee beans. The Chemex was invented by a man named John C. Hunt in 1908\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_ckpt = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_ckpt,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_ckpt,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Inference\n",
    "def generate(prompt, max_new_tokens=100):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    gen_tokens = model.generate(input_ids, max_new_tokens=max_new_tokens,\n",
    "                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                repetition_penalty=1.1)\n",
    "    return tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "print(generate(\"Here are step-by-step instructions to make a great cup of coffee with a Chemex coffee maker:\\n1.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we structured our prompt with completion in mind: we generated the first part of the full text and asked the model to complete it. What happens if, instead, we ask a question or give an instruction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I make coffee with a Chemex coffee maker?\n",
      "How do you make coffee with a Chemex?\n",
      "What is the best way to brew coffee with a Chemex?\n",
      "How do you make coffee with a Chemex filter?\n",
      "How do you make coffee with a Chemex filter and a Chemex?\n",
      "How do you make coffee with a Chemex filter and a Chemex filter?\n",
      "How do you make coffee with a Chemex filter and a Chemex filter?\n",
      "How do you make coffee with a Chemex filter and\n"
     ]
    }
   ],
   "source": [
    "# Question\n",
    "print(generate(\"How do I make coffee with a Chemex coffee maker?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me how to make coffee with a Chemex coffee maker.\n",
      "I'm not sure if you can get the Chemex in the UK, but I think it's worth a try.\n",
      "The Chemex is a great coffee maker and I love mine!\n",
      "I have one of these and I love it. It makes a great cup of coffee every time.\n",
      "I have a Chemex and I love it. It makes a great cup of coffee every time.\n",
      "I have a Chemex and I love it. It makes a great cup\n"
     ]
    }
   ],
   "source": [
    "# Instruction\n",
    "print(generate(\"Tell me how to make coffee with a Chemex coffee maker.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These did not work because the model has not been instruction tuned. Our task is to change that!\n",
    "\n",
    "# 2. Getting and Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ededdc76a920457cbb46ce9055a17ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/2.15k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4f51654556480a849519782f16303c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "507a5f9a1f604216bc27bbc59e72c433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/986M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd45180e0904c19bc7d3a43a4066317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e36025fd051e4e4b87e034e8650c1824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "slimorca = load_dataset('Open-Orca/SlimOrca',\n",
    "                           cache_dir=str(Path(CACHE_DIR) / \"data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"conversations\": [\n",
      "        {\n",
      "            \"from\": \"system\",\n",
      "            \"value\": \"You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\",\n",
      "            \"weight\": null\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"human\",\n",
      "            \"value\": \"Write an article based on this \\\"A man has been charged with murder and attempted murder after a woman and the man she was on a date with were stabbed at a restaurant in Sydney, Australia.\\\"\",\n",
      "            \"weight\": 0.0\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"gpt\",\n",
      "            \"value\": \"Title: Tragedy Strikes in Sydney: Victims Stabbed during Dinner Date, Suspect Charged with Murder and Attempted Murder\\n\\nIntroduction\\n\\nA seemingly ordinary evening took a tragic turn in Sydney, Australia when a romantic dinner date at a local restaurant ended in bloodshed, as a man allegedly attacked a couple, stabbing both victims. Local authorities have charged the suspect with murder and attempted murder in connection with the horrific crime. In this article, we delve into the chilling incident, the suspect, and the impact it has had on the community.\\n\\nThe Incident\\n\\nThe alarming event took place at a popular restaurant in Sydney, Australia. A woman and her date were enjoying their meal \\u2013 blissfully unaware of the tragic circumstances that would soon unfold. As they shared conversation and indulged in their dinner, a man approached the couple and, without warning, began to stab them with a sharp object.\\n\\nPatrons at the restaurant scrambled for safety, as the assailant continued his violent rampage. Bystanders quickly called emergency services, who promptly arrived at the scene. Although both victims were rushed to a nearby hospital, the woman succumbed to her injuries. The man she was on a date with remained in critical condition, fighting for his life.\\n\\nThe Suspect\\n\\nInitial investigations revealed that the alleged attacker had no known connections to the victims \\u2013 adding to the mystifying nature of this sudden and brutal assault. Witnesses reported that the assailant seemed to have no specific motive and appeared to carry out the act senselessly.\\n\\nFollowing a thorough investigation, local police identified and arrested the suspect. During the process, it was discovered that the alleged attacker had a history of criminal behavior and a troubled past, though it is unclear if this played a role in the tragic incident.\\n\\nAuthorities have formally charged the man with murder and attempted murder in connection with the heinous crime. He awaits a hearing to determine a trial date and, if convicted, could face a life sentence in prison.\\n\\nThe Community's Response\\n\\nThe shocking nature of the crime has left the residents of Sydney reeling, as they struggle to come to terms with the harrowing event. The restaurant where the attack occurred has since been closed, with a makeshift memorial being created outside to commemorate the victims.\\n\\nMany have questioned how such a vicious crime could happen in what is considered to be one of the safest cities in the world. This tragic event has spurred local officials to reassess current security measures and devise strategies to reinforce public safety. Additionally, discussions surrounding mental health and criminal rehabilitation have surfaced as residents seek to comprehend the actions of the alleged perpetrator and prevent similar incidents from occurring in the future.\\n\\nIn the wake of the stabbing, the community has banded together with an outpouring of grief and support for the victims and their families. Candlelight vigils have been held, and an online fundraising campaign is underway to assist the surviving victim with his medical expenses and recovery.\\n\\nConclusion\\n\\nThe tragic attack in Sydney serves as a chilling reminder that senseless acts of violence can happen anywhere and at any time. The community's response to this horrific and seemingly random act of brutality has been one of solidarity and determination to prevent such incidents in the future. As the case unfolds, the victims and their families remain in the hearts of the community, who are grieving the devastating loss of a life cut tragically short and supporting the recovering victim as he continues to endure this unimaginable ordeal.\",\n",
      "            \"weight\": 1.0\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(slimorca[\"train\"][0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that there are three components to the sample entry:\n",
    "1. A *system message*: this should be familiar if you've used e.g. ChatGPT via the API. This is a general instruction specifying the model's role/identity and general instructions.\n",
    "2. A *human message*: this is the specific instruction passed to the model by a human.\n",
    "3. a *gpt*: this is the AI model's response.\n",
    "\n",
    "So we want to use this dataset to fine-tune the model such that it will respond more like the *gpt* message when given the *system* and *human* messages.\n",
    "\n",
    "# 3. Formatting the Data\n",
    "\n",
    "First, we need to get these entries into a format we can actually use for fine-tuning. [Appendix A](#Appendix-A:-Looking-at-the-tinyllama-fine-tuning-code) digs into the fine tuning code from the TinyLlama repo to see how the authors handled data formatting. We're going to take a slightly different approach and use the [chat model templates](https://huggingface.co/docs/transformers/main/en/chat_templating#templates-for-chat-models) from the Transformers library. The Hugging Face docs recommend applying the chat templates as a preprocessing step. Let's take a look at how they work.\n",
    "\n",
    "## Transformers Chat Templates\n",
    "Chat Templates are attributes of tokenizers. If a chat template isn't set explicitly, the default template for that model class is used. Let's see if there is a chat template set here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\'t know the answer to a question, please don\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\n' + content.strip() + '\\n<</SYS>>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.chat_template), print(tokenizer.default_chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no chat template defined for this tokenizer, so we'll use the default LlamaTokenizerFast class default template. To populate this template, we apply the template to this standard chat format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant and an expert at making coffee.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I make coffee with a Chemex coffee maker?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"To make coffee with a Chemex:\\n1. Boil water to about 200°F (93°C).\\n2. Place the Chemex filter in the top and rinse it with hot water to remove paper taste and warm the vessel. Discard the rinse water.\\n3. Add coffee grounds to the filter. Use a medium-coarse grind, about 1 gram of coffee per 16 grams of water.\\n4. Pour just enough hot water to saturate the grounds. Wait 30 seconds for the coffee to 'bloom'.\\n5. Slowly pour the remaining water over the grounds in a circular motion. Aim for a total brew time of 3.5 to 4.5 minutes.\\n6. Once brewing is complete, remove the filter and enjoy.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a helpful assistant and an expert at making coffee.\n",
      "<</SYS>>\n",
      "\n",
      "How do I make coffee with a Chemex coffee maker? [/INST] To make coffee with a Chemex:\n",
      "1. Boil water to about 200°F (93°C).\n",
      "2. Place the Chemex filter in the top and rinse it with hot water to remove paper taste and warm the vessel. Discard the rinse water.\n",
      "3. Add coffee grounds to the filter. Use a medium-coarse grind, about 1 gram of coffee per 16 grams of water.\n",
      "4. Pour just enough hot water to saturate the grounds. Wait 30 seconds for the coffee to 'bloom'.\n",
      "5. Slowly pour the remaining water over the grounds in a circular motion. Aim for a total brew time of 3.5 to 4.5 minutes.\n",
      "6. Once brewing is complete, remove the filter and enjoy. </s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because we're trying to train on input/output pairs (following the TinyLlama fine tuning code examples), we'll split this at the `[/INST]` and have the input as everything up to and including the `[/INST]` and the output as everything after (including the space!). Let's write a method to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'from': 'gpt',\n",
       " 'value': \"Title: Tragedy Strikes in Sydney: Victims Stabbed during Dinner Date, Suspect Charged with Murder and Attempted Murder\\n\\nIntroduction\\n\\nA seemingly ordinary evening took a tragic turn in Sydney, Australia when a romantic dinner date at a local restaurant ended in bloodshed, as a man allegedly attacked a couple, stabbing both victims. Local authorities have charged the suspect with murder and attempted murder in connection with the horrific crime. In this article, we delve into the chilling incident, the suspect, and the impact it has had on the community.\\n\\nThe Incident\\n\\nThe alarming event took place at a popular restaurant in Sydney, Australia. A woman and her date were enjoying their meal – blissfully unaware of the tragic circumstances that would soon unfold. As they shared conversation and indulged in their dinner, a man approached the couple and, without warning, began to stab them with a sharp object.\\n\\nPatrons at the restaurant scrambled for safety, as the assailant continued his violent rampage. Bystanders quickly called emergency services, who promptly arrived at the scene. Although both victims were rushed to a nearby hospital, the woman succumbed to her injuries. The man she was on a date with remained in critical condition, fighting for his life.\\n\\nThe Suspect\\n\\nInitial investigations revealed that the alleged attacker had no known connections to the victims – adding to the mystifying nature of this sudden and brutal assault. Witnesses reported that the assailant seemed to have no specific motive and appeared to carry out the act senselessly.\\n\\nFollowing a thorough investigation, local police identified and arrested the suspect. During the process, it was discovered that the alleged attacker had a history of criminal behavior and a troubled past, though it is unclear if this played a role in the tragic incident.\\n\\nAuthorities have formally charged the man with murder and attempted murder in connection with the heinous crime. He awaits a hearing to determine a trial date and, if convicted, could face a life sentence in prison.\\n\\nThe Community's Response\\n\\nThe shocking nature of the crime has left the residents of Sydney reeling, as they struggle to come to terms with the harrowing event. The restaurant where the attack occurred has since been closed, with a makeshift memorial being created outside to commemorate the victims.\\n\\nMany have questioned how such a vicious crime could happen in what is considered to be one of the safest cities in the world. This tragic event has spurred local officials to reassess current security measures and devise strategies to reinforce public safety. Additionally, discussions surrounding mental health and criminal rehabilitation have surfaced as residents seek to comprehend the actions of the alleged perpetrator and prevent similar incidents from occurring in the future.\\n\\nIn the wake of the stabbing, the community has banded together with an outpouring of grief and support for the victims and their families. Candlelight vigils have been held, and an online fundraising campaign is underway to assist the surviving victim with his medical expenses and recovery.\\n\\nConclusion\\n\\nThe tragic attack in Sydney serves as a chilling reminder that senseless acts of violence can happen anywhere and at any time. The community's response to this horrific and seemingly random act of brutality has been one of solidarity and determination to prevent such incidents in the future. As the case unfolds, the victims and their families remain in the hearts of the community, who are grieving the devastating loss of a life cut tragically short and supporting the recovering victim as he continues to endure this unimaginable ordeal.\",\n",
       " 'weight': 1.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = slimorca[\"train\"][0]\n",
    "ex['conversations'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67a2ea2d43f467d8b58f75eee63d289",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/517982 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def format_slimorca(ex, tokenizer):\n",
    "    role_mapping = {\"gpt\": \"assistant\", \"system\": \"system\", \"human\": \"user\"}\n",
    "    chat = [\n",
    "        {\"role\": role_mapping[message[\"from\"]], \"content\": message[\"value\"]}\n",
    "        for message in ex[\"conversations\"]\n",
    "    ]\n",
    "    fmt_chat = tokenizer.apply_chat_template(\n",
    "        chat, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    split_chat = fmt_chat.split(\"[/INST]\")\n",
    "    return split_chat[0] + \"[/INST]\", split_chat[1]\n",
    "\n",
    "\n",
    "# Map to the dataset\n",
    "slimorca[\"train\"] = slimorca[\"train\"].map(\n",
    "    lambda ex: {\n",
    "        \"input\": format_slimorca(ex, tokenizer)[0],\n",
    "        \"output\": format_slimorca(ex, tokenizer)[1],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '<s>[INST] <<SYS>>\\nYou are an AI assistant. You will be given a task. You must generate a detailed and long answer.\\n<</SYS>>\\n\\nWrite an article based on this \"A man has been charged with murder and attempted murder after a woman and the man she was on a date with were stabbed at a restaurant in Sydney, Australia.\" [/INST]',\n",
       " 'output': \" Title: Tragedy Strikes in Sydney: Victims Stabbed during Dinner Date, Suspect Charged with Murder and Attempted Murder\\n\\nIntroduction\\n\\nA seemingly ordinary evening took a tragic turn in Sydney, Australia when a romantic dinner date at a local restaurant ended in bloodshed, as a man allegedly attacked a couple, stabbing both victims. Local authorities have charged the suspect with murder and attempted murder in connection with the horrific crime. In this article, we delve into the chilling incident, the suspect, and the impact it has had on the community.\\n\\nThe Incident\\n\\nThe alarming event took place at a popular restaurant in Sydney, Australia. A woman and her date were enjoying their meal – blissfully unaware of the tragic circumstances that would soon unfold. As they shared conversation and indulged in their dinner, a man approached the couple and, without warning, began to stab them with a sharp object.\\n\\nPatrons at the restaurant scrambled for safety, as the assailant continued his violent rampage. Bystanders quickly called emergency services, who promptly arrived at the scene. Although both victims were rushed to a nearby hospital, the woman succumbed to her injuries. The man she was on a date with remained in critical condition, fighting for his life.\\n\\nThe Suspect\\n\\nInitial investigations revealed that the alleged attacker had no known connections to the victims – adding to the mystifying nature of this sudden and brutal assault. Witnesses reported that the assailant seemed to have no specific motive and appeared to carry out the act senselessly.\\n\\nFollowing a thorough investigation, local police identified and arrested the suspect. During the process, it was discovered that the alleged attacker had a history of criminal behavior and a troubled past, though it is unclear if this played a role in the tragic incident.\\n\\nAuthorities have formally charged the man with murder and attempted murder in connection with the heinous crime. He awaits a hearing to determine a trial date and, if convicted, could face a life sentence in prison.\\n\\nThe Community's Response\\n\\nThe shocking nature of the crime has left the residents of Sydney reeling, as they struggle to come to terms with the harrowing event. The restaurant where the attack occurred has since been closed, with a makeshift memorial being created outside to commemorate the victims.\\n\\nMany have questioned how such a vicious crime could happen in what is considered to be one of the safest cities in the world. This tragic event has spurred local officials to reassess current security measures and devise strategies to reinforce public safety. Additionally, discussions surrounding mental health and criminal rehabilitation have surfaced as residents seek to comprehend the actions of the alleged perpetrator and prevent similar incidents from occurring in the future.\\n\\nIn the wake of the stabbing, the community has banded together with an outpouring of grief and support for the victims and their families. Candlelight vigils have been held, and an online fundraising campaign is underway to assist the surviving victim with his medical expenses and recovery.\\n\\nConclusion\\n\\nThe tragic attack in Sydney serves as a chilling reminder that senseless acts of violence can happen anywhere and at any time. The community's response to this horrific and seemingly random act of brutality has been one of solidarity and determination to prevent such incidents in the future. As the case unfolds, the victims and their families remain in the hearts of the community, who are grieving the devastating loss of a life cut tragically short and supporting the recovering victim as he continues to endure this unimaginable ordeal. </s>\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slimorca = slimorca.remove_columns(\n",
    "            [col for col in slimorca.column_names['train'] if col not in ['input', 'output']]\n",
    "        )\n",
    "\n",
    "slimorca[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will tokenize the inputs and the outputs. We could combine this with the above step, but we'll leave it separate for transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30720e09fa344817bbe67de37537e965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/517982 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(examples[\"input\"], padding=True, truncation=True, max_length=1024)[\"input_ids\"],\n",
    "        \"labels\": tokenizer(examples[\"output\"], padding=True, truncation=True, max_length=256)[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "slimorca_tokenized = slimorca.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_ids: 1024\n",
      "Length of labels: 256\n",
      "Length of input_ids: 1024\n",
      "Length of labels: 256\n",
      "Length of input_ids: 1024\n",
      "Length of labels: 256\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"Length of input_ids:\", len(slimorca_tokenized[\"train\"][i]['input_ids'])) # Adjust the dataset part ('train') as necessary\n",
    "    print(\"Length of labels:\", len(slimorca_tokenized[\"train\"][i]['labels'])) # Adjust the dataset part ('train') as necessary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the [gpt2 example](../2_gpt2_single_gpu/2.%20GPT2%20on%20a%20single%20GPU.ipynb), we will configure a *collator*. We can take some inspiration from the [collator defined in the TinyLlama fine-tuning script](https://github.com/jzhang38/TinyLlama/blob/11a02ce085c1670bd009e6d4385701ff06a7f6cf/sft/finetune.py#L252C19-L252C19)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'output', 'input_ids', 'labels'],\n",
       "        num_rows: 466183\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input', 'output', 'input_ids', 'labels'],\n",
       "        num_rows: 51799\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split the tokenized dataset into training and validation sets\n",
    "slimorca_tokenized_split = slimorca_tokenized['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "# Format the split datasets into a DatasetDict for compatibility with Hugging Face's Trainer\n",
    "slimorca_tokenized_split = DatasetDict(\n",
    "    {\n",
    "        \"train\": slimorca_tokenized_split[\"train\"],\n",
    "        \"valid\": slimorca_tokenized_split[\"test\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "slimorca_tokenized_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import mlflow\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1, \n",
    "    per_device_eval_batch_size=1, \n",
    "    warmup_steps=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=LOG_DIR,\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    evaluation_strategy=\"steps\",  # Evaluate every 'eval_steps'\n",
    "    eval_steps=10000,\n",
    "    fp16=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=slimorca_tokenized_split[\"train\"],\n",
    "    eval_dataset=slimorca_tokenized_split[\"valid\"],  # Use only the first 5k rows for eval data\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training and track with MLflow\n",
    "with mlflow.start_run(log_system_metrics=True):\n",
    "    trainer.train()\n",
    "    mlflow.log_params(training_args.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A: Looking at the tinyllama fine-tuning code\n",
    "\n",
    "You can fine the tinyllama fine-tuning code [here](https://github.com/jzhang38/TinyLlama/tree/main/sft). It's worth the time, at this phase of learning about fine-tuning, to read through it and learn about some of the approaches they use.\n",
    "\n",
    "Let's first take a look at the [train()](https://github.com/jzhang38/TinyLlama/blob/11a02ce085c1670bd009e6d4385701ff06a7f6cf/sft/finetune.py#L492) method. It begins by using the [`HFArgumentParser`](https://huggingface.co/docs/transformers/v4.36.1/en/internal/trainer_utils#transformers.HfArgumentParser) to configure the training arguments. Earlier, the training code defined a number of `dataclass`es for e.g. training arguments, data arguments, etc. `HFArgumentParser` provides an approach for parsing command line arguments directly into instances of these dataclass types. So instead of simply defining arguments in notebook cells, as we've been doing, this approach provides a structured way to parse command line arguments. And, indeed, the repo provides a [shell script](https://github.com/jzhang38/TinyLlama/blob/main/sft/script.sh) for running the fine-tuning script with a defined set of arguments.\n",
    "\n",
    "The [next major section](https://github.com/jzhang38/TinyLlama/blob/11a02ce085c1670bd009e6d4385701ff06a7f6cf/sft/finetune.py#L514C3-L514C3) of the training script is focused on preparing the data, using the [`make_data_module`](https://github.com/jzhang38/TinyLlama/blob/11a02ce085c1670bd009e6d4385701ff06a7f6cf/sft/finetune.py#L354) method defined earlier in the script. That method is set up to handle a few different potential fine-tuning data sources (slimorca is not included among them). It maps each of them to the expected format: an input string and an output string.\n",
    "\n",
    "I found the handling of alpaca-formatted datasets instructive. The [alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca?row=0) dataset includes instructions and optional inputs that follow a specified format. For examples with inputs, the format is:\n",
    "\n",
    "```\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "```\n",
    "\n",
    "The following code snippet in the TinyLlama repo handles this formatting (in the alpaca dataset, the inputs/outputs are not pre-formatted)\n",
    "\n",
    "```python\n",
    "ALPACA_PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response: \"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response: \"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def extract_alpaca_dataset(example):\n",
    "    if example.get(\"input\", \"\") != \"\":\n",
    "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_input\"]\n",
    "    else:\n",
    "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_no_input\"]\n",
    "    return {'input': prompt_format.format(**example)}\n",
    "```\n",
    "\n",
    "We're seeing some repeating patterns across training scripts (the examples so far in this repo and the TinyLlama code). Each fine-tuning run so far requires the following:\n",
    "- process the data\n",
    "- set up training arguments\n",
    "- set up logging\n",
    "\n",
    "An additional step, as we get to multi-gpu and multi-node setups, will be configuring devices and processses—-see the [script.sh](https://github.com/jzhang38/TinyLlama/blob/main/sft/script.sh) shell script from TinyLlama for an example, which uses [accelerate launch](https://huggingface.co/docs/accelerate/basic_tutorials/launch), a helper command that makes it easier to launch training scripts on different hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
