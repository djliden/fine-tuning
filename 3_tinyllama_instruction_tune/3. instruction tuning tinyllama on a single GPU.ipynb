{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE: WORK IN PROGRESS\n",
    "\n",
    "This is a work in progress and is under active development! The latest version worked quite poorly. I think it's because I split the chat-formatted inputs on the `[INST]` tag *before* tokenizing. The conseqeuence was that the tokenizer added the beginning/end of sequence tokens at the beginnings/endings of both the instruction and the output. I suspect this led to undesirable results.\n",
    "\n",
    "Another thing I'm finding from some testing...the model is extremely sensitive to punctuation! If I give an instruction or a question without punctuation, the results are nonsense.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The [TinyLlama](https://github.com/jzhang38/TinyLlama) project \"aims to pretrain a 1.1B Llama model on 3 trillion tokens.\" 1.1B tokens represents a considerable step up from the small GPT model we [previously fine-tuned](../2_gpt2_single_gpu/2.%20GPT2%20on%20a%20single%20GPU.ipynb). That model had 124M parameters; TinyLlama, while still small by the standards of most widely-used LLMs, is almost ten times the size. We will need around 20GB VRAM at a bare minimum to fine-tune this model.\n",
    "\n",
    "## Instruction Tuning\n",
    "We are going to focus on instruction tuning in this example. Instruction Tuning is a supervised learning technique in which we train the model on instruction/output pairs with the goal of training the model to follow human instructions. Before instruction tuning, the base model is trained on next-token completion. We saw this in the GPT2 example: we provided the start of a story and the model completed it. An instruction-tuned model, on the other hand, is trained to answer a question or instruction.\n",
    "\n",
    "[This repository](https://github.com/xiaoya-li/Instruction-Tuning-Survey) contains a wealth of information on the current state of the field of instruction tuning.\n",
    "\n",
    "The [TinyLlama repository](https://github.com/jzhang38/TinyLlama/tree/main/sft) includes scripts for fine-tuning. While these will be useful references, we will try to proceed with an approach similar to that used in the gpt2 and t5-small notebooks--purely for the sake of making this notebook a reasonable learning step following those.\n",
    "\n",
    "# The Data\n",
    "We will use the [SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca) dataset. This is a curated subset of the much larger [OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca) dataset. Why this dataset? It's one of the most popular sources of instruction data on Hugging Face, and its size is more manageable than the full OpenOrca dataset. That's all!\n",
    "\n",
    "# 1. Load the model and try some examples\n",
    "\n",
    "We'll begin, as always, by loading the model and trying out some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from -r ./tinyllama_requirements.txt (line 1)) (4.37.1)\n",
      "Requirement already satisfied: accelerate in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from -r ./tinyllama_requirements.txt (line 2)) (0.26.1)\n",
      "Requirement already satisfied: psutil in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from -r ./tinyllama_requirements.txt (line 3)) (5.9.8)\n",
      "Requirement already satisfied: pynvml in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from -r ./tinyllama_requirements.txt (line 4)) (11.5.0)\n",
      "Requirement already satisfied: bitsandbytes in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from -r ./tinyllama_requirements.txt (line 5)) (0.42.0)\n",
      "Requirement already satisfied: filelock in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from accelerate->-r ./tinyllama_requirements.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: scipy in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from bitsandbytes->-r ./tinyllama_requirements.txt (line 5)) (1.11.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r ./tinyllama_requirements.txt (line 1)) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r ./tinyllama_requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r ./tinyllama_requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r ./tinyllama_requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r ./tinyllama_requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from requests->transformers->-r ./tinyllama_requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from requests->transformers->-r ./tinyllama_requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from requests->transformers->-r ./tinyllama_requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from requests->transformers->-r ./tinyllama_requirements.txt (line 1)) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->-r ./tinyllama_requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate->-r ./tinyllama_requirements.txt (line 2)) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade -r ./tinyllama_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Environment Setup\n",
    "OUTPUT_DIR = \"../results/TinyLlama/\" # the path to the output directory; where model checkpoints will be saved\n",
    "LOG_DIR = \"../logs/TinyLlama/\" # the path to the log directory; where logs will be saved\n",
    "CACHE_DIR = \"../cache/TinyLlama/\" # the path to the cache directory; where cache files will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are step-by-step instructions to make a great cup of coffee with a Chemex coffee maker:\n",
      "1. Prepare the grounds\n",
      "2. Add water\n",
      "3. Grind the beans\n",
      "4. Pour the water into the pot\n",
      "5. Let it brew\n",
      "6. Enjoy your coffee!\n",
      "What is a Chemex?\n",
      "A Chemex is a type of coffee maker that uses a ceramic filter to extract the flavor from the coffee beans. The Chemex was invented by a man named John C. Hunt in 1908\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_ckpt = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_ckpt,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_ckpt,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Inference\n",
    "def generate(prompt, max_new_tokens=100):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    gen_tokens = model.generate(input_ids, max_new_tokens=max_new_tokens,\n",
    "                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                repetition_penalty=1.1)\n",
    "    return tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "print(generate(\"Here are step-by-step instructions to make a great cup of coffee with a Chemex coffee maker:\\n1.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we structured our prompt with completion in mind: we generated the first part of the full text and asked the model to complete it. What happens if, instead, we ask a question or give an instruction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I make coffee with a Chemex coffee maker?\n",
      "How do you make coffee with a Chemex?\n",
      "What is the best way to brew coffee with a Chemex?\n",
      "How do you make coffee with a Chemex filter?\n",
      "How do you make coffee with a Chemex filter and a Chemex?\n",
      "How do you make coffee with a Chemex filter and a Chemex filter?\n",
      "How do you make coffee with a Chemex filter and a Chemex filter?\n",
      "How do you make coffee with a Chemex filter and\n"
     ]
    }
   ],
   "source": [
    "# Question\n",
    "print(generate(\"How do I make coffee with a Chemex coffee maker?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me how to make coffee with a Chemex coffee maker.\n",
      "I'm not sure if you can get the Chemex in the UK, but I think it's worth a try.\n",
      "The Chemex is a great coffee maker and I love mine!\n",
      "I have one of these and I love it. It makes a great cup of coffee every time.\n",
      "I have a Chemex and I love it. It makes a great cup of coffee every time.\n",
      "I have a Chemex and I love it. It makes a great cup\n"
     ]
    }
   ],
   "source": [
    "# Instruction\n",
    "print(generate(\"Tell me how to make coffee with a Chemex coffee maker.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These did not work because the model has not been instruction tuned. Our task is to change that!\n",
    "\n",
    "# 2. Getting and Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "slimorca = load_dataset('Open-Orca/SlimOrca',\n",
    "                           cache_dir=str(Path(CACHE_DIR) / \"data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"conversations\": [\n",
      "        {\n",
      "            \"from\": \"system\",\n",
      "            \"value\": \"You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\",\n",
      "            \"weight\": null\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"human\",\n",
      "            \"value\": \"Write an article based on this \\\"A man has been charged with murder and attempted murder after a woman and the man she was on a date with were stabbed at a restaurant in Sydney, Australia.\\\"\",\n",
      "            \"weight\": 0.0\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"gpt\",\n",
      "            \"value\": \"Title: Tragedy Strikes in Sydney: Victims Stabbed during Dinner Date, Suspect Charged with Murder and Attempted Murder\\n\\nIntroduction\\n\\nA seemingly ordinary evening took a tragic turn in Sydney, Australia when a romantic dinner date at a local restaurant ended in bloodshed, as a man allegedly attacked a couple, stabbing both victims. Local authorities have charged the suspect with murder and attempted murder in connection with the horrific crime. In this article, we delve into the chilling incident, the suspect, and the impact it has had on the community.\\n\\nThe Incident\\n\\nThe alarming event took place at a popular restaurant in Sydney, Australia. A woman and her date were enjoying their meal \\u2013 blissfully unaware of the tragic circumstances that would soon unfold. As they shared conversation and indulged in their dinner, a man approached the couple and, without warning, began to stab them with a sharp object.\\n\\nPatrons at the restaurant scrambled for safety, as the assailant continued his violent rampage. Bystanders quickly called emergency services, who promptly arrived at the scene. Although both victims were rushed to a nearby hospital, the woman succumbed to her injuries. The man she was on a date with remained in critical condition, fighting for his life.\\n\\nThe Suspect\\n\\nInitial investigations revealed that the alleged attacker had no known connections to the victims \\u2013 adding to the mystifying nature of this sudden and brutal assault. Witnesses reported that the assailant seemed to have no specific motive and appeared to carry out the act senselessly.\\n\\nFollowing a thorough investigation, local police identified and arrested the suspect. During the process, it was discovered that the alleged attacker had a history of criminal behavior and a troubled past, though it is unclear if this played a role in the tragic incident.\\n\\nAuthorities have formally charged the man with murder and attempted murder in connection with the heinous crime. He awaits a hearing to determine a trial date and, if convicted, could face a life sentence in prison.\\n\\nThe Community's Response\\n\\nThe shocking nature of the crime has left the residents of Sydney reeling, as they struggle to come to terms with the harrowing event. The restaurant where the attack occurred has since been closed, with a makeshift memorial being created outside to commemorate the victims.\\n\\nMany have questioned how such a vicious crime could happen in what is considered to be one of the safest cities in the world. This tragic event has spurred local officials to reassess current security measures and devise strategies to reinforce public safety. Additionally, discussions surrounding mental health and criminal rehabilitation have surfaced as residents seek to comprehend the actions of the alleged perpetrator and prevent similar incidents from occurring in the future.\\n\\nIn the wake of the stabbing, the community has banded together with an outpouring of grief and support for the victims and their families. Candlelight vigils have been held, and an online fundraising campaign is underway to assist the surviving victim with his medical expenses and recovery.\\n\\nConclusion\\n\\nThe tragic attack in Sydney serves as a chilling reminder that senseless acts of violence can happen anywhere and at any time. The community's response to this horrific and seemingly random act of brutality has been one of solidarity and determination to prevent such incidents in the future. As the case unfolds, the victims and their families remain in the hearts of the community, who are grieving the devastating loss of a life cut tragically short and supporting the recovering victim as he continues to endure this unimaginable ordeal.\",\n",
      "            \"weight\": 1.0\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(slimorca[\"train\"][0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that there are three components to the sample entry:\n",
    "1. A *system message*: this should be familiar if you've used e.g. ChatGPT via the API. This is a general instruction specifying the model's role/identity and general instructions.\n",
    "2. A *human message*: this is the specific instruction passed to the model by a human.\n",
    "3. a *gpt*: this is the AI model's response.\n",
    "\n",
    "So we want to use this dataset to fine-tune the model such that it will respond more like the *gpt* message when given the *system* and *human* messages.\n",
    "\n",
    "# 3. Formatting the Data\n",
    "\n",
    "First, we need to get these entries into a format we can actually use for fine-tuning. [Appendix A](#Appendix-A:-Looking-at-the-tinyllama-fine-tuning-code) digs into the fine tuning code from the TinyLlama repo to see how the authors handled data formatting. We're going to take a slightly different approach and use the [chat model templates](https://huggingface.co/docs/transformers/main/en/chat_templating#templates-for-chat-models) from the Transformers library. The Hugging Face docs recommend applying the chat templates as a preprocessing step. Let's take a look at how they work.\n",
    "\n",
    "## Transformers Chat Templates\n",
    "Chat Templates are attributes of tokenizers. If a chat template isn't set explicitly, the default template for that model class is used. Let's see if there is a chat template set here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif false == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don\\'t know the answer to a question, please don\\'t share false information.' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\n' + content.strip() + '\\n<</SYS>>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.chat_template), print(tokenizer.default_chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no chat template defined for this tokenizer, so we'll use the default LlamaTokenizerFast class default template. To populate this template, we apply the template to this standard chat format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant and an expert at making coffee.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I make coffee with a Chemex coffee maker?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"To make coffee with a Chemex:\\n1. Boil water to about 200°F (93°C).\\n2. Place the Chemex filter in the top and rinse it with hot water to remove paper taste and warm the vessel. Discard the rinse water.\\n3. Add coffee grounds to the filter. Use a medium-coarse grind, about 1 gram of coffee per 16 grams of water.\\n4. Pour just enough hot water to saturate the grounds. Wait 30 seconds for the coffee to 'bloom'.\\n5. Slowly pour the remaining water over the grounds in a circular motion. Aim for a total brew time of 3.5 to 4.5 minutes.\\n6. Once brewing is complete, remove the filter and enjoy.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a helpful assistant and an expert at making coffee.\n",
      "<</SYS>>\n",
      "\n",
      "How do I make coffee with a Chemex coffee maker? [/INST] To make coffee with a Chemex:\n",
      "1. Boil water to about 200°F (93°C).\n",
      "2. Place the Chemex filter in the top and rinse it with hot water to remove paper taste and warm the vessel. Discard the rinse water.\n",
      "3. Add coffee grounds to the filter. Use a medium-coarse grind, about 1 gram of coffee per 16 grams of water.\n",
      "4. Pour just enough hot water to saturate the grounds. Wait 30 seconds for the coffee to 'bloom'.\n",
      "5. Slowly pour the remaining water over the grounds in a circular motion. Aim for a total brew time of 3.5 to 4.5 minutes.\n",
      "6. Once brewing is complete, remove the filter and enjoy. </s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because we're trying to train on input/output pairs (following the TinyLlama fine tuning code examples), we'll split this at the `[/INST]` and have the input as everything up to and including the `[/INST]` and the output as everything after (including the space!). Let's write a method to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# configure the model and tokenizer with chat tokens\n",
    "# Add the instruction tokens to the tokenizer\n",
    "special_tokens = [\"[INST]\", \"[/INST]\", \"<<SYS>>\", \"<</SYS>>\"]\n",
    "# Adding special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "# Update the model's embeddings accordingly\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def format_slimorca(ex, tokenizer, input_max_length=512, output_max_length=512):\n",
    "    role_mapping = {\"gpt\": \"assistant\", \"system\": \"system\", \"human\": \"user\"}\n",
    "    chat = [\n",
    "        {\"role\": role_mapping[message[\"from\"]], \"content\": message[\"value\"]}\n",
    "        for message in ex[\"conversations\"]\n",
    "    ]\n",
    "    fmt_chat = tokenizer.apply_chat_template(\n",
    "        chat, tokenize=True, add_generation_prompt=False,\n",
    "    )\n",
    "    inst_token_id = tokenizer.encode(\"[/INST]\")[1]\n",
    "    split_index = fmt_chat.index(inst_token_id) + 1\n",
    "    input_ids = fmt_chat[:split_index]\n",
    "    output_ids = fmt_chat[split_index:]\n",
    "\n",
    "    # Apply separate padding/truncation for input and output\n",
    "    input_ids = torch.tensor(input_ids[:input_max_length] + [tokenizer.pad_token_id] * max(0, input_max_length - len(input_ids)))\n",
    "    output_ids = torch.tensor(output_ids[:output_max_length] + [tokenizer.pad_token_id] * max(0, output_max_length - len(output_ids)))\n",
    "\n",
    "    return input_ids, output_ids\n",
    "\n",
    "# Map to the dataset\n",
    "slimorca_tokenized = slimorca.map(\n",
    "    lambda ex: {\n",
    "        \"input_ids\": format_slimorca(ex, tokenizer)[0],\n",
    "        \"labels\": format_slimorca(ex, tokenizer)[1],\n",
    "    }, num_proc=32\n",
    ").remove_columns(\"conversations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'labels'],\n",
       "        num_rows: 517982\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slimorca_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_ids: 512\n",
      "Length of labels: 512\n",
      "Length of input_ids: 512\n",
      "Length of labels: 512\n",
      "Length of input_ids: 512\n",
      "Length of labels: 512\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"Length of input_ids:\", len(slimorca_tokenized[\"train\"][i]['input_ids']))\n",
    "    print(\"Length of labels:\", len(slimorca_tokenized[\"train\"][i]['labels']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The average annual precipitation measured at Seattle-Tacoma International Airport from 1981 to 2010 was 37.49 inches (952 mm).\\n\\nThe answer is 37.49 inches (952 mm). </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect one example\n",
    "#tokenizer.decode(slimorca_tokenized[\"train\"][25]['input_ids'])\n",
    "tokenizer.decode(slimorca_tokenized[\"train\"][25]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Split the tokenized dataset into training and validation sets\n",
    "slimorca_tokenized_split = slimorca_tokenized['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "slimorca_tokenized_split[\"train\"] = slimorca_tokenized_split[\"train\"]\n",
    "slimorca_tokenized_split[\"test\"] = slimorca_tokenized_split[\"test\"]\n",
    "\n",
    "# Format the split datasets into a DatasetDict for compatibility with Hugging Face's Trainer\n",
    "slimorca_tokenized_split = DatasetDict(\n",
    "    {\n",
    "        \"train\": slimorca_tokenized_split[\"train\"],\n",
    "        \"valid\": slimorca_tokenized_split[\"test\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "slimorca_tokenized_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in the [gpt2 example](../2_gpt2_single_gpu/2.%20GPT2%20on%20a%20single%20GPU.ipynb), we will configure a *collator*. We can take some inspiration from the [collator defined in the TinyLlama fine-tuning script](https://github.com/jzhang38/TinyLlama/blob/11a02ce085c1670bd009e6d4385701ff06a7f6cf/sft/finetune.py#L252C19-L252C19)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import mlflow\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32, \n",
    "    per_device_eval_batch_size=4,\n",
    "    auto_find_batch_size=True, \n",
    "    warmup_steps=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=LOG_DIR,\n",
    "    logging_steps=25,  # Log every 25 steps\n",
    "    evaluation_strategy=\"steps\",  # Evaluate every 'eval_steps'\n",
    "    eval_steps=5000,\n",
    "    bf16=True,\n",
    "    #fp16=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=False,\n",
    "    #optim=\"adamw_bnb_8bit\",\n",
    "    save_steps=10000\n",
    ")\n",
    "\n",
    "training_args.set_logging(report_to=[\"mlflow\"],\n",
    "                          steps=50,\n",
    "                          level=\"info\")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=slimorca_tokenized_split[\"train\"],\n",
    "    eval_dataset=slimorca_tokenized_split[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training and track with MLflow\n",
    "with mlflow.start_run(log_system_metrics=True):\n",
    "    trainer.train()\n",
    "    mlflow.log_params(training_args.to_dict())\n",
    "\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load the Fine-Tuned Model Checkpoint and Run some Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def load_latest_checkpoint(output_dir = OUTPUT_DIR,\n",
    "                           default_tokenizer=\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"):\n",
    "    checkpoint_dir = max(\n",
    "        [d for d in next(os.walk(output_dir))[1] if re.match(r\"checkpoint-\\d+\", d)],\n",
    "        key=lambda d: int(d.split(\"-\")[-1]),\n",
    "    )\n",
    "    path = os.path.join(output_dir, checkpoint_dir)\n",
    "    model = AutoModelForCausalLM.from_pretrained(path,\n",
    "                                                  device_map=\"auto\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        path\n",
    "        if os.path.exists(os.path.join(path, \"tokenizer_config.json\"))\n",
    "        else default_tokenizer\n",
    "    )\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft, tokenizer_ft = load_latest_checkpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant and an expert at making coffee.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I make coffee with a Chemex coffee maker?\"},\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(prompt,\n",
    "                                       tokenize=False, add_generation_prompt=False)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the fine-tuned model\n",
    "\n",
    "The first version of this performed very badly. For example:\n",
    "\n",
    "```\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant and an expert at making coffee.\n",
    "<</SYS>>\n",
    "\n",
    "How do I make coffee with a Chemex coffee maker? [/INST]\n",
    "\n",
    "How can I make coffee with a Chemex coffee maker? [/INST]\n",
    "\n",
    "Please tell me if those questions are the same.\n",
    "Choose from:\n",
    "(a). no;\n",
    "(b). yes; [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST] [/INST]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining Changes\n",
    "1. add the instruction-related tokens as special tokens to the tokenizer\n",
    "2. increase the context size to 512 in / 512 out (from 512/256)\n",
    "3. increate gradient_accumulation_steps to 4\n",
    "4. add auto_find_batch_size=True\n",
    "5. change attention to flashattention2\n",
    "6. use regular adamw.\n",
    "7. use bf16 instead of fp16\n",
    "\n",
    "## Questions\n",
    "1. What enabled me to use regular adamw instead of adamw_bnb_8bit? Was it loading the model in bf16? Did flashattention2 make that big of a difference? Something else?\n",
    "2. How can I determine the batch size auto_find_batch_size landed on?\n",
    "3. I actually still have some vram to work with. How can I make the most use of it?\n",
    "\n",
    "Perhaps most importantly...we're going to try out the checkpoints along the way instead of training for more than a full day before testing the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model after the second fine-tuning attempt\n",
    "\n",
    "I tried out one of the model checkpoints during the fine-tuning process and it was performing better than the end result of the first fine-tuning attempt. It still didn't know *how* to make coffee with a Chemex coffee maker, but, well, it's a small model. What's important is that it took a question for a prompt and it responded with a more-or-less coherent answer.\n",
    "\n",
    "However—and here's another lesson learned about the perils of running everything from notebooks—I initialized the training run by running a whole notebook. A notebook that happened to have a cleanup script intended to delete intermediary checkpoints. So, yeah, I finished a 14.5 hour training run and deleted the results.\n",
    "\n",
    "On to round 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Next Steps\n",
    "\n",
    "This fine-tuning process pushed the limits of what we could accomplish on a single GPU. And it makes sense: our back-of-the-envelope calculations said that we would require *at least* 20GB of VRAM, before we even think about storing activations or scaling sequence lengths or batch sizes.\n",
    "\n",
    "We got around this in part by using a smaller sequence length than that shown in the tinyllama fine-tuning script. The biggest change we made was to use the `adamw_bnb_8bit` optimizer from the bitsandbytes library. The point is that we are running up against the limits of what we can reasonably accomplish with a single GPU, at least without more sophisticated approaches. So what's next? There are several directions we can pursue (and we can and should pursue them all):   \n",
    "1. Try to further optimize training this model on a single GPU. What can we do to make the training process run faster and more effectively? Can we find an approach that will still let us the normal `adamw` optimizer? Can we benefit from using e.g. [Deepspeed ZeRO](https://huggingface.co/docs/transformers/perf_train_gpu_one#deepspeed-zero)?\n",
    "2. Try to fine-tune this model on a multi-GPU setup. What are the benefits in terms of speed and ability to train larger batches and larger sequence lengths? And, perhaps more importantly in this setting, how do we make the leap from a single GPU to a multi-GPU setup?\n",
    "3. Train a bigger model! So far we have fine-tuned t5-small, gpt2, and tinyllama, with each subsequent model larger than the last. We ultimately want to work our way up to even larger models, so after this, it might be time to train a 3B parameter model, and then a 7B parameter model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix A: Looking at the tinyllama fine-tuning code\n",
    "\n",
    "You can fine the tinyllama fine-tuning code [here](https://github.com/jzhang38/TinyLlama/tree/main/sft). It's worth the time, at this phase of learning about fine-tuning, to read through it and learn about some of the approaches they use.\n",
    "\n",
    "Let's first take a look at the [train()](https://github.com/jzhang38/TinyLlama/blob/11a02ce085c1670bd009e6d4385701ff06a7f6cf/sft/finetune.py#L492) method. It begins by using the [`HFArgumentParser`](https://huggingface.co/docs/transformers/v4.36.1/en/internal/trainer_utils#transformers.HfArgumentParser) to configure the training arguments. Earlier, the training code defined a number of `dataclass`es for e.g. training arguments, data arguments, etc. `HFArgumentParser` provides an approach for parsing command line arguments directly into instances of these dataclass types. So instead of simply defining arguments in notebook cells, as we've been doing, this approach provides a structured way to parse command line arguments. And, indeed, the repo provides a [shell script](https://github.com/jzhang38/TinyLlama/blob/main/sft/script.sh) for running the fine-tuning script with a defined set of arguments.\n",
    "\n",
    "The [next major section](https://github.com/jzhang38/TinyLlama/blob/11a02ce085c1670bd009e6d4385701ff06a7f6cf/sft/finetune.py#L514C3-L514C3) of the training script is focused on preparing the data, using the [`make_data_module`](https://github.com/jzhang38/TinyLlama/blob/11a02ce085c1670bd009e6d4385701ff06a7f6cf/sft/finetune.py#L354) method defined earlier in the script. That method is set up to handle a few different potential fine-tuning data sources (slimorca is not included among them). It maps each of them to the expected format: an input string and an output string.\n",
    "\n",
    "I found the handling of alpaca-formatted datasets instructive. The [alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca?row=0) dataset includes instructions and optional inputs that follow a specified format. For examples with inputs, the format is:\n",
    "\n",
    "```\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "```\n",
    "\n",
    "The following code snippet in the TinyLlama repo handles this formatting (in the alpaca dataset, the inputs/outputs are not pre-formatted)\n",
    "\n",
    "```python\n",
    "ALPACA_PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response: \"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response: \"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def extract_alpaca_dataset(example):\n",
    "    if example.get(\"input\", \"\") != \"\":\n",
    "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_input\"]\n",
    "    else:\n",
    "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_no_input\"]\n",
    "    return {'input': prompt_format.format(**example)}\n",
    "```\n",
    "\n",
    "We're seeing some repeating patterns across training scripts (the examples so far in this repo and the TinyLlama code). Each fine-tuning run so far requires the following:\n",
    "- process the data\n",
    "- set up training arguments\n",
    "- set up logging\n",
    "\n",
    "An additional step, as we get to multi-gpu and multi-node setups, will be configuring devices and processses—-see the [script.sh](https://github.com/jzhang38/TinyLlama/blob/main/sft/script.sh) shell script from TinyLlama for an example, which uses [accelerate launch](https://huggingface.co/docs/accelerate/basic_tutorials/launch), a helper command that makes it easier to launch training scripts on different hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B: Resuming from a Checkpoint\n",
    "\n",
    "I made a few mistakes in terms of handling checkpoints. In one of those cases, I saved checkpoints and assumed a final model would also be saved. This was not the case. So I had a checkpoint at step 20,000 out of 29,000. In this case, instead of starting over, it made more sense to load the checkpoint and finish training. To do so with the Hugging Face trainer, we can:\n",
    "\n",
    "1. Load the desired model checkpoint with e.g. \n",
    "```\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/path/to/checkpoint-20000\",\n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "```\n",
    "\n",
    "Also make sure the tokenizer is loaded.\n",
    "2. After configuring the trainer/training arguments as before, call `trainer.train` with the `resume_from_checkpoint` argument set to the desired checkpoint.\n",
    "```\n",
    "trainer.train(resume_from_checkpoint=\"/path/to/checkpoint-20000\")\n",
    "```\n",
    "\n",
    "The training will then pick up at step 20,000. And then you can make sure to save the final model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
