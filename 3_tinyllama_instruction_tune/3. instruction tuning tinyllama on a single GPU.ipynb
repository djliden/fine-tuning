{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The [TinyLlama](https://github.com/jzhang38/TinyLlama) project \"aims to pretrain a 1.1B Llama model on 3 trillion tokens.\" 1.1B tokens represents a considerable step up from the small GPT model we [previously fine-tuned](../2_gpt2_single_gpu/2.%20GPT2%20on%20a%20single%20GPU.ipynb). That model had 124M parameters; TinyLlama, while still small by the standards of most widely-used LLMs, is almost ten times the size. We will need around 20GB VRAM at a bare minimum to fine-tune this model.\n",
    "\n",
    "## Instruction Tuning\n",
    "We are going to focus on instruction tuning in this example. Instruction Tuning is a supervised learning technique in which we train the model on instruction/output pairs with the goal of training the model to follow human instructions. Before instruction tuning, the base model is trained on next-token completion. We saw this in the GPT2 example: we provided the start of a story and the model completed it. An instruction-tuned model, on the other hand, is trained to answer a question or instruction.\n",
    "\n",
    "[This repository](https://github.com/xiaoya-li/Instruction-Tuning-Survey) contains a wealth of information on the current state of the field of instruction tuning.\n",
    "\n",
    "The [TinyLlama repository](https://github.com/jzhang38/TinyLlama/tree/main/sft) includes scripts for fine-tuning. While these will be useful references, we will try to proceed with an approach similar to that used in the gpt2 and t5-small notebooks--purely for the sake of making this notebook a reasonable learning step following those.\n",
    "\n",
    "# The Data\n",
    "We will use the [SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca) dataset. This is a curated subset of the much larger [OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca) dataset. Why this dataset? It's one of the most popular sources of instruction data on Hugging Face, and its size is more manageable than the full OpenOrca dataset. That's all!\n",
    "\n",
    "# 1. Load the model and try some examples\n",
    "\n",
    "We'll begin, as always, by loading the model and trying out some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Environment Setup\n",
    "OUTPUT_DIR = \"../results/TinyLlama/\" # the path to the output directory; where model checkpoints will be saved\n",
    "LOG_DIR = \"../logs/TinyLlama/\" # the path to the log directory; where logs will be saved\n",
    "CACHE_DIR = \"../cache/TinyLlama/\" # the path to the cache directory; where cache files will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are step-by-step instructions to make a great cup of coffee with a Chemex coffee maker:\n",
      "1. Prepare the grounds\n",
      "2. Add water\n",
      "3. Grind the beans\n",
      "4. Pour the water into the pot\n",
      "5. Let it brew\n",
      "6. Enjoy your coffee!\n",
      "What is a Chemex?\n",
      "A Chemex is a type of coffee maker that uses a ceramic filter to extract the flavor from the coffee beans. The Chemex was invented by a man named John C. Hunt in 1908\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_ckpt = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_ckpt,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_ckpt,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Inference\n",
    "def generate(prompt, max_new_tokens=100):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    gen_tokens = model.generate(input_ids, max_new_tokens=max_new_tokens,\n",
    "                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                repetition_penalty=1.1)\n",
    "    return tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "print(generate(\"Here are step-by-step instructions to make a great cup of coffee with a Chemex coffee maker:\\n1.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we structured our prompt with completion in mind: we generated the first part of the full text and asked the model to complete it. What happens if, instead, we ask a question or give an instruction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I make coffee with a Chemex coffee maker?\n",
      "How do you make coffee with a Chemex?\n",
      "What is the best way to brew coffee with a Chemex?\n",
      "How do you make coffee with a Chemex filter?\n",
      "How do you make coffee with a Chemex filter and a Chemex?\n",
      "How do you make coffee with a Chemex filter and a Chemex filter?\n",
      "How do you make coffee with a Chemex filter and a Chemex filter?\n",
      "How do you make coffee with a Chemex filter and\n"
     ]
    }
   ],
   "source": [
    "# Question\n",
    "print(generate(\"How do I make coffee with a Chemex coffee maker?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me how to make coffee with a Chemex coffee maker.\n",
      "I'm not sure if you can get the Chemex in the UK, but I think it's worth a try.\n",
      "The Chemex is a great coffee maker and I love mine!\n",
      "I have one of these and I love it. It makes a great cup of coffee every time.\n",
      "I have a Chemex and I love it. It makes a great cup of coffee every time.\n",
      "I have a Chemex and I love it. It makes a great cup\n"
     ]
    }
   ],
   "source": [
    "# Instruction\n",
    "print(generate(\"Tell me how to make coffee with a Chemex coffee maker.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These did not work because the model has not been instruction tuned. Our task is to change that!\n",
    "\n",
    "# 2. Getting and Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "slimorca = load_dataset('Open-Orca/SlimOrca',\n",
    "                           cache_dir=str(Path(CACHE_DIR) / \"data\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"conversations\": [\n",
      "        {\n",
      "            \"from\": \"system\",\n",
      "            \"value\": \"You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\",\n",
      "            \"weight\": null\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"human\",\n",
      "            \"value\": \"Write an article based on this \\\"A man has been charged with murder and attempted murder after a woman and the man she was on a date with were stabbed at a restaurant in Sydney, Australia.\\\"\",\n",
      "            \"weight\": 0.0\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"gpt\",\n",
      "            \"value\": \"Title: Tragedy Strikes in Sydney: Victims Stabbed during Dinner Date, Suspect Charged with Murder and Attempted Murder\\n\\nIntroduction\\n\\nA seemingly ordinary evening took a tragic turn in Sydney, Australia when a romantic dinner date at a local restaurant ended in bloodshed, as a man allegedly attacked a couple, stabbing both victims. Local authorities have charged the suspect with murder and attempted murder in connection with the horrific crime. In this article, we delve into the chilling incident, the suspect, and the impact it has had on the community.\\n\\nThe Incident\\n\\nThe alarming event took place at a popular restaurant in Sydney, Australia. A woman and her date were enjoying their meal \\u2013 blissfully unaware of the tragic circumstances that would soon unfold. As they shared conversation and indulged in their dinner, a man approached the couple and, without warning, began to stab them with a sharp object.\\n\\nPatrons at the restaurant scrambled for safety, as the assailant continued his violent rampage. Bystanders quickly called emergency services, who promptly arrived at the scene. Although both victims were rushed to a nearby hospital, the woman succumbed to her injuries. The man she was on a date with remained in critical condition, fighting for his life.\\n\\nThe Suspect\\n\\nInitial investigations revealed that the alleged attacker had no known connections to the victims \\u2013 adding to the mystifying nature of this sudden and brutal assault. Witnesses reported that the assailant seemed to have no specific motive and appeared to carry out the act senselessly.\\n\\nFollowing a thorough investigation, local police identified and arrested the suspect. During the process, it was discovered that the alleged attacker had a history of criminal behavior and a troubled past, though it is unclear if this played a role in the tragic incident.\\n\\nAuthorities have formally charged the man with murder and attempted murder in connection with the heinous crime. He awaits a hearing to determine a trial date and, if convicted, could face a life sentence in prison.\\n\\nThe Community's Response\\n\\nThe shocking nature of the crime has left the residents of Sydney reeling, as they struggle to come to terms with the harrowing event. The restaurant where the attack occurred has since been closed, with a makeshift memorial being created outside to commemorate the victims.\\n\\nMany have questioned how such a vicious crime could happen in what is considered to be one of the safest cities in the world. This tragic event has spurred local officials to reassess current security measures and devise strategies to reinforce public safety. Additionally, discussions surrounding mental health and criminal rehabilitation have surfaced as residents seek to comprehend the actions of the alleged perpetrator and prevent similar incidents from occurring in the future.\\n\\nIn the wake of the stabbing, the community has banded together with an outpouring of grief and support for the victims and their families. Candlelight vigils have been held, and an online fundraising campaign is underway to assist the surviving victim with his medical expenses and recovery.\\n\\nConclusion\\n\\nThe tragic attack in Sydney serves as a chilling reminder that senseless acts of violence can happen anywhere and at any time. The community's response to this horrific and seemingly random act of brutality has been one of solidarity and determination to prevent such incidents in the future. As the case unfolds, the victims and their families remain in the hearts of the community, who are grieving the devastating loss of a life cut tragically short and supporting the recovering victim as he continues to endure this unimaginable ordeal.\",\n",
      "            \"weight\": 1.0\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(slimorca[\"train\"][0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that there are three components to the sample entry:\n",
    "1. A *system message*: this should be familiar if you've used e.g. ChatGPT via the API. This is a general instruction specifying the model's role/identity and general instructions.\n",
    "2. A *human message*: this is the specific instruction passed to the model by a human.\n",
    "3. a *gpt*: this is the AI model's response.\n",
    "\n",
    "So we want to use this dataset to fine-tune the model such that it will respond more like the *gpt* message when given the *system* and *human* messages.\n",
    "\n",
    "First, we need to get these entries into a format we can actually use for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
