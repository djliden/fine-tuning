{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The [TinyLlama](https://github.com/jzhang38/TinyLlama) project \"aims to pretrain a 1.1B Llama model on 3 trillion tokens.\" 1.1B tokens represents a considerable step up from the small GPT model we [previously fine-tuned](../2_gpt2_single_gpu/2.%20GPT2%20on%20a%20single%20GPU.ipynb). That model had 124M parameters; TinyLlama, while still small by the standards of most widely-used LLMs, is almost ten times the size. We will need around 20GB VRAM at a bare minimum to fine-tune this model.\n",
    "\n",
    "## Instruction Tuning\n",
    "We are going to focus on instruction tuning in this example. Instruction Tuning is a supervised learning technique in which we train the model on instruction/output pairs with the goal of training the model to follow human instructions. Before instruction tuning, the base model is trained on next-token completion. We saw this in the GPT2 example: we provided the start of a story and the model completed it. An instruction-tuned model, on the other hand, is trained to answer a question or instruction.\n",
    "\n",
    "[This repository](https://github.com/xiaoya-li/Instruction-Tuning-Survey) contains a wealth of information on the current state of the field of instruction tuning.\n",
    "\n",
    "The [TinyLlama repository](https://github.com/jzhang38/TinyLlama/tree/main/sft) includes scripts for fine-tuning. While these will be useful references, we will try to proceed with an approach similar to that used in the gpt2 and t5-small notebooks--purely for the sake of making this notebook a reasonable learning step following those.\n",
    "\n",
    "# The Data\n",
    "We will use the [SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca) dataset. This is a curated subset of the much larger [OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca) dataset. Why this dataset? It's one of the most popular sources of instruction data on Hugging Face, and its size is more manageable than the full OpenOrca dataset. That's all!\n",
    "\n",
    "# 1. Load the model and try some examples\n",
    "\n",
    "We'll begin, as always, by loading the model and trying out some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are step-by-step instructions to make a great cup of coffee with a Chemex coffee maker:\n",
      "1. Prepare the grounds\n",
      "2. Add water\n",
      "3. Grind the beans\n",
      "4. Pour the water into the pot\n",
      "5. Let it brew\n",
      "6. Enjoy your coffee!\n",
      "What is a Chemex?\n",
      "A Chemex is a type of coffee maker that uses a ceramic filter to extract the flavor from the coffee beans. The Chemex was invented by a man named John C. Hunt in 1908\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_ckpt = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_ckpt,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_ckpt,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Inference\n",
    "def generate(prompt, max_new_tokens=100):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    gen_tokens = model.generate(input_ids, max_new_tokens=max_new_tokens,\n",
    "                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                repetition_penalty=1.1)\n",
    "    return tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "print(generate(\"Here are step-by-step instructions to make a great cup of coffee with a Chemex coffee maker:\\n1.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we structured our prompt with completion in mind: we generated the first part of the full text and asked the model to complete it. What happens if, instead, we ask a question or give an instruction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I make coffee with a Chemex coffee maker?\n",
      "How do you make coffee with a Chemex?\n",
      "What is the best way to brew coffee with a Chemex?\n",
      "How do you make coffee with a Chemex filter?\n",
      "How do you make coffee with a Chemex filter and a Chemex?\n",
      "How do you make coffee with a Chemex filter and a Chemex filter?\n",
      "How do you make coffee with a Chemex filter and a Chemex filter?\n",
      "How do you make coffee with a Chemex filter and\n"
     ]
    }
   ],
   "source": [
    "# Question\n",
    "print(generate(\"How do I make coffee with a Chemex coffee maker?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me how to make coffee with a Chemex coffee maker.\n",
      "I'm not sure if you can get the Chemex in the UK, but I think it's worth a try.\n",
      "The Chemex is a great coffee maker and I love mine!\n",
      "I have one of these and I love it. It makes a great cup of coffee every time.\n",
      "I have a Chemex and I love it. It makes a great cup of coffee every time.\n",
      "I have a Chemex and I love it. It makes a great cup\n"
     ]
    }
   ],
   "source": [
    "# Instruction\n",
    "print(generate(\"Tell me how to make coffee with a Chemex coffee maker.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These did not work because the model has not been instruction tuned. Our task is to change that!\n",
    "\n",
    "# 2. Getting and Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
