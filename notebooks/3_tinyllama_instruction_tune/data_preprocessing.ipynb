{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Over the course of experimenting with fine-tuning tinyllama, I realized that a lot of the complexity comes from the data pre-processing step. This notebook will break down the pre-processing process and call attention to different possible considerations and approaches.\n",
    "\n",
    "In pre-processing the data, it is important to understand (at least) the following:\n",
    "- How is the raw data stuctured?\n",
    "- What does a single example from the raw data look like?\n",
    "- How do we need to transform the data for training?\n",
    "- What does a single example in the transformed data look like?\n",
    "- How do we need to tokenize the data for training?\n",
    "- How do we batch the tokenized data for training?\n",
    "\n",
    "## Setup\n",
    "\n",
    "We're going to load the tokenizer and the dataset, but not the model. We won't need it in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from -r ./tinyllama_requirements.txt (line 1)) (4.37.1)\n",
      "Requirement already satisfied: accelerate in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from -r ./tinyllama_requirements.txt (line 2)) (0.26.1)\n",
      "Requirement already satisfied: psutil in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from -r ./tinyllama_requirements.txt (line 3)) (5.9.8)\n",
      "Requirement already satisfied: pynvml in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from -r ./tinyllama_requirements.txt (line 4)) (11.5.0)\n",
      "Requirement already satisfied: bitsandbytes in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from -r ./tinyllama_requirements.txt (line 5)) (0.42.0)\n",
      "Requirement already satisfied: filelock in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (2023.10.3)\n",
      "Requirement already satisfied: requests in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from transformers->-r ./tinyllama_requirements.txt (line 1)) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from accelerate->-r ./tinyllama_requirements.txt (line 2)) (2.1.2)\n",
      "Requirement already satisfied: scipy in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from bitsandbytes->-r ./tinyllama_requirements.txt (line 5)) (1.11.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r ./tinyllama_requirements.txt (line 1)) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r ./tinyllama_requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r ./tinyllama_requirements.txt (line 2)) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r ./tinyllama_requirements.txt (line 2)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r ./tinyllama_requirements.txt (line 2)) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from requests->transformers->-r ./tinyllama_requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from requests->transformers->-r ./tinyllama_requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from requests->transformers->-r ./tinyllama_requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from requests->transformers->-r ./tinyllama_requirements.txt (line 1)) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->-r ./tinyllama_requirements.txt (line 2)) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate->-r ./tinyllama_requirements.txt (line 2)) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade -r ./tinyllama_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR =  \"../cache/TinyLlama/\" # the path to the cache directory; where cache files will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the tokenizer corresponding to the model checkpoint\n",
    "model_ckpt = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_ckpt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does not specify a pad token.  If we want to use padding (more on this later), we need to set the pad token. We can do this with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "slimorca = load_dataset(\"Open-Orca/SlimOrca\", cache_dir=str(Path(CACHE_DIR) / \"data\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is the raw data stuctured?\n",
    "\n",
    "What have we actually loaded here? We used the Hugging Face `datasets` library to load the dataset, and the resulting object is a `datasetdict`. A `datasetdict` is a dictionary-like object for managing datasets, where the keys are the names of the datasets, and the values are the actual datasets. Usually the datasets are splits such as \"train\", \"valid\", and \"test\". In this case, we only have a \"train\" split, with only one feature, \"conversations\".\n",
    "\n",
    "`DatasetDict`s enable us to map various pre-processing operations to the datasets they contain concisely and efficiently.\n",
    "\n",
    "Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 517982\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slimorca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train` split within the `DatasetDict` is a `Dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversations'],\n",
       "    num_rows: 517982\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slimorca[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we need to transform the data for training?\n",
    "\n",
    "Hugging Face `Dataset`s provide [various methods for querying, subsetting, and processing data](https://huggingface.co/docs/datasets/process), generally quite efficiently because they use the [Apache Arrow format](https://huggingface.co/docs/datasets/about_arrow). For example, if we want a validation set, we can split our training set into separate train/valid sets. The data are shuffled by default, so we don't need to shuffle as a separate step. Note that we apply this operation to the `Dataset`, not the `DatasetDict`.\n",
    "\n",
    "The resulting object is a new `DatasetDict` with two keys: \"train\" and \"test\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 466183\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conversations'],\n",
       "        num_rows: 51799\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slimorca_split = slimorca[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "slimorca_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does a single example from the raw data look like?\n",
    "\n",
    "Now let's look at some of the actual data examples. What do the `conversations` look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'conversations': [{'from': 'system',\n",
       "   'value': 'You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.',\n",
       "   'weight': None},\n",
       "  {'from': 'human',\n",
       "   'value': 'Q:Read the article and select the best answer. Article: Tina was not like many of her classmates. She didn\\'t listen to popular music or watch many movies, and she wasn\\'t interested in nice clothes. When she got together with her friends, they wanted to listen to rock and pop music. When Tina asked  if  they would  like  to  try classical    music, they all looked at her strangely.\"Classical music  is  for old people, \" one of  her friends said. Tina was worried that something was wrong with her. She decided to talk to her father. As she entered his study  , her father could feel something was wrong. \"Dad, am I strange?\" she asked her father.\"Of course not, \" he answered. \"Why do you ask that?\" \"Because I don\\'t like the same things as my classmates do. They want to listen to Mariah Carey\\'s music. I like Yo Yo Ma\\'s.\" \"I can understand, Tina,  it\\'s all  right _ You don\\'t have to copy   what other people do. Everybody has different tastes. Some of them are popular, and others aren\\'t. \"After talking with her father, Tina felt better. She realized    that being different made her special.  It was an important lesson for her to learn. Question: Tina\\'s father made Tina feel  _  . Options: A: angry B: worried C: excited D: better\\nA:',\n",
       "   'weight': 0.0},\n",
       "  {'from': 'gpt', 'value': 'D: better', 'weight': 1.0}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slimorca_split[\"train\"][42]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objects are dictionaries with information about the roles (\"system\", \"human\", or \"gpt\") and values making up the exchange. Our task is to map this to a format on which we can train the model.\n",
    "\n",
    "The Hugging Face Transformers library provides convenient [chat model templates](https://huggingface.co/docs/transformers/main/en/chat_templating#templates-for-chat-models). The Hugging Face docs recommend applying the chat templates as a preprocessing step.\n",
    "\n",
    "We won't go into too much detail about the concept of chat templates—you can read more [here](https://huggingface.co/blog/chat-templates). For now, just know that they provide a means of clearly indicating which part of a string came from the user and which part is the LLM's expected response. Models are trained on specific chat formats, and using different formats will generally result in bad responses at inference time.\n",
    "\n",
    "Here's an example of a chat dictionary translated to a string with the chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "You are a helpful assistant and an expert at making coffee.\n",
      "<</SYS>>\n",
      "\n",
      "How do I make coffee with a Chemex coffee maker? [/INST] To make coffee with a Chemex:\n",
      "1. Boil water to about 200°F (93°C).\n",
      "2. Place the Chemex filter in the top and rinse it with hot water to remove paper taste and warm the vessel. Discard the rinse water.\n",
      "3. Add coffee grounds to the filter. Use a medium-coarse grind, about 1 gram of coffee per 16 grams of water.\n",
      "4. Pour just enough hot water to saturate the grounds. Wait 30 seconds for the coffee to 'bloom'.\n",
      "5. Slowly pour the remaining water over the grounds in a circular motion. Aim for a total brew time of 3.5 to 4.5 minutes.\n",
      "6. Once brewing is complete, remove the filter and enjoy. </s>\n"
     ]
    }
   ],
   "source": [
    "chat = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant and an expert at making coffee.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How do I make coffee with a Chemex coffee maker?\"},\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"To make coffee with a Chemex:\\n1. Boil water to about 200°F (93°C).\\n2. Place the Chemex filter in the top and rinse it with hot water to remove paper taste and warm the vessel. Discard the rinse water.\\n3. Add coffee grounds to the filter. Use a medium-coarse grind, about 1 gram of coffee per 16 grams of water.\\n4. Pour just enough hot water to saturate the grounds. Wait 30 seconds for the coffee to 'bloom'.\\n5. Slowly pour the remaining water over the grounds in a circular motion. Aim for a total brew time of 3.5 to 4.5 minutes.\\n6. Once brewing is complete, remove the filter and enjoy.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(tokenizer.apply_chat_template(chat, tokenize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that using the chat template adds some special tokens indicating the beginning/end of the chat (`<s>` and `</s>`) and the beginning/end of the instruction (`[INST]` and `[/INST]`). We will need to add these to our tokenizer as special tokens later.\n",
    "\n",
    "Let's map the `conversations` to the expected input for the chat template. The Hugging Face chat model templates expect a dictionary similar to `conversations` but with some notable differences. We need to replace `from` with `role`, `value` with `content`, `gpt` with `assistant`, and `human` with `user`. We will save the actual conversion until later, as there are other considerations we still need to address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['conversations', 'chat'],\n",
       "        num_rows: 466183\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['conversations', 'chat'],\n",
       "        num_rows: 51799\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_chat(ex):\n",
    "    role_mapping = {\"gpt\": \"assistant\", \"system\": \"system\", \"human\": \"user\"}\n",
    "    chat = [\n",
    "        {\"role\": role_mapping[message[\"from\"]], \"content\": message[\"value\"]}\n",
    "        for message in ex[\"conversations\"]\n",
    "    ]\n",
    "\n",
    "    return {\"chat\": chat}\n",
    "\n",
    "\n",
    "slimorca_split_formatted_chat = slimorca_split.map(format_chat, num_proc=32)\n",
    "slimorca_split_formatted_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does a single example of the transformed data look like?\n",
    "\n",
    "Now we have added a 'chat' key to each example in `slimorca_split_formatted_chat`. Note how we applied this transformation. We wrote a function that processed a single example and then used the `map` method to apply it to all examples in the `DatasetDict`. The `num_proc` parameter specifies how many processes to use for parallel processing, dramatically increasing the speed of the process.\n",
    "\n",
    "Let's compare the original and formatted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'from': 'system', 'value': 'You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.', 'weight': None}, {'from': 'human', 'value': 'Q:Read the article and select the best answer. Article: Tina was not like many of her classmates. She didn\\'t listen to popular music or watch many movies, and she wasn\\'t interested in nice clothes. When she got together with her friends, they wanted to listen to rock and pop music. When Tina asked  if  they would  like  to  try classical    music, they all looked at her strangely.\"Classical music  is  for old people, \" one of  her friends said. Tina was worried that something was wrong with her. She decided to talk to her father. As she entered his study  , her father could feel something was wrong. \"Dad, am I strange?\" she asked her father.\"Of course not, \" he answered. \"Why do you ask that?\" \"Because I don\\'t like the same things as my classmates do. They want to listen to Mariah Carey\\'s music. I like Yo Yo Ma\\'s.\" \"I can understand, Tina,  it\\'s all  right _ You don\\'t have to copy   what other people do. Everybody has different tastes. Some of them are popular, and others aren\\'t. \"After talking with her father, Tina felt better. She realized    that being different made her special.  It was an important lesson for her to learn. Question: Tina\\'s father made Tina feel  _  . Options: A: angry B: worried C: excited D: better\\nA:', 'weight': 0.0}, {'from': 'gpt', 'value': 'D: better', 'weight': 1.0}]\n",
      "[{'content': 'You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer.', 'role': 'system'}, {'content': 'Q:Read the article and select the best answer. Article: Tina was not like many of her classmates. She didn\\'t listen to popular music or watch many movies, and she wasn\\'t interested in nice clothes. When she got together with her friends, they wanted to listen to rock and pop music. When Tina asked  if  they would  like  to  try classical    music, they all looked at her strangely.\"Classical music  is  for old people, \" one of  her friends said. Tina was worried that something was wrong with her. She decided to talk to her father. As she entered his study  , her father could feel something was wrong. \"Dad, am I strange?\" she asked her father.\"Of course not, \" he answered. \"Why do you ask that?\" \"Because I don\\'t like the same things as my classmates do. They want to listen to Mariah Carey\\'s music. I like Yo Yo Ma\\'s.\" \"I can understand, Tina,  it\\'s all  right _ You don\\'t have to copy   what other people do. Everybody has different tastes. Some of them are popular, and others aren\\'t. \"After talking with her father, Tina felt better. She realized    that being different made her special.  It was an important lesson for her to learn. Question: Tina\\'s father made Tina feel  _  . Options: A: angry B: worried C: excited D: better\\nA:', 'role': 'user'}, {'content': 'D: better', 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(slimorca_split_formatted_chat[\"train\"][42][\"conversations\"])\n",
    "print(slimorca_split_formatted_chat[\"train\"][42][\"chat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the Data\n",
    "\n",
    "As mentioned above, our instruction formatting includes some special tokens we would like to add to the tokenizer's vocabulary. We can do that as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the instruction tokens to the tokenizer\n",
    "special_tokens = [\"[INST]\", \"[/INST]\", \"<<SYS>>\", \"<</SYS>>\"]\n",
    "# Adding special tokens to the tokenizer\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few questions might occur to you at this point:\n",
    "> Why don't we add the `<s>` and `</s>` tokens to the tokenizer? Those were used in the chat formatting too!\n",
    "\n",
    "We don't need to add those tokens because they are already the tokenizer's `bos` (beginning of sequence) and `eos` (end of sequence) tokens, as shown below.\n",
    "\n",
    "> What exactly does it mean to \"add tokens to the tokenizer\"?\n",
    "Adding tokens to the tokenizer expands the tokenizer's *vocabulary*, the list of possible tokens that the model can use. Tokens in the tokenizer's vocabulary won't be split into smaller units. Before adding `[INST]` to the vocabulary, for example, it could only be formed through the following combination of tokens: `['[', '/', 'INST', ']']`. This allows the model to recognize the `[INST]` token as a single, distinct semantic unit: it only needs to learn one token to recognize the start of an instruction, not a sequence of four tokens that might also be used in other contexts. Furthermore, since all of the sequences we want to train on will include the special tokens above, there are some effiency gains: each of these tokens will take up only one unit of context length rather than multiple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<s>', '</s>')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the <s> and </s> tokens are already part of the vocabulary\n",
    "tokenizer.bos_token, tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure the Tokenized Data\n",
    "\n",
    "Before we actually tokenize the chat data in our `DatasetDict`, we need to make some decisions about how to structure the tokenized data. So far, we've been thinking in terms of individual *examples* from the training data. This isn't necessarily the most relevant perspective to the model. We should think in terms of *sequences* of tokes and *batches* of sequences.\n",
    "- A *sequence* is a single list of tokens from the training data, usually limited to some uniform length. Sequences of the desired length can be formed by starting with a single example and *padding* it with the tokenizer's padding token to make it the desired length if it is too short, or by *truncating* it if it is too long. Another option is sequence *packing*, wherein multiple shorter sequences are packed into a single longer sequence.\n",
    "- A *batch* is a list of sequences. During training, the model is fed a batch of sequences at a time.\n",
    "\n",
    "We also need to think about our training objective as that can also affect the way in which we have to structure the data. Do the data have an input/output structure? In our case, we have instruction/response pairs: how do we structure them? This relates to the behavior of the model during training and is neither obvious nor easy to figure out.\n",
    "\n",
    "In the Hugging Face training ecosystem, the *collator* is responsible for taking inputs, generating labels, and assembling the inputs into batches. Let's see what happens if we tokenize our data and supply it to the `DataCollatorForLanguageModeling` collator.\n",
    "\n",
    "Note that, while the `apply_chat_template` function provides the option to tokenize the chat, we're going to apply the chat template and then tokenize in separate steps. This makes the process more transparent; we don't have to wonder how the arguments from the helper function are passed to the tokenizer. This also corresponds to the approach shown in the [docs](https://huggingface.co/docs/transformers/main/en/chat_templating#can-i-use-chat-templates-in-training). The training example does not include direct tokenization as part of the `apply_chat_template` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bff3685d4b74568acabb15feddc3de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/466183 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a812632c9aab4091b55639f07e6e0f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/51799 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(ex):\n",
    "    # Apply chat template for formatting\n",
    "    formatted_chat = tokenizer.apply_chat_template(\n",
    "        ex[\"chat\"],\n",
    "        tokenize=False,  # Apply formatting but do not tokenize\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    # Tokenize using the standard tokenizer method\n",
    "    tokenized_output = tokenizer(\n",
    "        formatted_chat,\n",
    "        add_special_tokens=False,  # apply_chat_template already added special tokens\n",
    "    )\n",
    "    \n",
    "    return tokenized_output\n",
    "\n",
    "slimorca_split_formatted_chat_tokenized = slimorca_split_formatted_chat.map(\n",
    "    tokenize_function, num_proc=32\n",
    ").remove_columns([\"conversations\", \"chat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 466183\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 51799\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slimorca_split_formatted_chat_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened here? We ended up with two features, `input_ids` and `attention_mask`.\n",
    "- The `input_ids` are our tokenized chats. We converted the chat dictionaries into formatted strings with the `apply_chat_template` function and then tokenized them using the standard tokenizer.\n",
    "- The `attention_mask` is a binary mask that indicates whether each token is a padding token or not. During training, the model will ignore the tokens indicated by the mask. The attention mask is particularly useful for batching inputs. We need inputs of the same length, but the actual texts will be different lengths, so we pad (and/or truncate) them to the desired length. The attention mask tells the model to ignore the padding tokens.\n",
    "\n",
    "A quick note: `tokenizer()` and `tokenizer.encode()` are different! The former generates an attention mask and the latter does not. When preparing the data for training, we should use the former. The latter is useful for inference or for other cases where the attention mask is not needed.\n",
    "\n",
    "Let's see what these actually look like. We'll examine the beginnings and ends of the first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs:  [1, 32000, 259, 32002, 29871, 13, 3492, 526, 385, 319] ... [13, 13, 22550, 29901, 11837, 2897, 7681, 275, 29871, 2]\n",
      "Attention Mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs: \", slimorca_split_formatted_chat_tokenized[\"train\"][0][\"input_ids\"][0:10], \"...\", slimorca_split_formatted_chat_tokenized[\"train\"][0][\"input_ids\"][-10:])\n",
    "print(\"Attention Mask: \", slimorca_split_formatted_chat_tokenized[\"train\"][0][\"attention_mask\"][0:10], \"...\", slimorca_split_formatted_chat_tokenized[\"train\"][0][\"attention_mask\"][-10:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mask is just a list of `1`s! This is because we did not instruct the tokenizer to apply any padding or truncation.\n",
    "\n",
    "The first example is 370 tokens long. Let's see what happens if we specify that the output should be 375 tokens long and pad to that length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_with_padding(ex):\n",
    "    # Apply chat template for formatting\n",
    "    formatted_chat = tokenizer.apply_chat_template(\n",
    "        ex[\"chat\"],\n",
    "        tokenize=False,  # Apply formatting but do not tokenize\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    # Tokenize using the standard tokenizer method\n",
    "    tokenized_output = tokenizer(\n",
    "        formatted_chat,\n",
    "        add_special_tokens=False,  # apply_chat_template already added special tokens\n",
    "        padding=\"max_length\",\n",
    "        max_length=375,\n",
    "    )\n",
    "    \n",
    "    return tokenized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs:  [1, 32000, 259, 32002, 29871, 13, 3492, 526, 385, 319] ... [2897, 7681, 275, 29871, 2, 2, 2, 2, 2, 2]\n",
      "Attention Mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "x = tokenize_function_with_padding(slimorca_split_formatted_chat[\"train\"][0])\n",
    "\n",
    "print(\"Input IDs: \", x[\"input_ids\"][0:10], \"...\", x[\"input_ids\"][-10:])\n",
    "print(\"Attention Mask: \", x[\"attention_mask\"][0:10], \"...\", x[\"attention_mask\"][-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last five tokens in the `input_id` are the pad token, and the last five `attention_mask` values are 0, indicating that those padding tokens should be ignored.\n",
    "\n",
    "There are quite a few ways we can handle sequence lengths, mostly involving different padding and/or truncation schemes:\n",
    "- Padding means adding padding tokens (often the same as the `eos` token) to a sequence to make it the desired length\n",
    "    - We can choose left or right padding: padding the beginning of the sequence or the end of the sequence.\n",
    "    - We typically pad to the maximum length of the batch or to a pre-defined maximum length.\n",
    "- Truncation means removing tokens from a sequence to make it the desired length.\n",
    "    - As with padding, we can truncate from either side of the sequence, though truncating the end is more common.\n",
    "- Padding and Truncation are usually used together. Short sequences are padded to reach a desired length, and long sequences are truncated to reach the desired length.\n",
    "\n",
    "Let's see what changes if we use truncation and padding. We're going to use a sequence length of 1024 tokens, which should give us a mix of truncated and padded results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730a77973951448b9512c208d6333801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/466183 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd04c185ca24447999b7eb0ed278749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=32):   0%|          | 0/51799 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function_max_length(ex):\n",
    "    # Apply chat template for formatting\n",
    "    formatted_chat = tokenizer.apply_chat_template(\n",
    "        ex[\"chat\"],\n",
    "        tokenize=False,  # Apply formatting but do not tokenize\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "\n",
    "    # Tokenize using the standard tokenizer method\n",
    "    tokenized_output = tokenizer(\n",
    "        formatted_chat,\n",
    "        add_special_tokens=False,  # apply_chat_template already added special tokens\n",
    "        padding=\"max_length\", # pad to the specified length\n",
    "        max_length=1024, # max length at which to truncate or to which to pad\n",
    "        truncation=True # truncate to the specified length\n",
    "    )\n",
    "    \n",
    "    return tokenized_output\n",
    "\n",
    "slimorca_split_formatted_chat_tokenized_max_length = slimorca_split_formatted_chat.map(\n",
    "    tokenize_function_max_length, num_proc=32\n",
    ").remove_columns([\"conversations\", \"chat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs:  [1, 32000, 259, 32002, 29871, 13, 3492, 526, 385, 319] ... [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Attention Mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs: \", slimorca_split_formatted_chat_tokenized_max_length[\"train\"][0][\"input_ids\"][0:10], \"...\", slimorca_split_formatted_chat_tokenized_max_length[\"train\"][0][\"input_ids\"][-10:])\n",
    "print(\"Attention Mask: \", slimorca_split_formatted_chat_tokenized_max_length[\"train\"][0][\"attention_mask\"][0:10], \"...\", slimorca_split_formatted_chat_tokenized_max_length[\"train\"][0][\"attention_mask\"][-10:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 12th example, on the other hand, is longer than 1024 tokens, so we won't see any padding. In fact, the input has been truncated, as we can see if we decode the final tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs:  [1, 32000, 259, 32002, 29871, 13, 3492, 526, 385, 319] ... [29889, 29903, 29889, 18148, 29889, 29871, 13, 259, 13, 17302]\n",
      "Attention Mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] ... [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "End of Decoded Output: job approval was 45 percent with 42 percent of those surveyed saying they disapprove of what he is doing in the U.S. Senate. \n",
      "  \n",
      " Among\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs: \", slimorca_split_formatted_chat_tokenized_max_length[\"train\"][11][\"input_ids\"][0:10], \"...\", slimorca_split_formatted_chat_tokenized_max_length[\"train\"][11][\"input_ids\"][-10:])\n",
    "print(\"Attention Mask: \", slimorca_split_formatted_chat_tokenized_max_length[\"train\"][11][\"attention_mask\"][0:10], \"...\", slimorca_split_formatted_chat_tokenized_max_length[\"train\"][11][\"attention_mask\"][-10:])\n",
    "print(\"\\n\", \"End of Decoded Output: \", tokenizer.decode(slimorca_split_formatted_chat_tokenized_max_length[\"train\"][11][\"input_ids\"][-40:]), sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collation\n",
    "A [*collator*](https://huggingface.co/docs/transformers/main_classes/data_collator#data-collator) is responsible for forming batches from a collection of sequences. The simplest collators just form batches of sequences from the inputs, where a \"batch\" is a tensor of shape `[batch_size, sequence_length]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "# Assuming 'slimorca_tokenized_split' is your dataset and 'data_collator' is your collator\n",
    "batch_size = 5  # Define your batch size\n",
    "\n",
    "# Create an instance of the data collator\n",
    "collator = DefaultDataCollator()\n",
    "\n",
    "# Retrieve examples from the dataset\n",
    "examples = [slimorca_split_formatted_chat_tokenized_max_length['train'][i] for i in range(batch_size)]\n",
    "\n",
    "# Run the collator on these examples to create a batch\n",
    "batch = collator(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll just tokenize the data as-is and explore what the collator does with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[    1, 32000,   259,  ...,     2,     2,     2],\n",
       "          [    1, 32000,   259,  ...,     2,     2,     2],\n",
       "          [    1, 32000,   259,  ...,     2,     2,     2],\n",
       "          [    1, 32000,   259,  ...,     2,     2,     2],\n",
       "          [    1, 32000,   259,  ...,     2,     2,     2]]),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0],\n",
       "          [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " torch.Size([5, 1024]),\n",
       " dict)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with a dictionary of tensors, where each tensor has a shape of `[batch_size, sequence_length]`.\n",
    "\n",
    "We don't usually use the `DefaultDataCollator`. Instead, we use the `DataCollatorForLanguageModeling` collator. Let's see what it does with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Assuming 'slimorca_tokenized_split' is your dataset and 'data_collator' is your collator\n",
    "batch_size = 5  # Define your batch size\n",
    "\n",
    "# Create an instance of the data collator\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Retrieve examples from the dataset\n",
    "examples = [slimorca_split_formatted_chat_tokenized_max_length['train'][i] for i in range(batch_size)]\n",
    "\n",
    "# Run the collator on these examples to create a batch\n",
    "batch = collator(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST]  <<SYS>> \\nYou are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.\\n<</SYS>> \\n\\nPossible review types:\\npick from the following.\\n (1). negative;\\n (2). positive;.\\nGenerate a (1). review for a place [/INST]  Alright sweetie, imagine we went to a place and we didn\\'t like it. So, a negative review is when we share our experience with others to let them know what went wrong. \\n\\nFor example: \"I went to this place with my family, and we didn\\'t have a good time. The people working there were not very friendly, and we waited a long time for our food. When the food came, it didn\\'t taste good either. We were sad about our visit and wouldn\\'t want to go back.\" </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(examples[2]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1024])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch contents: dict_keys(['input_ids', 'attention_mask', 'labels']) \n",
      " batch shape:  torch.Size([5, 1024])\n",
      "Input IDs:  tensor([[    1, 32000,   259, 32002, 29871,    13,  3492,   526,   385,   319],\n",
      "        [    1, 32000,   259, 32002, 29871,    13,  3492,   526,   385,   319],\n",
      "        [    1, 32000,   259, 32002, 29871,    13,  3492,   526,   263,  8444],\n",
      "        [    1, 32000,   259, 32002, 29871,    13,  3492,   526,   385,   319],\n",
      "        [    1, 32000,   259, 32002, 29871,    13,  3492,   526,   385,   319]]) ... tensor([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
      "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]])\n",
      "Attention Mask:  tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) ... tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "Labels:  tensor([[    1, 32000,   259, 32002, 29871,    13,  3492,   526,   385,   319],\n",
      "        [    1, 32000,   259, 32002, 29871,    13,  3492,   526,   385,   319],\n",
      "        [    1, 32000,   259, 32002, 29871,    13,  3492,   526,   263,  8444],\n",
      "        [    1, 32000,   259, 32002, 29871,    13,  3492,   526,   385,   319],\n",
      "        [    1, 32000,   259, 32002, 29871,    13,  3492,   526,   385,   319]]) ... tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n"
     ]
    }
   ],
   "source": [
    "print(\"batch contents:\" , batch.keys(), \"\\n\", \"batch shape: \", batch['input_ids'].shape)\n",
    "print(\"Input IDs: \", batch['input_ids'][:, 0:10], \"...\", batch['input_ids'][:, -10:])\n",
    "\n",
    "print(\"Attention Mask: \", batch['attention_mask'][:, 0:10], \"...\", batch['attention_mask'][:, -10:])\n",
    "\n",
    "print(\"Labels: \", batch['labels'][:, 0:10], \"...\", batch['labels'][:, -10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few observations:\n",
    "- the collator generates labels that are the same length as the input. The labels are just the inputs.\n",
    "- `eos` tokens (`2`) are replaced with `-100` in the labels, signifying that the model should not use them in calculating the loss.\n",
    "- the collator pads the labels to the maximum length of the batch; that is, to the length of the longest sequence in the batch.\n",
    "\n",
    "#### An aside on shifting labels\n",
    "\n",
    "What do the labels actually do? In the most basic structure of *Causal Language Modeling*, our labels should be our input ids shifted one to the right. Why shifted to the right? In causal language modeling, the objective is to predict the *next token* given all of the preceding tokens. So if the model sees `input_ids` `1`, it should predict `input_ids` `2`. By making the labels the same as the input ids shifted one to the right, we establish that the task is to predict the next token given all of the preceding tokens.\n",
    "\n",
    "Crucially, the `DataCollatorForLanguageModeling` will *not* shift the labels to the right. [This](https://github.com/huggingface/transformers/blob/345b9b1a6a308a1fa6559251eb33ead2211240ac/src/transformers/trainer_pt_utils.py#L494) is left to the model training code. E.g. this is where it happens for the Hugging Face `Trainer` for Causal Language Models.\n",
    "\n",
    "#### Labels and Instruction Tuning\n",
    "It is natural to think that, in the case of instruction tuning, we can simply treat the tokenized inputs as out `input_ids` and have the tokenized responses as our `labels`. This is not the case, though. With causal language modeling, the model generates a single token at a time based on some number of preceding tokens. That \"next token\" is the label. Having a set of labels that fundamentally differ from the inputs will not effectively train the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "- [This series of posts](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2) on Weights & Biases provides a great overview of preparing data for instruction tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
