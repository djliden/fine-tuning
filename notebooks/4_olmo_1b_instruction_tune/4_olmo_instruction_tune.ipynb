{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction Tuning Olmo-1b\n",
    "\n",
    "This notebook follows a very similar approach to that from the [tinyllama instruction tuning](tinyllama) notebook, making just a few small adjustments based on lessons learned in that process. The biggest change was that we trained this model using a much smaller training set after noticing that even the first checkpoint from fine-tuning tinyllama was able to response appropriately to instructions.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cebb7cf8-a8f9-45a7-9f28-78f28a621243",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade -r ./olma_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2526b7d-9ca8-444a-85a9-a176eb33ff29",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Some Environment Setup\n",
    "OUTPUT_DIR = \"../results/olmo/\" # the path to the output directory; where model checkpoints will be saved\n",
    "LOG_DIR = \"../logs/olmo/\" # the path to the log directory; where logs will be saved\n",
    "CACHE_DIR = \"../cache/olmo/\" # the path to the cache directory; where cache files will be saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model and test some prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47f3426-08a6-49af-b14d-ed582e1e1a03",
     "showTitle": true,
     "title": "Model Loading and Initialization"
    }
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import hf_olmo\n",
    "\n",
    "model_ckpt = \"allenai/OLMo-1B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_ckpt,\n",
    ")\n",
    "\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_ckpt,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    #attn_implementation=\"flash_attention_2\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a2b2d8-19a2-40e1-ac1b-e45cd90eb2d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that, at the time of writing this notebook, flash attention was not yet working with OLMo.\n",
    "\n",
    "### Test some Prompts\n",
    "\n",
    "We'll start with a simple completion-structured prompt, which we know this model can handle. In this type of prompt, we provide a partial text and expect the model to finish it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a146a775-dcff-4844-8767-1629c4b7b18e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are step-by-step instructions to make a great cup of coffee with a Chemex coffee maker:\n",
      "1. Fill the filter basket with ground coffee and place it in the bottom of your Chemex.\n",
      "2. Add hot water to fill the rest of the way, about 1/4 cup (or more if you like strong coffee).\n",
      "3. Place the filter on top of the water and let sit for 5 minutes.\n",
      "4. Pour the water out and rinse the filter under cold running water.\n",
      "5. Repeat steps 2 and 3 until all the water is used up.\n",
      "6.\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "def generate(prompt, max_new_tokens=100):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    gen_tokens = model.generate(input_ids, max_new_tokens=max_new_tokens,\n",
    "                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                repetition_penalty=1.1)\n",
    "    return tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "print(generate(\"Here are step-by-step instructions to make a great cup of coffee with a Chemex coffee maker:\\n1.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d887b85-bdcd-4752-a579-f932e2138ae4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "What happens if, instead, we ask a question or give an instruction? As the model has not been instruction tuned, these will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5709d467-28b5-48a6-92f3-fa7c5e09b55a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I make coffee with a Chemex coffee maker?\n",
      "Step 1: Fill the filter basket with ground coffee. Step 2: Pour hot water into the reservoir and let it sit for about 30 seconds to infuse the grounds. Step 3: Add more hot water, if needed. Step 4: Stir the mixture until it is evenly distributed throughout the grounds.\n",
      "What are the best Chemex filters?\n",
      "The Best Chemex Filters of 2021 â€“ Reviewed & Compared.\n",
      "Can you use regular coffee filters in a Chemex?\n",
      "You can\n"
     ]
    }
   ],
   "source": [
    "# Question\n",
    "print(generate(\"How do I make coffee with a Chemex coffee maker?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24e59d5f-ef82-4cfb-b2ae-5c5bdfa2044b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me how to make coffee with a Chemex coffee maker.\n",
      "I have a chemex and I love it! It's so easy to use, and the results are great. I would definitely recommend it to anyone who is looking for a good coffee maker.\n"
     ]
    }
   ],
   "source": [
    "# Instruction\n",
    "print(generate(\"Tell me how to make coffee with a Chemex coffee maker.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b845dc87-6319-4f33-a3c9-0da15ea165b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "These did not work because the model has not been instruction tuned. Our task is to change that!\n",
    "\n",
    "## Get and Explore the Data\n",
    "\n",
    "As in the [tinyllama](tinyllama) notebook will be using the [SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca) dataset, which contains 518,000 instruction and response pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38515af3-6113-4550-8726-f32e46a0b65d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "slimorca = load_dataset('Open-Orca/SlimOrca',\n",
    "                           cache_dir=str(Path(CACHE_DIR) / \"data\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fe430a4-b83d-425e-9a17-9f8513d07df8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here's one record from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e762f9f-cde0-40e4-9f96-5bfb979456fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"conversations\": [\n",
      "        {\n",
      "            \"from\": \"system\",\n",
      "            \"value\": \"You are an AI assistant. You will be given a task. You must generate a detailed and long answer.\",\n",
      "            \"weight\": null\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"human\",\n",
      "            \"value\": \"Write an article based on this \\\"A man has been charged with murder and attempted murder after a woman and the man she was on a date with were stabbed at a restaurant in Sydney, Australia.\\\"\",\n",
      "            \"weight\": 0.0\n",
      "        },\n",
      "        {\n",
      "            \"from\": \"gpt\",\n",
      "            \"value\": \"Title: Tragedy Strikes in Sydney: Victims Stabbed during Dinner Date, Suspect Charged with Murder and Attempted Murder\\n\\nIntroduction\\n\\nA seemingly ordinary evening took a tragic turn in Sydney, Australia when a romantic dinner date at a local restaurant ended in bloodshed, as a man allegedly attacked a couple, stabbing both victims. Local authorities have charged the suspect with murder and attempted murder in connection with the horrific crime. In this article, we delve into the chilling incident, the suspect, and the impact it has had on the community.\\n\\nThe Incident\\n\\nThe alarming event took place at a popular restaurant in Sydney, Australia. A woman and her date were enjoying their meal \\u2013 blissfully unaware of the tragic circumstances that would soon unfold. As they shared conversation and indulged in their dinner, a man approached the couple and, without warning, began to stab them with a sharp object.\\n\\nPatrons at the restaurant scrambled for safety, as the assailant continued his violent rampage. Bystanders quickly called emergency services, who promptly arrived at the scene. Although both victims were rushed to a nearby hospital, the woman succumbed to her injuries. The man she was on a date with remained in critical condition, fighting for his life.\\n\\nThe Suspect\\n\\nInitial investigations revealed that the alleged attacker had no known connections to the victims \\u2013 adding to the mystifying nature of this sudden and brutal assault. Witnesses reported that the assailant seemed to have no specific motive and appeared to carry out the act senselessly.\\n\\nFollowing a thorough investigation, local police identified and arrested the suspect. During the process, it was discovered that the alleged attacker had a history of criminal behavior and a troubled past, though it is unclear if this played a role in the tragic incident.\\n\\nAuthorities have formally charged the man with murder and attempted murder in connection with the heinous crime. He awaits a hearing to determine a trial date and, if convicted, could face a life sentence in prison.\\n\\nThe Community's Response\\n\\nThe shocking nature of the crime has left the residents of Sydney reeling, as they struggle to come to terms with the harrowing event. The restaurant where the attack occurred has since been closed, with a makeshift memorial being created outside to commemorate the victims.\\n\\nMany have questioned how such a vicious crime could happen in what is considered to be one of the safest cities in the world. This tragic event has spurred local officials to reassess current security measures and devise strategies to reinforce public safety. Additionally, discussions surrounding mental health and criminal rehabilitation have surfaced as residents seek to comprehend the actions of the alleged perpetrator and prevent similar incidents from occurring in the future.\\n\\nIn the wake of the stabbing, the community has banded together with an outpouring of grief and support for the victims and their families. Candlelight vigils have been held, and an online fundraising campaign is underway to assist the surviving victim with his medical expenses and recovery.\\n\\nConclusion\\n\\nThe tragic attack in Sydney serves as a chilling reminder that senseless acts of violence can happen anywhere and at any time. The community's response to this horrific and seemingly random act of brutality has been one of solidarity and determination to prevent such incidents in the future. As the case unfolds, the victims and their families remain in the hearts of the community, who are grieving the devastating loss of a life cut tragically short and supporting the recovering victim as he continues to endure this unimaginable ordeal.\",\n",
      "            \"weight\": 1.0\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(slimorca[\"train\"][0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f1c0e6e-730d-46f6-9d04-ad02710ca401",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Format the Data\n",
    "\n",
    "We format the data in much the same way as in the [tinyllama](tinyllama) notebook. However, there are a few differences to note.\n",
    "- The default chat template is different, using the `<|im_start|>` and `<|im_end|>` special tokens\n",
    "- the template did not, by default, add an `<|endoftext|>` token at the end of the chat, so we needed to do this manually. Without training on data including the `<|endoftext|>` token, at inference time, the model just keeps generating until it hits the token limit instead of stopping naturally after addressing the instruction.\n",
    "\n",
    "#### Examine the chat template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec6a1b0d-cc0e-40ea-96d7-9f306040687c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.chat_template), print(tokenizer.default_chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4238ba27-9179-4864-b2c0-1172b1df0db2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There is no chat template defined for this tokenizer, so we'll use the default, which is the [ChatML](https://github.com/MicrosoftDocs/azure-docs/blob/main/articles/ai-services/openai/includes/chat-markup-language.md) format. In order to use the template, we first need to adjust the slimorca records to match the following format, with `role` and `content` instead of `from` and `value` keys, and `system`/`assistant`/`user` roles instead of `system`/`gpt`/`human`. The chat is still structured as a list of dictionaries. Here's an example of a chat in the expected format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd6d125-3811-465a-b6cc-8211e0a08c26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chat = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant and an expert at making coffee.\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I make coffee with a Chemex coffee maker?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"To make coffee with a Chemex:\\n1. Boil water to about 200Â°F (93Â°C).\\n2. Place the Chemex filter in the top and rinse it with hot water to remove paper taste and warm the vessel. Discard the rinse water.\\n3. Add coffee grounds to the filter. Use a medium-coarse grind, about 1 gram of coffee per 16 grams of water.\\n4. Pour just enough hot water to saturate the grounds. Wait 30 seconds for the coffee to 'bloom'.\\n5. Slowly pour the remaining water over the grounds in a circular motion. Aim for a total brew time of 3.5 to 4.5 minutes.\\n6. Once brewing is complete, remove the filter and enjoy.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99cf4b61-4027-483d-8394-34e868916ba4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we can apply the chat template and obtain a string-formatted chat that we can tokenize and train on. Note the lack of a token indicating the end of the string! We will need to add the `tokenizer.eos_token` to the end of the string manually. This tokenizer did not define a `bos_token`, so we will proceed without one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19efb2b5-03a0-4593-88d5-6406f352fae2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant and an expert at making coffee.<|im_end|>\n",
      "<|im_start|>user\n",
      "How do I make coffee with a Chemex coffee maker?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "To make coffee with a Chemex:\n",
      "1. Boil water to about 200Â°F (93Â°C).\n",
      "2. Place the Chemex filter in the top and rinse it with hot water to remove paper taste and warm the vessel. Discard the rinse water.\n",
      "3. Add coffee grounds to the filter. Use a medium-coarse grind, about 1 gram of coffee per 16 grams of water.\n",
      "4. Pour just enough hot water to saturate the grounds. Wait 30 seconds for the coffee to 'bloom'.\n",
      "5. Slowly pour the remaining water over the grounds in a circular motion. Aim for a total brew time of 3.5 to 4.5 minutes.\n",
      "6. Once brewing is complete, remove the filter and enjoy.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d164cc52-4900-4429-af99-698176c77242",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Apply the template to the whole dataset\n",
    "\n",
    "Now we need to apply the template to the whole slimorca dataset. We will first convert the slimorca entries into the expected format, and then use `tokenizer.apply_chat_template` to apply the template.\n",
    "\n",
    "Unlike in the tinyllama notebook, we do not need to add new tokens to the vocabulary. The necessary tokens (`<|im_start|>`, `<|im_end|>`, and `<|endoftext|>`) are already defined in the tokenizer. Also unlike the tinyllama notebook, we add the `tokenizer.eos_token` to the end of the string here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6d4e43d-a1bf-41c3-8fe8-d4996f4181ff",
     "showTitle": true,
     "title": "Tokenize SLIMORCA Conversations"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def format_slimorca(ex):\n",
    "    role_mapping = {\"gpt\": \"assistant\", \"system\": \"system\", \"human\": \"user\"}\n",
    "    chat = [\n",
    "        {\"role\": role_mapping[message[\"from\"]], \"content\": message[\"value\"]}\n",
    "        for message in ex[\"conversations\"]\n",
    "    ]\n",
    "    formatted_chat = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        tokenize=False,  # Apply formatting but do not tokenize\n",
    "        add_generation_prompt=False,\n",
    "    ) + tokenizer.eos_token # add the end of sequence token\n",
    "\n",
    "    # Tokenize using the standard tokenizer method\n",
    "    tokenized_output = tokenizer(\n",
    "        formatted_chat,\n",
    "        add_special_tokens=False,  # apply_chat_template already added special tokens\n",
    "        padding=\"max_length\",  # pad to the specified length\n",
    "        max_length=512,  # max length at which to truncate or to which to pad\n",
    "        truncation=True,  # truncate to the specified length\n",
    "    )\n",
    "\n",
    "    return tokenized_output\n",
    "\n",
    "\n",
    "# Map to the dataset\n",
    "slimorca_tokenized = slimorca.map(format_slimorca, num_proc=16).remove_columns(\n",
    "    \"conversations\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5596ff0-8aa7-48c5-8c61-d8a0582fb172",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 517982\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slimorca_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c575fde4-62af-4b8d-bd12-ac9981f8bc2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now let's inspect a single example and make sure it corresponds to the format we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fce41a8-3289-40e1-a70f-23a22c5e9714",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.<|im_end|>\n",
      "<|im_start|>user\n",
      "Read this: From 1981 to 2010, the average annual precipitation measured at Seattleâ€“Tacoma International Airport was 37.49 inches (952 mm). Annual precipitation has ranged from 23.78 in (604 mm) in 1952 to 55.14 in (1,401 mm) in 1950; for water year (October 1 â€“ September 30) precipitation, the range is 23.16 in (588 mm) in 1976â€“77 to 51.82 in (1,316 mm) in 1996â€“97. Due to local variations in microclimate, Seattle also receives significantly lower precipitation than some other locations west of the Cascades. Around 80 mi (129 km) to the west, the Hoh Rain Forest in Olympic National Park on the western flank of the Olympic Mountains receives an annual average precipitation of 142 in (3.61 m). Sixty miles to the south of Seattle, the state capital Olympia, which is out of the Olympic Mountains' rain shadow, receives an annual average precipitation of 50 in (1,270 mm). The city of Bremerton, about 15 mi (24 km) west of downtown Seattle, receives 56.4 in (1,430 mm) of precipitation annually.\n",
      "\n",
      "What is the average rainfall in Seattle? \n",
      "What is the answer? (If it cannot be answered, return \"unanswerable\")<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The average annual precipitation measured at Seattle-Tacoma International Airport from 1981 to 2010 was 37.49 inches (952 mm).\n",
      "\n",
      "The answer is 37.49 inches (952 mm).<|im_end|>\n",
      "<|endoftext|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|>\n"
     ]
    }
   ],
   "source": [
    "# Inspect one example\n",
    "print(tokenizer.decode(slimorca_tokenized[\"train\"][25]['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1e1f02-1e32-4145-97df-9b683bdea801",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note the padding tokens at the end. The whole example was shorter than 512 tokens, so it was padded to reach 512 tokens.\n",
    "\n",
    "#### Split the dataset into training and validation\n",
    "\n",
    "Here we also limit to a training subset of 5,000 examples. This is based on the [LIMIT](https://www.databricks.com/blog/limit-less-more-instruction-tuning) paper, which found that a small number of high-quality examples is sufficient for instruction-tuning. Note that, under ideal circumstances, we would choose more *domain-specific* examples with a variety of different formats. Given that we are not tailoring this fine-tuning job for a specific domain, we will just choose 5,000 random examples from the SlimOrca dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af93c8b3-fd06-4032-a17e-6be9382c505d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Assume slimorca_tokenized['train'] is your initial dataset\n",
    "# First, perform the initial train-test split as before\n",
    "slimorca_tokenized_split = slimorca_tokenized['train'].train_test_split(test_size=0.1)\n",
    "\n",
    "# Subset the training set to 5000 examples\n",
    "train_subset = slimorca_tokenized_split[\"train\"].select(range(5000))\n",
    "\n",
    "# Subset the test set to 500 examples\n",
    "test_subset = slimorca_tokenized_split[\"test\"].select(range(500))\n",
    "\n",
    "# Create a new DatasetDict with these subsets\n",
    "slimorca_tokenized_split_subset = DatasetDict({\n",
    "    \"train\": train_subset,\n",
    "    \"valid\": test_subset,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d83bb55-deea-493c-a73c-1b47a0ca276f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we will configure a *collator*. The collator is responsible for taking inputs, generating labels, and assembling the inputs into batches. See the [data preprocessing notebook](../3_tinyllama_instruction_tune/data_preprocessing.ipynb) for more details.\n",
    "\n",
    "Since we already padded/truncated the inputs to the same lengths, we don't need anything special here. The `DataCollatorForLanguageModeling` collator will add labels to each entry. Importantly, the labels are the same as the inputs. The trainer handles shifting the labels; we do not need to implement any custom logic to align the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d9c337c-de67-42d1-8728-9b3c4dd64c74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b76522-bc8f-49cc-9b42-52d0b38d53fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Fine-tune the model\n",
    "\n",
    "Now that the data are ready, we can train the model using the Hugging Face `Trainer`. This part is essentially the same as in the [tinyllama](tinyllama) example.\n",
    "\n",
    "### Hyperparameters and Training Arguments\n",
    "At a high level: this is a fairly naive fine-tuning job. We aren't trying to excel at a specific benchmark or task. Our main goal is simply equipping the model with the ability to respond to instructions and questions in an appropriate format. We attain this goal fairly easily with a variety of different hyperparameter configurations.\n",
    "- We set `auto_find_batch_size` to `True`. The trainer will try multiple batch sizes, starting from the specified `per_device_train_batch_size`, and reduce the batch size if it encounters an OOM error.\n",
    "- We use gradient accumulation to simulate a larger batch size. Gradients are accumulated over multiple mini-batches of data (because we cannot use a very large batch size). The weights are only updated after the specified number of gradient accumulation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aa3f56c-53cb-4f40-831a-d92f66fdd9b8",
     "showTitle": true,
     "title": "MLflow Training with Transformers"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import mlflow\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=4,\n",
    "    auto_find_batch_size=True,\n",
    "    warmup_steps=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=LOG_DIR,\n",
    "    logging_steps=5,\n",
    "    evaluation_strategy=\"steps\",  # Evaluate every 'eval_steps'\n",
    "    eval_steps=100,\n",
    "    bf16=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=False,\n",
    "    save_steps=5000,\n",
    ")\n",
    "\n",
    "training_args.set_logging(report_to=[\"mlflow\"], steps=5, level=\"info\")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=slimorca_tokenized_split_subset[\"train\"],\n",
    "    eval_dataset=slimorca_tokenized_split_subset[\"valid\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training and track with MLflow\n",
    "with mlflow.start_run(log_system_metrics=True):\n",
    "    trainer.train()\n",
    "    mlflow.log_params(training_args.to_dict())\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR + \"/final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "893a9b35-95c8-4656-9300-eeb3adf2be4b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load the Fine-Tuned Model Checkpoint and Run some Examples\n",
    "\n",
    "Here are some example outputs from the fine-tuned olmo-1b model (including the prompts and instruction tokens):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<|im_start|>system\n",
    "You are a food critic with strong opinions.<|im_end|>\n",
    "<|im_start|>user\n",
    "What style of bagel is better, Montreal or New York? Answer briefly.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Montreal's bagels tend to be more flavorful and have softer texture than their New York counterparts. This difference in taste can vary depending on the type of flour used for baking them as well as how they're prepared (e.g., boiled water vs steamed). However, both styles offer unique flavors that cater to different tastes while maintaining consistency across regions. In general, it would depend on personal preference which one you prefer over another based on your own preferences!<|im_end|>\n",
    "<|endoftext|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction tuning on general instruction/response data without domain specificity cannot compensate for the fact that this is a 1 billion parameter model that does not know how to make coffee.\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "You are a helpful assistant and an expert at making coffee.<|im_end|>\n",
    "<|im_start|>user\n",
    "Tell me how to make coffee with a Chemex coffee maker.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Making coffee using the Chemex Coffee Maker is easy, but it's important to follow these steps: \n",
    "\n",
    "1. Choose your beans â€“ Before you start brewing, choose the right type of coffee bean for this particular brewed coffee. You can find different types in various grocery stores or online shopping platforms like Amazon. Make sure that they're fresh (preferably within 2-3 days) because stale coffee will not taste good when brewed on a regular basis.\n",
    "2. Prepare the brew â€“ Fill up all necessary measuring scoops into the filter basket before adding ground coffee. This ensures proper extraction by ensuring there isn't too much watery grounds left behind after filtering out excess liquid from the grinds. If possible, use a scale instead of weighing directly onto the countertop since scales tend to be more accurate than hand measurements.\n",
    "3. Brew slowly â€“ Pour about 1/4 cup (60 ml) of freshly brewed coffee per serving. The amount may vary depending on personal preference; some people prefer less flavorful coffees while others enjoy stronger flavors. It's also essential to remember that if you have multiple cups of coffee served simultaneously during one session, then each person should get their own separate pot so as not to overpower everyone else!\n",
    "4. Add hot water â€“ After pouring enough coffee through the filter, add warm water back into the machine until the desired temperature has been reached without any lumps present. Be careful not to pour boiling water straight down the drain line, which could potentially damage the valve mechanism inside the unit due to heat buildup caused by excessive pressure generated by high temperatures. Instead, let the system cool off slightly first before turning on steam production mode again.\n",
    "5. Close the lid and serve immediately â€“ Once everything looks clean and ready, close the door securely and turn on the power switch located underneath the handle. Wait patiently for approximately 10 minutes for the coffee to steepen naturally, allowing time for air bubbles to form around the top cap area where the milk might settle. Then gently lift the cover away once steeping process is complete. Enjoying your deliciously brewed coffee now! Remember, always check the manufacturer's instructions regarding recommended storage times for both preheated and cold beverages such as keeping them refrigerated between 0Â°C - 5Â°F (-32Â°C), otherwise known as \"cold\" storage period.<|im_end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4. instruction tuning olmo on a single GPU",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
