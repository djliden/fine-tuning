{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(olmo_peft)=\n",
    "# QLoRA on OLMo-1B\n",
    "\n",
    "In this notebook, we show how to fine-tune OLMo-1B with QLoRA.\n",
    "\n",
    "## Intro to QLoRA\n",
    "\n",
    "QLoRA is a parameter-efficient fine-tuning approach. It involves loading a quantized model (with 8-bit or 4-bit weights), freezing the weights, and backpropagating the gradients through the frozen model into low-rank adapters (LoRA).\n",
    "\n",
    "For more, check out the [GitHub Repo](https://github.com/artidoro/qlora?tab=readme-ov-file), which also links to the QLoRA paper and other resources.\n",
    "\n",
    "## Fine-tuning task\n",
    "\n",
    "In this example, we're going to try to fine-tune OLMo 1b on a task many models struggle with: changing the letter e to 3 in arbitrary input text. We'll use the wikitext dataset to do this, and we will process each entry such that it combines a \"normal\" input with an output with the letter e replaced with 3.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we'll install the required dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c73d0e-9914-4437-9b1b-066109b4537d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ./olmo_peft_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set up the [`LoraConfig`](https://huggingface.co/docs/peft/en/package_reference/lora#peft.LoraConfig). This config specifies which layers we apply the LoRA adapters to (`target_modules`); the rank and scaling factor for the adapters (`r` and `lora_alpha`); and the dropout rate (`lora_dropout`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fabc1358-403a-40b6-8246-e833ae68c1e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 14:04:56.582847: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-10 14:04:56.582919: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-10 14:04:56.582943: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-10 14:04:56.589971: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Unexpected internal error when monkey patching `Trainer.train`: Failed to import transformers.trainer because of the following error (look up to see its traceback):\n",
      "cannot import name 'PeftModel' from partially initialized module 'peft' (most likely due to a circular import) (/local_disk0/.ephemeral_nfs/envs/pythonEnv-884c91c9-7d68-405b-998e-c6cd56c7bfe3/lib/python3.10/site-packages/peft/__init__.py)\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    target_modules=[\"att_proj\", \"ff_proj\"],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22372d23-9401-40b5-b89d-31a805a181d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import hf_olmo\n",
    "from hf_olmo import *\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf6e63c9-9dac-4558-9d59-d14c8b5c2edd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Some weights of OLMoForCausalLM were not initialized from the model checkpoint at allenai/OLMo-1B and are newly initialized: ['model.transformer.ff_out.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-1B\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             cache_dir = \"/Volumes/daniel_liden/fine_tuning/assets\",\n",
    "                                             device_map=\"auto\",\n",
    "                                             load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0b96625-d26e-4f00-b206-9397fd81365b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model.add_adapter(lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340e4dc6-1696-4251-bce5-44ad7aaf590e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:27: UserWarning: This dataset can not be stored in DBFS because either `cache_dir` or the environment variable `HF_DATASETS_CACHE` is set to a non-DBFS path. If this cluster restarts, all saved dataset information will be lost.\n",
      "  warnings.warn(\n",
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:13: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c487a04bc4842ad8ad06b7e070970e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75e47e3b68394ce5842f699f4eda8d77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "# Load the WikiText-2 dataset\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    # Split the example into individual lines\n",
    "    lines = example[\"text\"].split(\"\\n\")\n",
    "    \n",
    "    # Remove empty lines and lines starting with ' ='\n",
    "    filtered_lines = [line for line in lines if line.strip() and not line.startswith(' =')]\n",
    "    \n",
    "    # Join the filtered lines back into a single string\n",
    "    text = \"\\n\".join(filtered_lines)\n",
    "    \n",
    "    input_text = \"Replace all es or Es with 3s in the following text.\\n\\n### Input:\\n\" + text + \"\\n\\n### Output:\\n\"\n",
    "    output_text = text.replace(\"e\", \"3\").replace(\"E\", \"3\") # + \"<|endoftext|>\"\n",
    "    \n",
    "    return tokenizer(input_text + output_text, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize the train and validation splits\n",
    "tokenized_train = wikitext[\"train\"].map(tokenize_function, num_proc=4, remove_columns=[\"text\"])\n",
    "tokenized_validation = wikitext[\"validation\"].map(tokenize_function, num_proc=4, remove_columns=[\"text\"])\n",
    "\n",
    "# Shuffle the datasets\n",
    "tokenized_train = tokenized_train.shuffle(seed=42)\n",
    "tokenized_validation = tokenized_validation.shuffle(seed=42)\n",
    "\n",
    "# Select the desired number of examples\n",
    "train_dataset = tokenized_train.select(range(8000))\n",
    "eval_dataset = tokenized_validation.select(range(2000))\n",
    "\n",
    "# Create a DatasetDict with the selected subsets\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"eval\": eval_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72426a90-e2de-426d-b54b-b8af2c73d65d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f858e5-2215-4656-a5a2-a6fd7d8b2b4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-884c91c9-7d68-405b-998e-c6cd56c7bfe3/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "# Define the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Volumes/daniel_liden/fine_tuning/assets\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    save_steps=250,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict['train'],\n",
    "    eval_dataset=dataset_dict['eval'],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "470d23ae-3a4d-4fd0-bf2b-61542fb4713f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/10 14:10:23 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 05:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='358' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [358/750 28:38 < 31:32, 0.21 it/s, Epoch 1.43/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.252400</td>\n",
       "      <td>1.297852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.970000</td>\n",
       "      <td>1.192383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.102500</td>\n",
       "      <td>1.148438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.040300</td>\n",
       "      <td>1.120117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.132800</td>\n",
       "      <td>1.105469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.126600</td>\n",
       "      <td>1.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.047500</td>\n",
       "      <td>1.083984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-884c91c9-7d68-405b-998e-c6cd56c7bfe3/lib/python3.10/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Start training and track with MLflow\n",
    "with mlflow.start_run(log_system_metrics=True):\n",
    "    trainer.evaluate() # eval before starting tuning\n",
    "    trainer.train()\n",
    "    mlflow.log_params(training_args.to_dict())\n",
    "\n",
    "trainer.save_model(\"/Volumes/daniel_liden/fine_tuning/assets\" + \"/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = AutoModelForCausalLM.from_pretrained(\"/Volumes/daniel_liden/fine_tuning/assets/final/\", device_map=\"auto\", load_in_8bit=True)\n",
    "\n",
    "prompt_template = \"\"\"\"Replace all es or Es with 3s in the following text.\\n\\n### Input:\\n {prompt} \\n\\n### Output:\\n\"\"\"\n",
    "\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = \"Replace all es or Es with 3s in the following text.\\n\\n### Input:\\n{prompt}\\n\\n### Output:\\n\"\n",
    "\n",
    "# Define the sample input/output\n",
    "sample_input = \"The quick brown fox jumps over the lazy dog.\\nElephants are the largest land mammals on Earth.\\nThe Earth revolves around the Sun, which is a star.\"\n",
    "sample_output = prompt_template.format(prompt=sample_input) + \"Th3 quick brown fox jumps ov3r th3 lazy dog.\\n3l3phants ar3 th3 larg3st land mammals on 3arth.\\nTh3 3arth r3volv3s around th3 Sun, which is a star.<|endoftext|>\"\n",
    "\n",
    "# Define the sample parameters\n",
    "sample_params = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "}\n",
    "\n",
    "# MLflow infers schema from the provided sample input/output/params\n",
    "signature = infer_signature(\n",
    "    model_input=sample_input,\n",
    "    model_output=sample_output,\n",
    "    params=sample_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the ID of the MLflow Run that was automatically created above\n",
    "last_run_id = mlflow.last_active_run().info.run_id\n",
    "\n",
    "with mlflow.start_run(run_id=last_run_id):\n",
    "    mlflow.log_params(lora_config.to_dict())\n",
    "    mlflow.transformers.log_model(\n",
    "        transformers_model={\"model\": final_model, \"tokenizer\": tokenizer},\n",
    "        signature=signature,\n",
    "        artifact_path=\"model\",  # This is a relative path to save model files within MLflow run\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1623462c-25ba-4b4e-b65f-20685d2f79aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import hf_olmo\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47f73ed0-6ce8-485c-8ecd-237117ea67ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_model = AutoModelForCausalLM.from_pretrained(\"/Volumes/daniel_liden/datasets/h2o_rag/output/checkpoint-500/\", load_in_8bit=True,\n",
    "                                                  device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2edd82ac-80a8-473c-888e-ead32f857bee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "964d128a-3741-4eab-82fa-8e41d358afc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate(input_text, max_new_tokens=100):\n",
    "    # Create the prompt template\n",
    "    prompt_template = \"Replace all es or Es with 3s in the following text.\\n\\n### Input:\\n{input_text}\\n\\n### Output:\\n\"\n",
    "    \n",
    "    # Format the prompt with the input text\n",
    "    formatted_prompt = prompt_template.format(input_text=input_text)\n",
    "    \n",
    "    # Tokenize the formatted prompt\n",
    "    input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(peft_model.device)\n",
    "    \n",
    "    # Generate the output using the trained model\n",
    "    gen_tokens = peft_model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    \n",
    "    # Decode the generated output\n",
    "    generated_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=False)[0]\n",
    "    \n",
    "    # Extract the generated output after \"### Output:\"\n",
    "    generated_output = generated_text.split(\"### Output:\")[-1].strip()\n",
    "    \n",
    "    return generated_output\n",
    "\n",
    "# Example usage\n",
    "example_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "\"\"\"\n",
    "\n",
    "# Generate the output using the trained model\n",
    "generated_output = generate(example_text)\n",
    "\n",
    "print(\"Generated Output:\")\n",
    "print(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ef12ea2-78e5-4aea-9be5-8a5da6650c26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate(\"\"\"Tear the emblems from our sleeves just like the others / Apostate Brothers please stay for the dawn of a new day / Watch the sun come up from the mud / Our cups are empty / Our wine has turned to ether that's good and fine\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c40ac79f-2852-4e8e-a4da-1449a402d7e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1516462198892786,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "OLMo peft",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
