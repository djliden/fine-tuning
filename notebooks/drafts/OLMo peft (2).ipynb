{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97c73d0e-9914-4437-9b1b-066109b4537d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: transformers in /databricks/python3/lib/python3.10/site-packages (4.34.0)\nCollecting transformers\n  Using cached transformers-4.39.3-py3-none-any.whl (8.8 MB)\nRequirement already satisfied: accelerate in /databricks/python3/lib/python3.10/site-packages (0.23.0)\nCollecting accelerate\n  Using cached accelerate-0.29.2-py3-none-any.whl (297 kB)\nCollecting peft\n  Using cached peft-0.10.0-py3-none-any.whl (199 kB)\nCollecting ai2-olmo\n  Using cached ai2_olmo-0.2.5-py3-none-any.whl (118 kB)\nCollecting bitsandbytes\n  Using cached bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\nCollecting mlflow\n  Using cached mlflow-2.11.3-py3-none-any.whl (19.7 MB)\nCollecting pynvml\n  Using cached pynvml-11.5.0-py3-none-any.whl (53 kB)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from transformers) (3.9.0)\nCollecting safetensors>=0.4.1\n  Using cached safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /databricks/python3/lib/python3.10/site-packages (from transformers) (0.14.0)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.10/site-packages (from transformers) (2022.7.9)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.10/site-packages (from transformers) (22.0)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.10/site-packages (from transformers) (6.0)\nCollecting huggingface-hub<1.0,>=0.19.3\n  Using cached huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\nRequirement already satisfied: torch>=1.10.0 in /databricks/python3/lib/python3.10/site-packages (from accelerate) (2.0.1+cu118)\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.10/site-packages (from accelerate) (5.9.0)\nCollecting rich\n  Using cached rich-13.7.1-py3-none-any.whl (240 kB)\nCollecting cached-path>=1.6.2\n  Using cached cached_path-1.6.2-py3-none-any.whl (35 kB)\nRequirement already satisfied: boto3 in /databricks/python3/lib/python3.10/site-packages (from ai2-olmo) (1.24.28)\nCollecting omegaconf\n  Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\nRequirement already satisfied: google-cloud-storage in /databricks/python3/lib/python3.10/site-packages (from ai2-olmo) (2.11.0)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.4.39)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (8.0.4)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (0.4.2)\nCollecting graphene<4\n  Using cached graphene-3.3-py2.py3-none-any.whl (128 kB)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (3.1.2)\nCollecting querystring-parser<2\n  Using cached querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.1.1)\nRequirement already satisfied: gunicorn<22 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (20.1.0)\nRequirement already satisfied: Flask<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (2.2.5)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (3.1.27)\nRequirement already satisfied: markdown<4,>=3.3 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (3.4.1)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.10.0)\nCollecting docker<8,>=4.0.0\n  Using cached docker-7.0.0-py3-none-any.whl (147 kB)\nRequirement already satisfied: pyarrow<16,>=4.0.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (8.0.0)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (0.4)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (1.5.3)\nCollecting alembic!=1.10.0,<2\n  Using cached alembic-1.13.1-py3-none-any.whl (233 kB)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (3.7.0)\nRequirement already satisfied: pytz<2025 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (2022.7)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (4.24.0)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (2.0.0)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /databricks/python3/lib/python3.10/site-packages (from mlflow) (4.11.3)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (1.2.0)\nRequirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow) (4.4.0)\nCollecting huggingface-hub<1.0,>=0.19.3\n  Using cached huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /databricks/python3/lib/python3.10/site-packages (from boto3->ai2-olmo) (0.6.2)\nRequirement already satisfied: botocore<1.28.0,>=1.27.28 in /databricks/python3/lib/python3.10/site-packages (from boto3->ai2-olmo) (1.27.96)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.10/site-packages (from boto3->ai2-olmo) (0.10.0)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.10/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.14)\nRequirement already satisfied: Werkzeug>=2.2.2 in /databricks/python3/lib/python3.10/site-packages (from Flask<4->mlflow) (2.2.2)\nRequirement already satisfied: itsdangerous>=2.0 in /databricks/python3/lib/python3.10/site-packages (from Flask<4->mlflow) (2.0.1)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow) (4.0.11)\nRequirement already satisfied: google-resumable-media>=2.6.0 in /databricks/python3/lib/python3.10/site-packages (from google-cloud-storage->ai2-olmo) (2.6.0)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /databricks/python3/lib/python3.10/site-packages (from google-cloud-storage->ai2-olmo) (2.3.3)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /databricks/python3/lib/python3.10/site-packages (from google-cloud-storage->ai2-olmo) (2.14.0)\nRequirement already satisfied: google-auth<3.0dev,>=1.25.0 in /databricks/python3/lib/python3.10/site-packages (from google-cloud-storage->ai2-olmo) (2.21.0)\nCollecting graphql-relay<3.3,>=3.1\n  Using cached graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nCollecting graphql-core<3.3,>=3.1\n  Using cached graphql_core-3.2.3-py3-none-any.whl (202 kB)\nCollecting aniso8601<10,>=8\n  Using cached aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\nRequirement already satisfied: setuptools>=3.0 in /databricks/python3/lib/python3.10/site-packages (from gunicorn<22->mlflow) (65.6.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /databricks/python3/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow) (3.11.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from Jinja2<4,>=2.11->mlflow) (2.1.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.4.4)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (9.4.0)\nRequirement already satisfied: contourpy>=1.0.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (1.0.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (3.0.9)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (4.25.0)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (2.8.2)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.10/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from querystring-parser<2->mlflow) (1.16.0)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\nCollecting pygments<3.0.0,>=2.13.0\n  Using cached pygments-2.17.2-py3-none-any.whl (1.2 MB)\nCollecting markdown-it-py>=2.2.0\n  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\nRequirement already satisfied: joblib>=1.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from scikit-learn<2->mlflow) (2.2.0)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (2.0.1)\nCollecting tokenizers<0.19,>=0.14\n  Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\nRequirement already satisfied: sympy in /databricks/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.11.1)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.8.4)\nRequirement already satisfied: triton==2.0.0 in /databricks/python3/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.0.0)\nRequirement already satisfied: lit in /databricks/python3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.5)\nRequirement already satisfied: cmake in /databricks/python3/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.7)\nCollecting antlr4-python3-runtime==4.9.*\n  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow) (5.0.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /databricks/python3/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage->ai2-olmo) (1.61.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->ai2-olmo) (5.3.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->ai2-olmo) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage->ai2-olmo) (4.9)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /databricks/python3/lib/python3.10/site-packages (from google-resumable-media>=2.6.0->google-cloud-storage->ai2-olmo) (1.5.0)\nCollecting mdurl~=0.1\n  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nRequirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.2.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage->ai2-olmo) (0.4.8)\nInstalling collected packages: antlr4-python3-runtime, aniso8601, safetensors, querystring-parser, pynvml, pygments, omegaconf, mdurl, graphql-core, markdown-it-py, huggingface-hub, graphql-relay, docker, alembic, tokenizers, rich, graphene, transformers, mlflow, cached-path, accelerate, peft, bitsandbytes, ai2-olmo\n  Attempting uninstall: safetensors\n    Found existing installation: safetensors 0.4.0\n    Not uninstalling safetensors at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-cf66221c-4e25-42fd-906e-2aa6106d29b9\n    Can't uninstall 'safetensors'. No files were found to uninstall.\n  Attempting uninstall: pygments\n    Found existing installation: Pygments 2.11.2\n    Not uninstalling pygments at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-cf66221c-4e25-42fd-906e-2aa6106d29b9\n    Can't uninstall 'Pygments'. No files were found to uninstall.\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.16.4\n    Not uninstalling huggingface-hub at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-cf66221c-4e25-42fd-906e-2aa6106d29b9\n    Can't uninstall 'huggingface-hub'. No files were found to uninstall.\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.14.0\n    Not uninstalling tokenizers at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-cf66221c-4e25-42fd-906e-2aa6106d29b9\n    Can't uninstall 'tokenizers'. No files were found to uninstall.\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.34.0\n    Not uninstalling transformers at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-cf66221c-4e25-42fd-906e-2aa6106d29b9\n    Can't uninstall 'transformers'. No files were found to uninstall.\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.23.0\n    Not uninstalling accelerate at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-cf66221c-4e25-42fd-906e-2aa6106d29b9\n    Can't uninstall 'accelerate'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nnbclassic 0.5.2 requires prometheus-client, which is not installed.\njupyter-server 1.23.4 requires prometheus-client, which is not installed.\ndatabricks-feature-store 0.16.3 requires pyspark<4,>=3.1.2, which is not installed.\nSuccessfully installed accelerate-0.29.2 ai2-olmo-0.2.5 alembic-1.13.1 aniso8601-9.0.1 antlr4-python3-runtime-4.9.3 bitsandbytes-0.43.0 cached-path-1.6.2 docker-7.0.0 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 huggingface-hub-0.21.4 markdown-it-py-3.0.0 mdurl-0.1.2 mlflow-2.11.3 omegaconf-2.3.0 peft-0.10.0 pygments-2.17.2 pynvml-11.5.0 querystring-parser-1.2.4 rich-13.7.1 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.39.3\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers accelerate peft ai2-olmo bitsandbytes mlflow pynvml\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fabc1358-403a-40b6-8246-e833ae68c1e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 19:41:44.772182: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-09 19:41:44.772244: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-09 19:41:44.772268: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2024-04-09 19:41:44.779373: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\nUnexpected internal error when monkey patching `Trainer.train`: Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'PeftModel' from partially initialized module 'peft' (most likely due to a circular import) (/local_disk0/.ephemeral_nfs/envs/pythonEnv-cf66221c-4e25-42fd-906e-2aa6106d29b9/lib/python3.10/site-packages/peft/__init__.py)\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"att_proj\", \"ff_proj\"],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22372d23-9401-40b5-b89d-31a805a181d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import hf_olmo\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf6e63c9-9dac-4558-9d59-d14c8b5c2edd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\nThe model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\nSome weights of OLMoForCausalLM were not initialized from the model checkpoint at allenai/OLMo-1B and are newly initialized: ['model.transformer.ff_out.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMo-1B\",\n",
    "                                             trust_remote_code=True,\n",
    "                                             cache_dir = \"/Volumes/daniel_liden/datasets/h2o_rag\",\n",
    "                                             device_map=\"auto\",\n",
    "                                             load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15b11da-3a23-47c3-ad12-be285b76ad33",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Olmo(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50304, 2048)\n",
       "    (emb_drop): Dropout(p=0.0, inplace=False)\n",
       "    (ln_f): LayerNorm()\n",
       "    (blocks): ModuleList(\n",
       "      (0-15): 16 x OlmoSequentialBlock(\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (act): SwiGLU()\n",
       "        (attn_out): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
       "        (ff_out): Linear8bitLt(in_features=8192, out_features=2048, bias=False)\n",
       "        (rotary_emb): RotaryEmbedding()\n",
       "        (attn_norm): LayerNorm()\n",
       "        (ff_norm): LayerNorm()\n",
       "        (att_proj): Linear8bitLt(in_features=2048, out_features=6144, bias=False)\n",
       "        (ff_proj): Linear8bitLt(in_features=2048, out_features=16384, bias=False)\n",
       "      )\n",
       "    )\n",
       "    (ff_out): Embedding(50304, 2048)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a1a1721-5a5a-46ff-9fb5-8e0188e17e35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 1,183,580,160 || trainable%: 0.5758582502768549\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0b96625-d26e-4f00-b206-9397fd81365b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model.add_adapter(lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "340e4dc6-1696-4251-bce5-44ad7aaf590e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:27: UserWarning: This dataset can not be stored in DBFS because either `cache_dir` or the environment variable `HF_DATASETS_CACHE` is set to a non-DBFS path. If this cluster restarts, all saved dataset information will be lost.\n  warnings.warn(\n/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:13: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "# Load the WikiText-2 dataset\n",
    "wikitext = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(example):\n",
    "    # Split the example into individual lines\n",
    "    lines = example[\"text\"].split(\"\\n\")\n",
    "    \n",
    "    # Remove empty lines and lines starting with ' ='\n",
    "    filtered_lines = [line for line in lines if line.strip() and not line.startswith(' =')]\n",
    "    \n",
    "    # Join the filtered lines back into a single string\n",
    "    text = \"\\n\".join(filtered_lines)\n",
    "    \n",
    "    input_text = \"Replace all es or Es with 3s in the following text.\\n\\n### Input:\\n\" + text + \"\\n\\n### Output:\\n\"\n",
    "    output_text = text.replace(\"e\", \"3\").replace(\"E\", \"3\") + \"<|endoftext|>\"\n",
    "    \n",
    "    return tokenizer(input_text + output_text, truncation=True, max_length=128)\n",
    "\n",
    "# Tokenize the train and validation splits\n",
    "tokenized_train = wikitext[\"train\"].map(tokenize_function, num_proc=4, remove_columns=[\"text\"])\n",
    "tokenized_validation = wikitext[\"validation\"].map(tokenize_function, num_proc=4, remove_columns=[\"text\"])\n",
    "\n",
    "# Shuffle the datasets\n",
    "tokenized_train = tokenized_train.shuffle(seed=42)\n",
    "tokenized_validation = tokenized_validation.shuffle(seed=42)\n",
    "\n",
    "# Select the desired number of examples\n",
    "train_dataset = tokenized_train.select(range(8000))\n",
    "eval_dataset = tokenized_validation.select(range(2000))\n",
    "\n",
    "# Create a DatasetDict with the selected subsets\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"eval\": eval_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72426a90-e2de-426d-b54b-b8af2c73d65d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 8000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28f858e5-2215-4656-a5a2-a6fd7d8b2b4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-cf66221c-4e25-42fd-906e-2aa6106d29b9/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \ndataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "# Define the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/Volumes/daniel_liden/datasets/h2o_rag/output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict['train'],\n",
    "    eval_dataset=dataset_dict['eval'],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "470d23ae-3a4d-4fd0-bf2b-61542fb4713f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/09 19:41:55 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 01:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 20:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.577100</td>\n",
       "      <td>1.766602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.825700</td>\n",
       "      <td>1.725586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.658200</td>\n",
       "      <td>1.712891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.665500</td>\n",
       "      <td>1.704102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.966300</td>\n",
       "      <td>1.694336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.596400</td>\n",
       "      <td>1.697266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.677700</td>\n",
       "      <td>1.700195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.702900</td>\n",
       "      <td>1.696289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.143300</td>\n",
       "      <td>1.694336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.667700</td>\n",
       "      <td>1.692383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.531500</td>\n",
       "      <td>1.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.247300</td>\n",
       "      <td>1.711914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.403700</td>\n",
       "      <td>1.711914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.639900</td>\n",
       "      <td>1.710938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.392500</td>\n",
       "      <td>1.710938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local_disk0/.ephemeral_nfs/envs/pythonEnv-cf66221c-4e25-42fd-906e-2aa6106d29b9/lib/python3.10/site-packages/transformers/integrations/peft.py:391: FutureWarning: The `active_adapter` method is deprecated and will be removed in a future version.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69183a3da5e74711ae2456ddebb24d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/09 20:02:32 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n2024/04/09 20:02:32 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Start training and track with MLflow\n",
    "with mlflow.start_run(log_system_metrics=True):\n",
    "    trainer.evaluate() # eval before starting tuning\n",
    "    trainer.train()\n",
    "    mlflow.log_params(training_args.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "964d128a-3741-4eab-82fa-8e41d358afc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-422193997890608>, line 35\u001B[0m\n",
       "\u001B[1;32m     28\u001B[0m example_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[1;32m     29\u001B[0m \u001B[38;5;124mThe quick brown fox jumps over the lazy dog.\u001B[39m\n",
       "\u001B[1;32m     30\u001B[0m \u001B[38;5;124mElephants are the largest land mammals on Earth.\u001B[39m\n",
       "\u001B[1;32m     31\u001B[0m \u001B[38;5;124mThe Earth revolves around the Sun, which is a star.\u001B[39m\n",
       "\u001B[1;32m     32\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Generate the output using the trained model\u001B[39;00m\n",
       "\u001B[0;32m---> 35\u001B[0m generated_output \u001B[38;5;241m=\u001B[39m generate(example_text)\n",
       "\u001B[1;32m     37\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerated Output:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     38\u001B[0m \u001B[38;5;28mprint\u001B[39m(generated_output)\n",
       "\n",
       "File \u001B[0;32m<command-422193997890608>, line 12\u001B[0m, in \u001B[0;36mgenerate\u001B[0;34m(input_text, max_new_tokens)\u001B[0m\n",
       "\u001B[1;32m      9\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m tokenizer(formatted_prompt, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39minput_ids\u001B[38;5;241m.\u001B[39mto(model\u001B[38;5;241m.\u001B[39mdevice)\n",
       "\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Generate the output using the trained model\u001B[39;00m\n",
       "\u001B[0;32m---> 12\u001B[0m gen_tokens \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrepetition_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.1\u001B[39;49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m     17\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Decode the generated output\u001B[39;00m\n",
       "\u001B[1;32m     20\u001B[0m generated_text \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mbatch_decode(gen_tokens, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m]\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n",
       "\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n",
       "\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n",
       "\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-cf66221c-4e25-42fd-906e-2aa6106d29b9/lib/python3.10/site-packages/transformers/generation/utils.py:1527\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1509\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massisted_decoding(\n",
       "\u001B[1;32m   1510\u001B[0m         input_ids,\n",
       "\u001B[1;32m   1511\u001B[0m         candidate_generator\u001B[38;5;241m=\u001B[39mcandidate_generator,\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   1523\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n",
       "\u001B[1;32m   1524\u001B[0m     )\n",
       "\u001B[1;32m   1525\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mGREEDY_SEARCH:\n",
       "\u001B[1;32m   1526\u001B[0m     \u001B[38;5;66;03m# 11. run greedy search\u001B[39;00m\n",
       "\u001B[0;32m-> 1527\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_greedy_search\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1528\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1529\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1530\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1531\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1532\u001B[0m \u001B[43m        \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1533\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_scores\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1534\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_logits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_logits\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1535\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1536\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1537\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1538\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   1539\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1541\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mCONTRASTIVE_SEARCH:\n",
       "\u001B[1;32m   1542\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-cf66221c-4e25-42fd-906e-2aa6106d29b9/lib/python3.10/site-packages/transformers/generation/utils.py:2411\u001B[0m, in \u001B[0;36mGenerationMixin._greedy_search\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001B[0m\n",
       "\u001B[1;32m   2408\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_inputs_for_generation(input_ids, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n",
       "\u001B[1;32m   2410\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n",
       "\u001B[0;32m-> 2411\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   2412\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   2413\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   2414\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   2415\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n",
       "\u001B[1;32m   2416\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2418\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synced_gpus \u001B[38;5;129;01mand\u001B[39;00m this_peer_finished:\n",
       "\u001B[1;32m   2419\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# don't waste resources running the code we don't need\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n",
       "\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n",
       "\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n",
       "\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n",
       "\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n",
       "\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n",
       "\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
       "\n",
       "File \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-cf66221c-4e25-42fd-906e-2aa6106d29b9/lib/python3.10/site-packages/accelerate/hooks.py:166\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    164\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 166\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: OLMoForCausalLM.forward() got an unexpected keyword argument 'cache_position'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-422193997890608>, line 35\u001B[0m\n\u001B[1;32m     28\u001B[0m example_text \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;124mThe quick brown fox jumps over the lazy dog.\u001B[39m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;124mElephants are the largest land mammals on Earth.\u001B[39m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;124mThe Earth revolves around the Sun, which is a star.\u001B[39m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Generate the output using the trained model\u001B[39;00m\n\u001B[0;32m---> 35\u001B[0m generated_output \u001B[38;5;241m=\u001B[39m generate(example_text)\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerated Output:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28mprint\u001B[39m(generated_output)\n\nFile \u001B[0;32m<command-422193997890608>, line 12\u001B[0m, in \u001B[0;36mgenerate\u001B[0;34m(input_text, max_new_tokens)\u001B[0m\n\u001B[1;32m      9\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m tokenizer(formatted_prompt, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39minput_ids\u001B[38;5;241m.\u001B[39mto(model\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Generate the output using the trained model\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m gen_tokens \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrepetition_penalty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1.1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Decode the generated output\u001B[39;00m\n\u001B[1;32m     20\u001B[0m generated_text \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mbatch_decode(gen_tokens, skip_special_tokens\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m]\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-cf66221c-4e25-42fd-906e-2aa6106d29b9/lib/python3.10/site-packages/transformers/generation/utils.py:1527\u001B[0m, in \u001B[0;36mGenerationMixin.generate\u001B[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39massisted_decoding(\n\u001B[1;32m   1510\u001B[0m         input_ids,\n\u001B[1;32m   1511\u001B[0m         candidate_generator\u001B[38;5;241m=\u001B[39mcandidate_generator,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1523\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs,\n\u001B[1;32m   1524\u001B[0m     )\n\u001B[1;32m   1525\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mGREEDY_SEARCH:\n\u001B[1;32m   1526\u001B[0m     \u001B[38;5;66;03m# 11. run greedy search\u001B[39;00m\n\u001B[0;32m-> 1527\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_greedy_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1528\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1530\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1531\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpad_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpad_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1532\u001B[0m \u001B[43m        \u001B[49m\u001B[43meos_token_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meos_token_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1533\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_scores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_scores\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1534\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_logits\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_logits\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1535\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgeneration_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreturn_dict_in_generate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1536\u001B[0m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1537\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1538\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1539\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1541\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;241m==\u001B[39m GenerationMode\u001B[38;5;241m.\u001B[39mCONTRASTIVE_SEARCH:\n\u001B[1;32m   1542\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m model_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muse_cache\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-cf66221c-4e25-42fd-906e-2aa6106d29b9/lib/python3.10/site-packages/transformers/generation/utils.py:2411\u001B[0m, in \u001B[0;36mGenerationMixin._greedy_search\u001B[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001B[0m\n\u001B[1;32m   2408\u001B[0m model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprepare_inputs_for_generation(input_ids, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_kwargs)\n\u001B[1;32m   2410\u001B[0m \u001B[38;5;66;03m# forward pass to get next token\u001B[39;00m\n\u001B[0;32m-> 2411\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2412\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2413\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   2414\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2415\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2416\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2418\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m synced_gpus \u001B[38;5;129;01mand\u001B[39;00m this_peer_finished:\n\u001B[1;32m   2419\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# don't waste resources running the code we don't need\u001B[39;00m\n\nFile \u001B[0;32m/databricks/python/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n\nFile \u001B[0;32m/local_disk0/.ephemeral_nfs/envs/pythonEnv-cf66221c-4e25-42fd-906e-2aa6106d29b9/lib/python3.10/site-packages/accelerate/hooks.py:166\u001B[0m, in \u001B[0;36madd_hook_to_module.<locals>.new_forward\u001B[0;34m(module, *args, **kwargs)\u001B[0m\n\u001B[1;32m    164\u001B[0m         output \u001B[38;5;241m=\u001B[39m module\u001B[38;5;241m.\u001B[39m_old_forward(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 166\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_old_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m module\u001B[38;5;241m.\u001B[39m_hf_hook\u001B[38;5;241m.\u001B[39mpost_forward(module, output)\n\n\u001B[0;31mTypeError\u001B[0m: OLMoForCausalLM.forward() got an unexpected keyword argument 'cache_position'",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: OLMoForCausalLM.forward() got an unexpected keyword argument 'cache_position'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate(input_text, max_new_tokens=100):\n",
    "    # Create the prompt template\n",
    "    prompt_template = \"Replace all es or Es with 3s in the following text.\\n\\n### Input:\\n{input_text}\\n\\n### Output:\\n\"\n",
    "    \n",
    "    # Format the prompt with the input text\n",
    "    formatted_prompt = prompt_template.format(input_text=input_text)\n",
    "    \n",
    "    # Tokenize the formatted prompt\n",
    "    input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "    # Generate the output using the trained model\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    \n",
    "    # Decode the generated output\n",
    "    generated_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=False)[0]\n",
    "    \n",
    "    # Extract the generated output after \"### Output:\"\n",
    "    generated_output = generated_text.split(\"### Output:\")[-1].strip()\n",
    "    \n",
    "    return generated_output\n",
    "\n",
    "# Example usage\n",
    "example_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog.\n",
    "Elephants are the largest land mammals on Earth.\n",
    "The Earth revolves around the Sun, which is a star.\n",
    "\"\"\"\n",
    "\n",
    "# Generate the output using the trained model\n",
    "generated_output = generate(example_text)\n",
    "\n",
    "print(\"Generated Output:\")\n",
    "print(generated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ef12ea2-78e5-4aea-9be5-8a5da6650c26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "OLMo peft",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
