{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(evals_benchmarks)=\n",
    "# Evaluations and Benchmarks\n",
    "\n",
    "At some point, when fine-tuning a model, we want to be able to assess whether it's \"doing a good job\" in some sense. So far, for demonstration purposes, we have been been pursuing fairly straightforward goals. Starting with a model that cannot do a given task, can we make it so that it is able to do that task? For example, the [OLMo](olmo_1b_ift) and [gpt2](gpt2) instruction-tuning notebooks were focused on taking a model that could not respond appropriately to questions or instructions, and training it such that it could respond appropriately. We measured this simply by comparing the base models to the fine-tuned models on a hanful of question and instruction examples and confirming that the fine-tuned model responded how we wanted it to.\n",
    "\n",
    "This isn't a very rigorous approach, and we often have more ambitious aims than just changing the response style. We might, for example, want to improve a model's reasoning abilities, its knowledge of certain subjects, or its performance at particular tasks. Or we might, you know, just want to show off a model that performs better than others across multiple dimensions. We measure these aims using a variety of different evaluation and benchmarking techniques.\n",
    "\n",
    "It's worth noting at this point that LLM evaluation is an evolving field, and if anyone agrees on anything, it's that evaluations aren't yet very good at capturing what matters. If you find evaluations confusing and difficult, you're in good company. If you have an idea of a better way to evaluate a model, give it a try and write about it!\n",
    "\n",
    "## Types of evaluations\n",
    "\n",
    "There are quite a few different approaches to evaluating models. Some common categories include:\n",
    "- perplexity-based evaluations\n",
    "- classical NLP metrics (BLEU, ROUGE)\n",
    "- LLM as judge\n",
    "- Task-Oriented Evaluation\n",
    "- Reasoning and Knowledge evaluation\n",
    "\n",
    "Note that these are not industry-standard classifications (and I will likely change and rearrange them over time).\n",
    "\n",
    "We're going to start by discussing *perplexity* as it is an important building block for some popular evaluations and benchmarks used in the field today.\n",
    "\n",
    "## Perplexity\n",
    "\n",
    "[This Hugging Face Conceptual Guide](https://huggingface.co/docs/transformers/en/perplexity) provides a good overview of perplexity. Intuitively, you can think of perplexity as representing an LLM's level of uncertainty about the next token generated given the preceding tokens. The lower the perplexity, the more certain the model is about the next token. If a model has a perplexity of *k*, it is as if the model had to pick between *k* possible next tokens, each with the same probability. Thus, lower perplexity means the model is more \"confident\" in its choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b873a567de4794bff5172306e3c270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity for the question is: 3410.03\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_id)\n",
    "\n",
    "question = \"What is the capital of France?\"\n",
    "choices = [\"(A) London\", \"(B) Paris\", \"(C) Berlin\", \"(D) Madrid\"]\n",
    "\n",
    "# Formatting the input sequence\n",
    "input_sequence = f\"{question} {' '.join(choices)}\"\n",
    "input_ids = tokenizer.encode(input_sequence, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Getting the token ids for the correct answer\n",
    "correct_answer = \"(B) Paris\"\n",
    "correct_answer_ids = tokenizer.encode(\n",
    "    correct_answer, add_prefix_space=True, add_special_tokens=False\n",
    ")\n",
    "\n",
    "# Calculating the perplexity\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "    log_probs = outputs.logits.log_softmax(dim=-1)\n",
    "\n",
    "    # Find the location of the correct answer in the input sequence\n",
    "    answer_indices = (input_ids[0] == correct_answer_ids[0]).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Extract the log probabilities for the correct answer tokens\n",
    "    answer_log_probs = log_probs[0, answer_indices - 1, correct_answer_ids]\n",
    "\n",
    "    # Calculate the perplexity\n",
    "    perplexity = torch.exp(-answer_log_probs.mean()).item()\n",
    "\n",
    "print(f\"The perplexity for the question is: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[357, 33, 8, 6342]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_answer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
