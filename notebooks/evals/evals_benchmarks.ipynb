{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(evals_benchmarks)=\n",
    "# Evaluations and Benchmarks\n",
    "\n",
    "At some point, when fine-tuning a model, we want to be able to assess whether it's \"doing a good job\" in some sense. So far, for demonstration purposes, we have been been pursuing fairly straightforward goals. Starting with a model that cannot do a given task, can we make it so that it is able to do that task? For example, the [OLMo](olmo_1b_ift) and [gpt2](gpt2) instruction-tuning notebooks were focused on taking a model that could not respond appropriately to questions or instructions, and training it such that it could respond appropriately. We measured this simply by comparing the base models to the fine-tuned models on a hanful of question and instruction examples and confirming that the fine-tuned model responded how we wanted it to.\n",
    "\n",
    "This isn't a very rigorous approach, and we often have more ambitious aims than just changing the response style. We might, for example, want to improve a model's reasoning abilities, its knowledge of certain subjects, or its performance at particular tasks. Or we might, you know, just want to show off a model that performs better than others across multiple dimensions. We measure these aims using a variety of different evaluation and benchmarking techniques.\n",
    "\n",
    "It's worth noting at this point that LLM evaluation is an evolving field, and if anyone agrees on anything, it's that evaluations aren't yet very good at capturing what matters. If you find evaluations confusing and difficult, you're in good company. If you have an idea of a better way to evaluate a model, give it a try and write about it!\n",
    "\n",
    "## Types of evaluations\n",
    "\n",
    "There are quite a few different approaches to evaluating models. Some common categories include:\n",
    "- perplexity-based evaluations\n",
    "- classical NLP metrics (BLEU, ROUGE)\n",
    "- LLM as judge\n",
    "- Task-Oriented Evaluation\n",
    "- Reasoning and Knowledge evaluation\n",
    "\n",
    "Note that these are not industry-standard classifications (and I will likely change and rearrange them over time).\n",
    "\n",
    "We're going to start by discussing *perplexity* as it is an important building block for some popular evaluations and benchmarks used in the field today.\n",
    "\n",
    "## Perplexity\n",
    "\n",
    "[This Hugging Face Conceptual Guide](https://huggingface.co/docs/transformers/en/perplexity) provides a good overview of perplexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
