{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa5c3bf1-cfe1-4059-b163-47766c785f57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Introduction\n",
    "The [t5-small on a single GPU](1. T5-Small on Single GPU) example provided a straightforward example of fine-tuning a language model. However, you might have noticed that the training problem was still essentially structured as a supervised learning problem: we had a text (code snippet) and a desired completion. When training LLMs like the GPT models, labels are not provided manually. We instead use an approach called self-supervised learning wherein the objective is automatically computed from the inputs. One example of self-supervised learning is causal language modeling, where the task is to predict the next word based on the previous words. E.g. the sentence \"The boy hid behind the tree\" would be decomposed into the following training tasks:\n",
    "- Input: `The`, Target: `boy`\n",
    "- Input: `The boy`, Target: `hid`\n",
    "- Input: `The boy hid`, Target: `behind`\n",
    "- Input: `The boy hid behind`, Target: `the`\n",
    "- Input: `The boy hid behind the`, Target: `tree`.\n",
    "\n",
    "This requires us to preprocess our data and pass it along to the model somewhat differently, which will be the subject of this notebook. We will still limit this example to training on a single GPU (an a10 with 24GB VRAM). We will use the [gpt2](https://huggingface.co/gpt2) model with 124M parameters. Later, we will work though Eleuther's [Transformer Math blog post](https://blog.eleuther.ai/transformer-math/#training) to understand the memory costs associated with training this model under different conditions and verify that it matches our experience. Hugging Face also provides a guide to [model memory anatomy](https://huggingface.co/docs/transformers/model_memory_anatomy).\n",
    "\n",
    "According to the Hugging Face post, a good heuristic is that we require around 18GB VRAM + additional memory for activations (dependent on sequence length, batch size, and various model architecture details) for mixed-precision training. In this case, that translates to around 2GB VRAM + activations.\n",
    "\n",
    "# Topics Covered in this Notebook\n",
    "The major difference between this exampl and the t5-small example is the focus on self-supervised learning. Additionally, this notebook will go a little deeper into:\n",
    "- monitoring training metrics with MLflow\n",
    "- measuring memory usage\n",
    "\n",
    "Before progressing to multi-GPU and multi-node training, we will also explore ways to improve training efficiency on a single GPU with techniques such as mixed-precision training.\n",
    "\n",
    "# Choosing a Fine-Tuning Task\n",
    "We will fine-tune GPT2 on the [tinystories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset. TinyStories is:\n",
    "\n",
    "> a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4.\n",
    "\n",
    "and can be used to train small models (actually quite a bit smaller than GPT-2) that\n",
    "\n",
    "> still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities.\n",
    "\n",
    "([Source](https://arxiv.org/abs/2305.07759))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the model by passing prompts such as this example from the TinyStories paper:\n",
    "\n",
    "> Once upon a time there was a pumpkin. It was a very special pumpkin, it could speak. It was sad because it couldnâ€™t move. Every day, it would say\n",
    "\n",
    "and evaluating the grammar, consistency, and creativity of the output. We hope to see improvements in these areas after training.\n",
    "\n",
    "# 1. Load the model and try some examples\n",
    "\n",
    "We'll begin by loading the model and trying out some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for example 1:\n",
      "There was a cat with magic powers. It could turn invisible. But one day, the cat lost its magic and became the king of that world.\"\n",
      "\n",
      "\n",
      "Completion for example 2:\n",
      "There was a cloud that could laugh. It laughed every day. But one day, the cloud didn't laugh. The animals in the forest decided to come in. They were trying to live happily without us. This is a bad thing. They are like a deer that don't give a f*ck for me. The one who comes in here is the only one who likes the forest. But\n",
      "\n",
      "\n",
      "Completion for example 3:\n",
      "Every night, Mia looked at the stars. But one night, one star twinkled differently. It seemed to be sending a message. Mia thought hard about what it could mean and figured that it should be okay to not think the exact way they did.\n",
      "\n",
      "I am a girl who loves to play a game of cards. When I play poker, I want it to be about poker, the game in general. I have\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"There was a cat with magic powers. It could turn invisible. But one day, the cat lost its magic and\",\n",
    "    \"There was a cloud that could laugh. It laughed every day. But one day, the cloud didn't laugh. The animals in the forest decided to\",\n",
    "    \"Every night, Mia looked at the stars. But one night, one star twinkled differently. It seemed to be sending a message. Mia thought hard about what it could mean and\",\n",
    "]\n",
    "\n",
    "# Tokenize the examples\n",
    "inputs = tokenizer(examples, return_tensors=\"pt\", padding=True, add_special_tokens=True, truncation=True)\n",
    "\n",
    "# Move tensors to the same device as the model\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate text with the model\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    ")\n",
    "# Decode and print the outputs\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"Completion for example {i + 1}:\")\n",
    "    print(tokenizer.decode(output, skip_special_tokens=True))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not the most coherent results. Hopefully our fine-tuning will improve this. Let's get the dataset and take a look at it.\n",
    "\n",
    "# 2. Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47ea7ad462c4bea9bdcb06481b37ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel.liden/git/fine-tuning/myenv/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c24f9dd6c7488093003a8fa171fc7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2d8ac00631485c9dc5cd96304ddbec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/249M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bec7df7ef2b421f97365cbe2c667fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc22bb42c4247308c73226ea6532fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43c0ba7db2d4e7b9419c7ab320648f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/248M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90c38349d624701b25c7f4458faa700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/9.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380b532f897e400aaa7899f522c10d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "655f71330cb54160a7054ac9009fbf8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57de01935a4d469280030aad4a29fe6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "tinystories = load_dataset('roneneldan/TinyStories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinystories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are > 2 million training samples and > 20,000 validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  One day, a little girl named Lily found a need...\n",
      "1  Once upon a time, there was a little car named...\n",
      "2  One day, a little fish named Fin was swimming ...\n",
      "3  Once upon a time, in a land full of trees, the...\n",
      "4  Once upon a time, there was a little girl name...\n",
      "5  Once upon a time, in a big lake, there was a b...\n",
      "6  Once upon a time, in a small town, there was a...\n",
      "7  Once upon a time, in a peaceful town, there li...\n",
      "8  Once upon a time, there was a clever little do...\n",
      "9  One day, a fast driver named Tim went for a ri...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the train dataset to a pandas dataframe and preview the first few rows\n",
    "df = pd.DataFrame(tinystories['train'][:10])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. GPT2 on a single GPU",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
