{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78ac6102-c07d-4298-b02a-a692b7a488f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nRequirement already satisfied: transformers in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e20b63e5-450d-4946-8f9e-3038472f5aa5/lib/python3.10/site-packages (4.36.1)\nRequirement already satisfied: accelerate in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e20b63e5-450d-4946-8f9e-3038472f5aa5/lib/python3.10/site-packages (0.25.0)\nRequirement already satisfied: torch in /databricks/python3/lib/python3.10/site-packages (2.0.1+cu118)\nCollecting torch\n  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 670.2/670.2 MB 2.0 MB/s eta 0:00:00\nRequirement already satisfied: packaging>=20.0 in /databricks/python3/lib/python3.10/site-packages (from transformers) (22.0)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: pyyaml>=5.1 in /databricks/python3/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: tqdm>=4.27 in /databricks/python3/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: safetensors>=0.3.1 in /databricks/python3/lib/python3.10/site-packages (from transformers) (0.4.0)\nRequirement already satisfied: filelock in /databricks/python3/lib/python3.10/site-packages (from transformers) (3.9.0)\nRequirement already satisfied: regex!=2019.12.17 in /databricks/python3/lib/python3.10/site-packages (from transformers) (2022.7.9)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e20b63e5-450d-4946-8f9e-3038472f5aa5/lib/python3.10/site-packages (from transformers) (0.19.4)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-e20b63e5-450d-4946-8f9e-3038472f5aa5/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: psutil in /databricks/python3/lib/python3.10/site-packages (from accelerate) (5.9.0)\nRequirement already satisfied: typing-extensions in /databricks/python3/lib/python3.10/site-packages (from torch) (4.4.0)\nRequirement already satisfied: networkx in /databricks/python3/lib/python3.10/site-packages (from torch) (2.8.4)\nCollecting nvidia-cusolver-cu12==11.4.5.107\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 5.6 MB/s eta 0:00:00\nCollecting nvidia-cufft-cu12==11.0.2.54\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 5.7 MB/s eta 0:00:00\nCollecting nvidia-nvtx-cu12==12.1.105\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 874.7 kB/s eta 0:00:00\nCollecting nvidia-cuda-runtime-cu12==12.1.105\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 23.0 MB/s eta 0:00:00\nRequirement already satisfied: fsspec in /databricks/python3/lib/python3.10/site-packages (from torch) (2023.6.0)\nCollecting triton==2.1.0\n  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.2/89.2 MB 27.8 MB/s eta 0:00:00\nCollecting nvidia-cudnn-cu12==8.9.2.26\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 1.8 MB/s eta 0:00:00\nCollecting nvidia-cublas-cu12==12.1.3.1\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 3.6 MB/s eta 0:00:00\nCollecting nvidia-cusparse-cu12==12.1.0.106\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 11.8 MB/s eta 0:00:00\nCollecting nvidia-nccl-cu12==2.18.1\n  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.8/209.8 MB 10.4 MB/s eta 0:00:00\nCollecting nvidia-cuda-cupti-cu12==12.1.105\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 97.6 MB/s eta 0:00:00\nCollecting nvidia-curand-cu12==10.3.2.106\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 40.8 MB/s eta 0:00:00\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.10/site-packages (from torch) (3.1.2)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 74.7 MB/s eta 0:00:00\nRequirement already satisfied: sympy in /databricks/python3/lib/python3.10/site-packages (from torch) (1.11.1)\nCollecting nvidia-nvjitlink-cu12\n  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.5/20.5 MB 85.1 MB/s eta 0:00:00\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\nRequirement already satisfied: mpmath>=0.19 in /databricks/python3/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\nInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: triton\n    Found existing installation: triton 2.0.0\n    Not uninstalling triton at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e20b63e5-450d-4946-8f9e-3038472f5aa5\n    Can't uninstall 'triton'. No files were found to uninstall.\n  Attempting uninstall: torch\n    Found existing installation: torch 2.0.1+cu118\n    Not uninstalling torch at /databricks/python3/lib/python3.10/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-e20b63e5-450d-4946-8f9e-3038472f5aa5\n    Can't uninstall 'torch'. No files were found to uninstall.\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 2.1.2 which is incompatible.\nSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 torch-2.1.2 triton-2.1.0\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers accelerate torch\n",
    "dbutils.library.restartPython() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa5c3bf1-cfe1-4059-b163-47766c785f57",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Introduction\n",
    "The [t5-small on a single GPU](1. T5-Small on Single GPU) example provided a straightforward example of fine-tuning a language model. However, you might have noticed that the training problem was still essentially structured as a supervised learning problem: we had a text (code snippet) and a desired completion. When training LLMs like the GPT models, labels are not provided manually. We instead use an approach called self-supervised learning wherein the objective is automatically computed from the inputs. One example of self-supervised learning is causal language modeling, where the task is to predict the next word based on the previous words. E.g. the sentence \"The boy hid behind the tree\" would be decomposed into the following training tasks:\n",
    "- Input: `The`, Target: `boy`\n",
    "- Input: `The boy`, Target: `hid`\n",
    "- Input: `The boy hid`, Target: `behind`\n",
    "- Input: `The boy hid behind`, Target: `the`\n",
    "- Input: `The boy hid behind the`, Target: `tree`.\n",
    "\n",
    "This requires us to preprocess our data and pass it along to the model somewhat differently, which will be the subject of this notebook. We will still limit this example to training on a single GPU (an a10 with 24GB VRAM). We will use the [gpt2](https://huggingface.co/gpt2) model with 124M parameters. Later, we will work though Eleuther's [Transformer Math blog post](https://blog.eleuther.ai/transformer-math/#training) to understand the memory costs associated with training this model under different conditions and verify that it matches our experience. Hugging Face also provides a guide to [model memory anatomy](https://huggingface.co/docs/transformers/model_memory_anatomy).\n",
    "\n",
    "According to the Hugging Face post, a good heuristic is that we require around 18GB VRAM + additional memory for activations (dependent on sequence length, batch size, and various model architecture details) for mixed-precision training. In this case, that translates to around 2GB VRAM + activations.\n",
    "\n",
    "# Topics Covered in this Notebook\n",
    "The major difference between this exampl and the t5-small example is the focus on self-supervised learning. Additionally, this notebook will go a little deeper into:\n",
    "- monitoring training metrics with MLflow\n",
    "- measuring memory usage\n",
    "\n",
    "Before progressing to multi-GPU and multi-node training, we will also explore ways to improve training efficiency on a single GPU with techniques such as mixed-precision training.\n",
    "\n",
    "# Choosing a Fine-Tuning Task\n",
    "We will fine-tune GPT2 on the [tinystories](https://huggingface.co/datasets/roneneldan/TinyStories) dataset. TinyStories is:\n",
    "\n",
    "> a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4.\n",
    "\n",
    "and can be used to train small models (actually quite a bit smaller than GPT-2) that\n",
    "\n",
    "> still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities.\n",
    "\n",
    "([Source](https://arxiv.org/abs/2305.07759))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a684fb64-3c55-4520-ae35-8d0513861b69",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can evaluate the model by passing prompts such as this example from the TinyStories paper:\n",
    "\n",
    "> Once upon a time there was a pumpkin. It was a very special pumpkin, it could speak. It was sad because it couldn’t move. Every day, it would say\n",
    "\n",
    "and evaluating the grammar, consistency, and creativity of the output. We hope to see improvements in these areas after training.\n",
    "\n",
    "# 1. Load the model and try some examples\n",
    "\n",
    "We'll begin by loading the model and trying out some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d140d8d8-b02f-4a7e-b583-3fed7d41980f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-15 00:38:11.258375: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2023-12-15 00:38:11.258443: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2023-12-15 00:38:11.258465: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2023-12-15 00:38:11.265245: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c64eec-d976-4248-8072-8c22f0564329",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion for example 1:\nThere was a cat with magic powers. It could turn invisible. But one day, the cat lost its magic and became a demon. But the demon couldn't have survived for very long and attacked me and his companions. This is the final stage of the story.\n\n\"As expected, there are a few things you can say as well. First, your\n\n\nCompletion for example 2:\nThere was a cloud that could laugh. It laughed every day. But one day, the cloud didn't laugh. The animals in the forest decided to kill everyone.\n\n\"If there's something I'm trying to do, I can't run!\"\n\n\"Do you have a job to do?\"\n\n\"I don't want to die, so I can't run anymore.\"\n\n\n\nCompletion for example 3:\nEvery night, Mia looked at the stars. But one night, one star twinkled differently. It seemed to be sending a message. Mia thought hard about what it could mean and it didn't stop there.\n\n\"Why didn't you want to die? If you don't want to, I'm sure you'll need to see someone who loves him, or someone who's not afraid to talk about things with you.\n\n\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"There was a cat with magic powers. It could turn invisible. But one day, the cat lost its magic and\",\n",
    "    \"There was a cloud that could laugh. It laughed every day. But one day, the cloud didn't laugh. The animals in the forest decided to\",\n",
    "    \"Every night, Mia looked at the stars. But one night, one star twinkled differently. It seemed to be sending a message. Mia thought hard about what it could mean and\",\n",
    "]\n",
    "\n",
    "# Tokenize the examples\n",
    "inputs = tokenizer(examples, return_tensors=\"pt\", padding=True, add_special_tokens=True, truncation=True)\n",
    "\n",
    "# Move tensors to the same device as the model\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate text with the model\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    top_p=0.95,\n",
    ")\n",
    "# Decode and print the outputs\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"Completion for example {i + 1}:\")\n",
    "    print(tokenizer.decode(output, skip_special_tokens=True))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd07475d-f4fb-4e5b-990e-528224c5b6ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Not the most coherent results. Hopefully our fine-tuning will improve this. Let's get the dataset and take a look at it.\n",
    "\n",
    "# 2. Get the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f70849d-1177-4735-9bb1-f566e4e9b108",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:27: UserWarning: This dataset can not be stored in DBFS because either `cache_dir` or the environment variable `HF_DATASETS_CACHE` is set to a non-DBFS path. If this cluster restarts, all saved dataset information will be lost.\n  warnings.warn(\n/local_disk0/.ephemeral_nfs/envs/pythonEnv-be8a10cc-4f2f-488d-9bc2-8bbb8367cd8f/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n/databricks/python_shell/dbruntime/huggingface_patches/datasets.py:13: UserWarning: During large dataset downloads, there could be multiple progress bar widgets that can cause performance issues for your notebook or browser. To avoid these issues, use `datasets.utils.logging.disable_progress_bar()` to turn off the progress bars.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "tinystories = load_dataset('roneneldan/TinyStories')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16fec8cc-737a-44dc-97d3-f6ceb5df0f55",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Inspect the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d5fdc7-f470-4702-bd9f-85f67a5ca4d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinystories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8866c7d8-c935-40fa-b472-6c614de27167",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are > 2 million training samples and > 20,000 validation samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47b741b5-31ff-4fc3-8f25-00a8e04566c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n0  One day, a little girl named Lily found a need...\n1  Once upon a time, there was a little car named...\n2  One day, a little fish named Fin was swimming ...\n3  Once upon a time, in a land full of trees, the...\n4  Once upon a time, there was a little girl name...\n5  Once upon a time, in a big lake, there was a b...\n6  Once upon a time, in a small town, there was a...\n7  Once upon a time, in a peaceful town, there li...\n8  Once upon a time, there was a clever little do...\n9  One day, a fast driver named Tim went for a ri...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the train dataset to a pandas dataframe and preview the first few rows\n",
    "df = pd.DataFrame(tinystories['train'][:10])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c592f06b-612b-430b-9a72-99d375a2bf75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3. Fine-Tune the Model\n",
    "This time around, we're going to train the model with a little more care. In particular, we will:\n",
    "- keep a close eye on training metrics using MLflow\n",
    "- do a few test runs to choose a set of reasonable hyperparameters for our final fine-tuning run\n",
    "- use mixed-precision training for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3568fa5-28e4-4ae7-9b93-6dc5f9bd9b9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "\n",
    "if os.environ.get('DATABRICKS_RUNTIME_VERSION') is not None:\n",
    "    cache_file_path_train = \"/Volumes/daniel_liden/fine_tune/assets/cache/train_cache.arrow\"\n",
    "    cache_file_path_valid = \"/Volumes/daniel_liden/fine_tune/assets/cache/validation_cache.arrow\"\n",
    "else:\n",
    "    cache_file_path_train = \"./cache/train_cache.arrow\"\n",
    "    cache_file_path_valid = \"./cache/validation_cache.arrow\"\n",
    "\n",
    "\n",
    "if not os.path.exists(\"./cache/\"):\n",
    "    os.makedirs(\"./cache/\")\n",
    "\n",
    "# Tokenize and cache the train data\n",
    "tokenized_train_data = tinystories[\"train\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    cache_file_name=cache_file_path_train  # Cache file for the training set\n",
    ")\n",
    "\n",
    "# Tokenize and cache the validation data\n",
    "tokenized_validation_data = tinystories[\"validation\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    cache_file_name=cache_file_path_valid  # Cache file for the validation set\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25ea0a78-d78e-48d0-82f8-1538ed558c39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 42:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.860300</td>\n",
       "      <td>1.827593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.738100</td>\n",
       "      <td>1.753622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.746100</td>\n",
       "      <td>1.711488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.725400</td>\n",
       "      <td>1.688552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.692500</td>\n",
       "      <td>1.680552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import mlflow\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/Volumes/daniel_liden/fine_tune/assets/results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4, \n",
    "    per_device_eval_batch_size=4, \n",
    "    warmup_steps=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,  # Log every 10 steps\n",
    "    evaluation_strategy=\"steps\",  # Evaluate every 'eval_steps'\n",
    "    eval_steps=1000,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Initialize the data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data.select(range(20000)),  # Use only the first 20k rows for train data\n",
    "    eval_dataset=tokenized_validation_data.select(range(5000)),  # Use only the first 5k rows for eval data\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Start training and track with MLflow\n",
    "with mlflow.start_run():\n",
    "    trainer.train()\n",
    "    mlflow.log_params(training_args.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be94cace-4011-4264-aebe-3b44702cc8ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4. Load the Model Checkpoint and Run some Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6567c489-6bc3-4329-826d-618585f309fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a43bd036ce0448782cbbe0558df0625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fba8c58522b479d81c1b08677c036c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e691afa8503248ba887b1345fdeea962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8448752b611d4bbf8a00eab465201e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[{'generated_text': 'There was a cat with magic powers. It could turn invisible. But one day, the cat lost its magic and was stuck in a maze.\\n\\nThe cat wanted to find new ways to find its magic. So, it decided to learn how to build a castle. When the cat\\'s friends saw it, they were frightened. They said, \"Go away'}],\n",
       " [{'generated_text': \"There was a cloud that could laugh. It laughed every day. But one day, the cloud didn't laugh. The animals in the forest decided to jump on it and take it out. The animals got scared and ran away.\\n\\nThe clouds liked to laugh at the animals. They didn't want to make them scared. They wanted to go home and play with the animal.\\n\\nThe\"}],\n",
       " [{'generated_text': 'Every night, Mia looked at the stars. But one night, one star twinkled differently. It seemed to be sending a message. Mia thought hard about what it could mean and decided to do something different.\\n\\nShe grabbed her flashlight and made a circle around her star. She pointed to the sun and said, \"Wow, look at my sun!\" Then, she made a beautiful shape in her face. She smiled and'}]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "examples = [\n",
    "    \"There was a cat with magic powers. It could turn invisible. But one day, the cat lost its magic and\",\n",
    "    \"There was a cloud that could laugh. It laughed every day. But one day, the cloud didn't laugh. The animals in the forest decided to\",\n",
    "    \"Every night, Mia looked at the stars. But one night, one star twinkled differently. It seemed to be sending a message. Mia thought hard about what it could mean and\",\n",
    "]\n",
    "\n",
    "# Specify the path to your checkpoint\n",
    "checkpoint_path = \"/Volumes/daniel_liden/fine_tune/assets/results/checkpoint-5000\"\n",
    "\n",
    "# Load the tokenizer and model from the checkpoint\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path)\n",
    "\n",
    "# Create a pipeline for text generation (adjust task as needed)\n",
    "gpt2_pipeline = pipeline(\n",
    "    \"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Use the pipeline for inference\n",
    "gpt2_pipeline(examples, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b9d2612-07b8-49d7-96a1-9485e8eb914e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2. GPT2 on a single GPU",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
